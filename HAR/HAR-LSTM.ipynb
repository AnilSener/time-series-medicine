{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# HAR LSTM training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.utilities import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, labels_train, list_ch_train = read_data(data_path=\"./data/\", split=\"train\") # train\n",
    "X_test, labels_test, list_ch_test = read_data(data_path=\"./data/\", split=\"test\") # test\n",
    "\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Standardize?\n",
    "#X_train, X_test = standardize(X_train, X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n",
    "                                                stratify = labels_train, test_size = 0.2,\n",
    "                                                random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "\n",
    "lstm_size = 27         # 3 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 600       # Batch size\n",
    "seq_len = 128          # Number of steps\n",
    "learning_rate = 0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 500\n",
    "\n",
    "# Fixed\n",
    "n_classes = 6\n",
    "n_channels = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construct the graph\n",
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct inputs to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define forward pass, cost function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note: We could use gradient clipping if there are problems training. Then we would need to change the optimizer as follows:\n",
    "\n",
    "```python\n",
    "train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "    \n",
    "gradients = train_op.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "optimizer = train_op.apply_gradients(capped_gradients)  \n",
    "```\n",
    "This will ensure that gradients with absolute value larger than 1 will be clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints') == False):\n",
    "    !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 Iteration: 5 Train loss: 1.784034 Train acc: 0.203333\n",
      "Epoch: 1/500 Iteration: 10 Train loss: 1.774955 Train acc: 0.231667\n",
      "Epoch: 1/500 Iteration: 15 Train loss: 1.770144 Train acc: 0.231667\n",
      "Epoch: 2/500 Iteration: 20 Train loss: 1.755243 Train acc: 0.268333\n",
      "Epoch: 2/500 Iteration: 25 Train loss: 1.736787 Train acc: 0.300000\n",
      "Epoch: 2/500 Iteration: 25 Validation loss: 1.730827 Validation acc: 0.351667\n",
      "Epoch: 3/500 Iteration: 30 Train loss: 1.738233 Train acc: 0.293333\n",
      "Epoch: 3/500 Iteration: 35 Train loss: 1.736092 Train acc: 0.288333\n",
      "Epoch: 4/500 Iteration: 40 Train loss: 1.725342 Train acc: 0.286667\n",
      "Epoch: 4/500 Iteration: 45 Train loss: 1.702857 Train acc: 0.321667\n",
      "Epoch: 5/500 Iteration: 50 Train loss: 1.701926 Train acc: 0.316667\n",
      "Epoch: 5/500 Iteration: 50 Validation loss: 1.677944 Validation acc: 0.363333\n",
      "Epoch: 6/500 Iteration: 55 Train loss: 1.683910 Train acc: 0.321667\n",
      "Epoch: 6/500 Iteration: 60 Train loss: 1.672339 Train acc: 0.355000\n",
      "Epoch: 7/500 Iteration: 65 Train loss: 1.659570 Train acc: 0.351667\n",
      "Epoch: 7/500 Iteration: 70 Train loss: 1.634992 Train acc: 0.370000\n",
      "Epoch: 8/500 Iteration: 75 Train loss: 1.644396 Train acc: 0.385000\n",
      "Epoch: 8/500 Iteration: 75 Validation loss: 1.610685 Validation acc: 0.372500\n",
      "Epoch: 8/500 Iteration: 80 Train loss: 1.632591 Train acc: 0.335000\n",
      "Epoch: 9/500 Iteration: 85 Train loss: 1.626417 Train acc: 0.310000\n",
      "Epoch: 9/500 Iteration: 90 Train loss: 1.607136 Train acc: 0.331667\n",
      "Epoch: 10/500 Iteration: 95 Train loss: 1.596237 Train acc: 0.365000\n",
      "Epoch: 11/500 Iteration: 100 Train loss: 1.564778 Train acc: 0.378333\n",
      "Epoch: 11/500 Iteration: 100 Validation loss: 1.541419 Validation acc: 0.398333\n",
      "Epoch: 11/500 Iteration: 105 Train loss: 1.573473 Train acc: 0.343333\n",
      "Epoch: 12/500 Iteration: 110 Train loss: 1.549127 Train acc: 0.365000\n",
      "Epoch: 12/500 Iteration: 115 Train loss: 1.545596 Train acc: 0.345000\n",
      "Epoch: 13/500 Iteration: 120 Train loss: 1.536201 Train acc: 0.385000\n",
      "Epoch: 13/500 Iteration: 125 Train loss: 1.545488 Train acc: 0.330000\n",
      "Epoch: 13/500 Iteration: 125 Validation loss: 1.484013 Validation acc: 0.510833\n",
      "Epoch: 14/500 Iteration: 130 Train loss: 1.538848 Train acc: 0.338333\n",
      "Epoch: 14/500 Iteration: 135 Train loss: 1.504467 Train acc: 0.380000\n",
      "Epoch: 15/500 Iteration: 140 Train loss: 1.524174 Train acc: 0.353333\n",
      "Epoch: 16/500 Iteration: 145 Train loss: 1.467762 Train acc: 0.398333\n",
      "Epoch: 16/500 Iteration: 150 Train loss: 1.468778 Train acc: 0.401667\n",
      "Epoch: 16/500 Iteration: 150 Validation loss: 1.439072 Validation acc: 0.486667\n",
      "Epoch: 17/500 Iteration: 155 Train loss: 1.474748 Train acc: 0.368333\n",
      "Epoch: 17/500 Iteration: 160 Train loss: 1.464972 Train acc: 0.393333\n",
      "Epoch: 18/500 Iteration: 165 Train loss: 1.449069 Train acc: 0.435000\n",
      "Epoch: 18/500 Iteration: 170 Train loss: 1.472423 Train acc: 0.388333\n",
      "Epoch: 19/500 Iteration: 175 Train loss: 1.469846 Train acc: 0.373333\n",
      "Epoch: 19/500 Iteration: 175 Validation loss: 1.401374 Validation acc: 0.517500\n",
      "Epoch: 19/500 Iteration: 180 Train loss: 1.468049 Train acc: 0.371667\n",
      "Epoch: 20/500 Iteration: 185 Train loss: 1.449685 Train acc: 0.380000\n",
      "Epoch: 21/500 Iteration: 190 Train loss: 1.412509 Train acc: 0.430000\n",
      "Epoch: 21/500 Iteration: 195 Train loss: 1.421881 Train acc: 0.396667\n",
      "Epoch: 22/500 Iteration: 200 Train loss: 1.413491 Train acc: 0.413333\n",
      "Epoch: 22/500 Iteration: 200 Validation loss: 1.366703 Validation acc: 0.536667\n",
      "Epoch: 22/500 Iteration: 205 Train loss: 1.389949 Train acc: 0.420000\n",
      "Epoch: 23/500 Iteration: 210 Train loss: 1.392824 Train acc: 0.425000\n",
      "Epoch: 23/500 Iteration: 215 Train loss: 1.411576 Train acc: 0.431667\n",
      "Epoch: 24/500 Iteration: 220 Train loss: 1.400706 Train acc: 0.421667\n",
      "Epoch: 24/500 Iteration: 225 Train loss: 1.386373 Train acc: 0.435000\n",
      "Epoch: 24/500 Iteration: 225 Validation loss: 1.323747 Validation acc: 0.534167\n",
      "Epoch: 25/500 Iteration: 230 Train loss: 1.386050 Train acc: 0.446667\n",
      "Epoch: 26/500 Iteration: 235 Train loss: 1.328490 Train acc: 0.485000\n",
      "Epoch: 26/500 Iteration: 240 Train loss: 1.326306 Train acc: 0.470000\n",
      "Epoch: 27/500 Iteration: 245 Train loss: 1.314625 Train acc: 0.470000\n",
      "Epoch: 27/500 Iteration: 250 Train loss: 1.299066 Train acc: 0.481667\n",
      "Epoch: 27/500 Iteration: 250 Validation loss: 1.268077 Validation acc: 0.535833\n",
      "Epoch: 28/500 Iteration: 255 Train loss: 1.291721 Train acc: 0.491667\n",
      "Epoch: 28/500 Iteration: 260 Train loss: 1.322513 Train acc: 0.461667\n",
      "Epoch: 29/500 Iteration: 265 Train loss: 1.305294 Train acc: 0.488333\n",
      "Epoch: 29/500 Iteration: 270 Train loss: 1.299066 Train acc: 0.463333\n",
      "Epoch: 30/500 Iteration: 275 Train loss: 1.296817 Train acc: 0.478333\n",
      "Epoch: 30/500 Iteration: 275 Validation loss: 1.202695 Validation acc: 0.546667\n",
      "Epoch: 31/500 Iteration: 280 Train loss: 1.237750 Train acc: 0.508333\n",
      "Epoch: 31/500 Iteration: 285 Train loss: 1.226009 Train acc: 0.505000\n",
      "Epoch: 32/500 Iteration: 290 Train loss: 1.234648 Train acc: 0.500000\n",
      "Epoch: 32/500 Iteration: 295 Train loss: 1.208237 Train acc: 0.508333\n",
      "Epoch: 33/500 Iteration: 300 Train loss: 1.207748 Train acc: 0.518333\n",
      "Epoch: 33/500 Iteration: 300 Validation loss: 1.151240 Validation acc: 0.564167\n",
      "Epoch: 33/500 Iteration: 305 Train loss: 1.225838 Train acc: 0.486667\n",
      "Epoch: 34/500 Iteration: 310 Train loss: 1.253636 Train acc: 0.503333\n",
      "Epoch: 34/500 Iteration: 315 Train loss: 1.247246 Train acc: 0.500000\n",
      "Epoch: 35/500 Iteration: 320 Train loss: 1.226237 Train acc: 0.501667\n",
      "Epoch: 36/500 Iteration: 325 Train loss: 1.171307 Train acc: 0.533333\n",
      "Epoch: 36/500 Iteration: 325 Validation loss: 1.118580 Validation acc: 0.570000\n",
      "Epoch: 36/500 Iteration: 330 Train loss: 1.184489 Train acc: 0.516667\n",
      "Epoch: 37/500 Iteration: 335 Train loss: 1.163313 Train acc: 0.543333\n",
      "Epoch: 37/500 Iteration: 340 Train loss: 1.157236 Train acc: 0.535000\n",
      "Epoch: 38/500 Iteration: 345 Train loss: 1.157582 Train acc: 0.536667\n",
      "Epoch: 38/500 Iteration: 350 Train loss: 1.169867 Train acc: 0.530000\n",
      "Epoch: 38/500 Iteration: 350 Validation loss: 1.091819 Validation acc: 0.575833\n",
      "Epoch: 39/500 Iteration: 355 Train loss: 1.197487 Train acc: 0.505000\n",
      "Epoch: 39/500 Iteration: 360 Train loss: 1.178051 Train acc: 0.550000\n",
      "Epoch: 40/500 Iteration: 365 Train loss: 1.171042 Train acc: 0.520000\n",
      "Epoch: 41/500 Iteration: 370 Train loss: 1.090301 Train acc: 0.568333\n",
      "Epoch: 41/500 Iteration: 375 Train loss: 1.121893 Train acc: 0.541667\n",
      "Epoch: 41/500 Iteration: 375 Validation loss: 1.055489 Validation acc: 0.593333\n",
      "Epoch: 42/500 Iteration: 380 Train loss: 1.116042 Train acc: 0.566667\n",
      "Epoch: 42/500 Iteration: 385 Train loss: 1.094578 Train acc: 0.563333\n",
      "Epoch: 43/500 Iteration: 390 Train loss: 1.108656 Train acc: 0.570000\n",
      "Epoch: 43/500 Iteration: 395 Train loss: 1.102303 Train acc: 0.586667\n",
      "Epoch: 44/500 Iteration: 400 Train loss: 1.112326 Train acc: 0.588333\n",
      "Epoch: 44/500 Iteration: 400 Validation loss: 1.009858 Validation acc: 0.620833\n",
      "Epoch: 44/500 Iteration: 405 Train loss: 1.112923 Train acc: 0.546667\n",
      "Epoch: 45/500 Iteration: 410 Train loss: 1.116701 Train acc: 0.585000\n",
      "Epoch: 46/500 Iteration: 415 Train loss: 1.035209 Train acc: 0.593333\n",
      "Epoch: 46/500 Iteration: 420 Train loss: 1.053042 Train acc: 0.591667\n",
      "Epoch: 47/500 Iteration: 425 Train loss: 1.053084 Train acc: 0.615000\n",
      "Epoch: 47/500 Iteration: 425 Validation loss: 0.964128 Validation acc: 0.640000\n",
      "Epoch: 47/500 Iteration: 430 Train loss: 1.015366 Train acc: 0.628333\n",
      "Epoch: 48/500 Iteration: 435 Train loss: 1.018851 Train acc: 0.593333\n",
      "Epoch: 48/500 Iteration: 440 Train loss: 1.028512 Train acc: 0.621667\n",
      "Epoch: 49/500 Iteration: 445 Train loss: 1.031408 Train acc: 0.625000\n",
      "Epoch: 49/500 Iteration: 450 Train loss: 1.031254 Train acc: 0.596667\n",
      "Epoch: 49/500 Iteration: 450 Validation loss: 0.911722 Validation acc: 0.656667\n",
      "Epoch: 50/500 Iteration: 455 Train loss: 1.012209 Train acc: 0.620000\n",
      "Epoch: 51/500 Iteration: 460 Train loss: 0.946843 Train acc: 0.645000\n",
      "Epoch: 51/500 Iteration: 465 Train loss: 0.963075 Train acc: 0.645000\n",
      "Epoch: 52/500 Iteration: 470 Train loss: 0.924311 Train acc: 0.668333\n",
      "Epoch: 52/500 Iteration: 475 Train loss: 0.928592 Train acc: 0.633333\n",
      "Epoch: 52/500 Iteration: 475 Validation loss: 0.860342 Validation acc: 0.667500\n",
      "Epoch: 53/500 Iteration: 480 Train loss: 0.955499 Train acc: 0.648333\n",
      "Epoch: 53/500 Iteration: 485 Train loss: 0.937725 Train acc: 0.658333\n",
      "Epoch: 54/500 Iteration: 490 Train loss: 0.956177 Train acc: 0.630000\n",
      "Epoch: 54/500 Iteration: 495 Train loss: 0.953630 Train acc: 0.638333\n",
      "Epoch: 55/500 Iteration: 500 Train loss: 0.932150 Train acc: 0.635000\n",
      "Epoch: 55/500 Iteration: 500 Validation loss: 0.818461 Validation acc: 0.677500\n",
      "Epoch: 56/500 Iteration: 505 Train loss: 0.863637 Train acc: 0.705000\n",
      "Epoch: 56/500 Iteration: 510 Train loss: 0.901734 Train acc: 0.651667\n",
      "Epoch: 57/500 Iteration: 515 Train loss: 0.893713 Train acc: 0.653333\n",
      "Epoch: 57/500 Iteration: 520 Train loss: 0.856185 Train acc: 0.653333\n",
      "Epoch: 58/500 Iteration: 525 Train loss: 0.879613 Train acc: 0.653333\n",
      "Epoch: 58/500 Iteration: 525 Validation loss: 0.786588 Validation acc: 0.677500\n",
      "Epoch: 58/500 Iteration: 530 Train loss: 0.867615 Train acc: 0.676667\n",
      "Epoch: 59/500 Iteration: 535 Train loss: 0.898203 Train acc: 0.645000\n",
      "Epoch: 59/500 Iteration: 540 Train loss: 0.894331 Train acc: 0.640000\n",
      "Epoch: 60/500 Iteration: 545 Train loss: 0.870633 Train acc: 0.645000\n",
      "Epoch: 61/500 Iteration: 550 Train loss: 0.832496 Train acc: 0.680000\n",
      "Epoch: 61/500 Iteration: 550 Validation loss: 0.763948 Validation acc: 0.679167\n",
      "Epoch: 61/500 Iteration: 555 Train loss: 0.851417 Train acc: 0.675000\n",
      "Epoch: 62/500 Iteration: 560 Train loss: 0.849354 Train acc: 0.666667\n",
      "Epoch: 62/500 Iteration: 565 Train loss: 0.854686 Train acc: 0.668333\n",
      "Epoch: 63/500 Iteration: 570 Train loss: 0.830039 Train acc: 0.676667\n",
      "Epoch: 63/500 Iteration: 575 Train loss: 0.855047 Train acc: 0.648333\n",
      "Epoch: 63/500 Iteration: 575 Validation loss: 0.741183 Validation acc: 0.682500\n",
      "Epoch: 64/500 Iteration: 580 Train loss: 0.857031 Train acc: 0.673333\n",
      "Epoch: 64/500 Iteration: 585 Train loss: 0.866036 Train acc: 0.660000\n",
      "Epoch: 65/500 Iteration: 590 Train loss: 0.846232 Train acc: 0.655000\n",
      "Epoch: 66/500 Iteration: 595 Train loss: 0.772524 Train acc: 0.713333\n",
      "Epoch: 66/500 Iteration: 600 Train loss: 0.807735 Train acc: 0.685000\n",
      "Epoch: 66/500 Iteration: 600 Validation loss: 0.721569 Validation acc: 0.694167\n",
      "Epoch: 67/500 Iteration: 605 Train loss: 0.812151 Train acc: 0.661667\n",
      "Epoch: 67/500 Iteration: 610 Train loss: 0.776625 Train acc: 0.705000\n",
      "Epoch: 68/500 Iteration: 615 Train loss: 0.787947 Train acc: 0.676667\n",
      "Epoch: 68/500 Iteration: 620 Train loss: 0.803969 Train acc: 0.690000\n",
      "Epoch: 69/500 Iteration: 625 Train loss: 0.813175 Train acc: 0.680000\n",
      "Epoch: 69/500 Iteration: 625 Validation loss: 0.697228 Validation acc: 0.710000\n",
      "Epoch: 69/500 Iteration: 630 Train loss: 0.829099 Train acc: 0.663333\n",
      "Epoch: 70/500 Iteration: 635 Train loss: 0.778757 Train acc: 0.705000\n",
      "Epoch: 71/500 Iteration: 640 Train loss: 0.748603 Train acc: 0.713333\n",
      "Epoch: 71/500 Iteration: 645 Train loss: 0.772137 Train acc: 0.683333\n",
      "Epoch: 72/500 Iteration: 650 Train loss: 0.756981 Train acc: 0.705000\n",
      "Epoch: 72/500 Iteration: 650 Validation loss: 0.679919 Validation acc: 0.726667\n",
      "Epoch: 72/500 Iteration: 655 Train loss: 0.755035 Train acc: 0.718333\n",
      "Epoch: 73/500 Iteration: 660 Train loss: 0.786936 Train acc: 0.701667\n",
      "Epoch: 73/500 Iteration: 665 Train loss: 0.776835 Train acc: 0.700000\n",
      "Epoch: 74/500 Iteration: 670 Train loss: 0.778400 Train acc: 0.696667\n",
      "Epoch: 74/500 Iteration: 675 Train loss: 0.791454 Train acc: 0.683333\n",
      "Epoch: 74/500 Iteration: 675 Validation loss: 0.666936 Validation acc: 0.750000\n",
      "Epoch: 75/500 Iteration: 680 Train loss: 0.761142 Train acc: 0.723333\n",
      "Epoch: 76/500 Iteration: 685 Train loss: 0.696322 Train acc: 0.733333\n",
      "Epoch: 76/500 Iteration: 690 Train loss: 0.719277 Train acc: 0.715000\n",
      "Epoch: 77/500 Iteration: 695 Train loss: 0.716219 Train acc: 0.730000\n",
      "Epoch: 77/500 Iteration: 700 Train loss: 0.717219 Train acc: 0.723333\n",
      "Epoch: 77/500 Iteration: 700 Validation loss: 0.625575 Validation acc: 0.764167\n",
      "Epoch: 78/500 Iteration: 705 Train loss: 0.695464 Train acc: 0.745000\n",
      "Epoch: 78/500 Iteration: 710 Train loss: 0.733127 Train acc: 0.730000\n",
      "Epoch: 79/500 Iteration: 715 Train loss: 0.741285 Train acc: 0.740000\n",
      "Epoch: 79/500 Iteration: 720 Train loss: 0.768502 Train acc: 0.686667\n",
      "Epoch: 80/500 Iteration: 725 Train loss: 0.721654 Train acc: 0.741667\n",
      "Epoch: 80/500 Iteration: 725 Validation loss: 0.598465 Validation acc: 0.785000\n",
      "Epoch: 81/500 Iteration: 730 Train loss: 0.650122 Train acc: 0.770000\n",
      "Epoch: 81/500 Iteration: 735 Train loss: 0.682115 Train acc: 0.740000\n",
      "Epoch: 82/500 Iteration: 740 Train loss: 0.705875 Train acc: 0.740000\n",
      "Epoch: 82/500 Iteration: 745 Train loss: 0.651137 Train acc: 0.760000\n",
      "Epoch: 83/500 Iteration: 750 Train loss: 0.687990 Train acc: 0.770000\n",
      "Epoch: 83/500 Iteration: 750 Validation loss: 0.562834 Validation acc: 0.810833\n",
      "Epoch: 83/500 Iteration: 755 Train loss: 0.668486 Train acc: 0.756667\n",
      "Epoch: 84/500 Iteration: 760 Train loss: 0.695291 Train acc: 0.771667\n",
      "Epoch: 84/500 Iteration: 765 Train loss: 0.692314 Train acc: 0.751667\n",
      "Epoch: 85/500 Iteration: 770 Train loss: 0.642752 Train acc: 0.773333\n",
      "Epoch: 86/500 Iteration: 775 Train loss: 0.617003 Train acc: 0.783333\n",
      "Epoch: 86/500 Iteration: 775 Validation loss: 0.533476 Validation acc: 0.827500\n",
      "Epoch: 86/500 Iteration: 780 Train loss: 0.638686 Train acc: 0.771667\n",
      "Epoch: 87/500 Iteration: 785 Train loss: 0.633254 Train acc: 0.775000\n",
      "Epoch: 87/500 Iteration: 790 Train loss: 0.629815 Train acc: 0.773333\n",
      "Epoch: 88/500 Iteration: 795 Train loss: 0.615242 Train acc: 0.820000\n",
      "Epoch: 88/500 Iteration: 800 Train loss: 0.653368 Train acc: 0.778333\n",
      "Epoch: 88/500 Iteration: 800 Validation loss: 0.482681 Validation acc: 0.851667\n",
      "Epoch: 89/500 Iteration: 805 Train loss: 0.632340 Train acc: 0.796667\n",
      "Epoch: 89/500 Iteration: 810 Train loss: 0.612884 Train acc: 0.796667\n",
      "Epoch: 90/500 Iteration: 815 Train loss: 0.576157 Train acc: 0.820000\n",
      "Epoch: 91/500 Iteration: 820 Train loss: 0.573749 Train acc: 0.818333\n",
      "Epoch: 91/500 Iteration: 825 Train loss: 0.578975 Train acc: 0.831667\n",
      "Epoch: 91/500 Iteration: 825 Validation loss: 0.470856 Validation acc: 0.850833\n",
      "Epoch: 92/500 Iteration: 830 Train loss: 0.583916 Train acc: 0.801667\n",
      "Epoch: 92/500 Iteration: 835 Train loss: 0.592040 Train acc: 0.810000\n",
      "Epoch: 93/500 Iteration: 840 Train loss: 0.574453 Train acc: 0.826667\n",
      "Epoch: 93/500 Iteration: 845 Train loss: 0.608668 Train acc: 0.803333\n",
      "Epoch: 94/500 Iteration: 850 Train loss: 0.598162 Train acc: 0.793333\n",
      "Epoch: 94/500 Iteration: 850 Validation loss: 0.449946 Validation acc: 0.853333\n",
      "Epoch: 94/500 Iteration: 855 Train loss: 0.576070 Train acc: 0.826667\n",
      "Epoch: 95/500 Iteration: 860 Train loss: 0.550808 Train acc: 0.823333\n",
      "Epoch: 96/500 Iteration: 865 Train loss: 0.522039 Train acc: 0.845000\n",
      "Epoch: 96/500 Iteration: 870 Train loss: 0.556696 Train acc: 0.828333\n",
      "Epoch: 97/500 Iteration: 875 Train loss: 0.538122 Train acc: 0.836667\n",
      "Epoch: 97/500 Iteration: 875 Validation loss: 0.428853 Validation acc: 0.861667\n",
      "Epoch: 97/500 Iteration: 880 Train loss: 0.535262 Train acc: 0.821667\n",
      "Epoch: 98/500 Iteration: 885 Train loss: 0.579229 Train acc: 0.830000\n",
      "Epoch: 98/500 Iteration: 890 Train loss: 0.555880 Train acc: 0.826667\n",
      "Epoch: 99/500 Iteration: 895 Train loss: 0.540383 Train acc: 0.833333\n",
      "Epoch: 99/500 Iteration: 900 Train loss: 0.516751 Train acc: 0.861667\n",
      "Epoch: 99/500 Iteration: 900 Validation loss: 0.417500 Validation acc: 0.866667\n",
      "Epoch: 100/500 Iteration: 905 Train loss: 0.513711 Train acc: 0.843333\n",
      "Epoch: 101/500 Iteration: 910 Train loss: 0.479067 Train acc: 0.856667\n",
      "Epoch: 101/500 Iteration: 915 Train loss: 0.489509 Train acc: 0.861667\n",
      "Epoch: 102/500 Iteration: 920 Train loss: 0.517732 Train acc: 0.848333\n",
      "Epoch: 102/500 Iteration: 925 Train loss: 0.483355 Train acc: 0.833333\n",
      "Epoch: 102/500 Iteration: 925 Validation loss: 0.402217 Validation acc: 0.871667\n",
      "Epoch: 103/500 Iteration: 930 Train loss: 0.524014 Train acc: 0.846667\n",
      "Epoch: 103/500 Iteration: 935 Train loss: 0.548516 Train acc: 0.836667\n",
      "Epoch: 104/500 Iteration: 940 Train loss: 0.505968 Train acc: 0.861667\n",
      "Epoch: 104/500 Iteration: 945 Train loss: 0.495085 Train acc: 0.858333\n",
      "Epoch: 105/500 Iteration: 950 Train loss: 0.494395 Train acc: 0.845000\n",
      "Epoch: 105/500 Iteration: 950 Validation loss: 0.391972 Validation acc: 0.874167\n",
      "Epoch: 106/500 Iteration: 955 Train loss: 0.449232 Train acc: 0.873333\n",
      "Epoch: 106/500 Iteration: 960 Train loss: 0.484264 Train acc: 0.850000\n",
      "Epoch: 107/500 Iteration: 965 Train loss: 0.463374 Train acc: 0.856667\n",
      "Epoch: 107/500 Iteration: 970 Train loss: 0.461157 Train acc: 0.886667\n",
      "Epoch: 108/500 Iteration: 975 Train loss: 0.494430 Train acc: 0.863333\n",
      "Epoch: 108/500 Iteration: 975 Validation loss: 0.376432 Validation acc: 0.879167\n",
      "Epoch: 108/500 Iteration: 980 Train loss: 0.493442 Train acc: 0.856667\n",
      "Epoch: 109/500 Iteration: 985 Train loss: 0.513250 Train acc: 0.860000\n",
      "Epoch: 109/500 Iteration: 990 Train loss: 0.494442 Train acc: 0.870000\n",
      "Epoch: 110/500 Iteration: 995 Train loss: 0.454838 Train acc: 0.861667\n",
      "Epoch: 111/500 Iteration: 1000 Train loss: 0.426309 Train acc: 0.880000\n",
      "Epoch: 111/500 Iteration: 1000 Validation loss: 0.359269 Validation acc: 0.885833\n",
      "Epoch: 111/500 Iteration: 1005 Train loss: 0.481180 Train acc: 0.861667\n",
      "Epoch: 112/500 Iteration: 1010 Train loss: 0.460754 Train acc: 0.875000\n",
      "Epoch: 112/500 Iteration: 1015 Train loss: 0.423232 Train acc: 0.890000\n",
      "Epoch: 113/500 Iteration: 1020 Train loss: 0.471568 Train acc: 0.868333\n",
      "Epoch: 113/500 Iteration: 1025 Train loss: 0.473362 Train acc: 0.870000\n",
      "Epoch: 113/500 Iteration: 1025 Validation loss: 0.333172 Validation acc: 0.898333\n",
      "Epoch: 114/500 Iteration: 1030 Train loss: 0.441929 Train acc: 0.870000\n",
      "Epoch: 114/500 Iteration: 1035 Train loss: 0.489261 Train acc: 0.856667\n",
      "Epoch: 115/500 Iteration: 1040 Train loss: 0.456644 Train acc: 0.860000\n",
      "Epoch: 116/500 Iteration: 1045 Train loss: 0.398944 Train acc: 0.895000\n",
      "Epoch: 116/500 Iteration: 1050 Train loss: 0.392977 Train acc: 0.886667\n",
      "Epoch: 116/500 Iteration: 1050 Validation loss: 0.326047 Validation acc: 0.899167\n",
      "Epoch: 117/500 Iteration: 1055 Train loss: 0.430568 Train acc: 0.875000\n",
      "Epoch: 117/500 Iteration: 1060 Train loss: 0.385640 Train acc: 0.893333\n",
      "Epoch: 118/500 Iteration: 1065 Train loss: 0.427729 Train acc: 0.885000\n",
      "Epoch: 118/500 Iteration: 1070 Train loss: 0.453302 Train acc: 0.875000\n",
      "Epoch: 119/500 Iteration: 1075 Train loss: 0.437589 Train acc: 0.883333\n",
      "Epoch: 119/500 Iteration: 1075 Validation loss: 0.312747 Validation acc: 0.903333\n",
      "Epoch: 119/500 Iteration: 1080 Train loss: 0.407840 Train acc: 0.901667\n",
      "Epoch: 120/500 Iteration: 1085 Train loss: 0.421352 Train acc: 0.876667\n",
      "Epoch: 121/500 Iteration: 1090 Train loss: 0.392283 Train acc: 0.900000\n",
      "Epoch: 121/500 Iteration: 1095 Train loss: 0.373996 Train acc: 0.893333\n",
      "Epoch: 122/500 Iteration: 1100 Train loss: 0.382596 Train acc: 0.885000\n",
      "Epoch: 122/500 Iteration: 1100 Validation loss: 0.292749 Validation acc: 0.910833\n",
      "Epoch: 122/500 Iteration: 1105 Train loss: 0.404806 Train acc: 0.886667\n",
      "Epoch: 123/500 Iteration: 1110 Train loss: 0.399504 Train acc: 0.890000\n",
      "Epoch: 123/500 Iteration: 1115 Train loss: 0.409764 Train acc: 0.891667\n",
      "Epoch: 124/500 Iteration: 1120 Train loss: 0.407329 Train acc: 0.890000\n",
      "Epoch: 124/500 Iteration: 1125 Train loss: 0.383739 Train acc: 0.900000\n",
      "Epoch: 124/500 Iteration: 1125 Validation loss: 0.282002 Validation acc: 0.916667\n",
      "Epoch: 125/500 Iteration: 1130 Train loss: 0.385995 Train acc: 0.905000\n",
      "Epoch: 126/500 Iteration: 1135 Train loss: 0.326486 Train acc: 0.928333\n",
      "Epoch: 126/500 Iteration: 1140 Train loss: 0.374332 Train acc: 0.906667\n",
      "Epoch: 127/500 Iteration: 1145 Train loss: 0.379217 Train acc: 0.908333\n",
      "Epoch: 127/500 Iteration: 1150 Train loss: 0.369255 Train acc: 0.893333\n",
      "Epoch: 127/500 Iteration: 1150 Validation loss: 0.273761 Validation acc: 0.917500\n",
      "Epoch: 128/500 Iteration: 1155 Train loss: 0.364278 Train acc: 0.920000\n",
      "Epoch: 128/500 Iteration: 1160 Train loss: 0.442091 Train acc: 0.881667\n",
      "Epoch: 129/500 Iteration: 1165 Train loss: 0.440920 Train acc: 0.875000\n",
      "Epoch: 129/500 Iteration: 1170 Train loss: 0.387657 Train acc: 0.890000\n",
      "Epoch: 130/500 Iteration: 1175 Train loss: 0.398423 Train acc: 0.891667\n",
      "Epoch: 130/500 Iteration: 1175 Validation loss: 0.284144 Validation acc: 0.912500\n",
      "Epoch: 131/500 Iteration: 1180 Train loss: 0.321230 Train acc: 0.916667\n",
      "Epoch: 131/500 Iteration: 1185 Train loss: 0.319814 Train acc: 0.915000\n",
      "Epoch: 132/500 Iteration: 1190 Train loss: 0.361114 Train acc: 0.911667\n",
      "Epoch: 132/500 Iteration: 1195 Train loss: 0.329384 Train acc: 0.910000\n",
      "Epoch: 133/500 Iteration: 1200 Train loss: 0.338430 Train acc: 0.913333\n",
      "Epoch: 133/500 Iteration: 1200 Validation loss: 0.253931 Validation acc: 0.925833\n",
      "Epoch: 133/500 Iteration: 1205 Train loss: 0.352846 Train acc: 0.896667\n",
      "Epoch: 134/500 Iteration: 1210 Train loss: 0.357867 Train acc: 0.901667\n",
      "Epoch: 134/500 Iteration: 1215 Train loss: 0.353466 Train acc: 0.920000\n",
      "Epoch: 135/500 Iteration: 1220 Train loss: 0.341517 Train acc: 0.913333\n",
      "Epoch: 136/500 Iteration: 1225 Train loss: 0.318010 Train acc: 0.923333\n",
      "Epoch: 136/500 Iteration: 1225 Validation loss: 0.248623 Validation acc: 0.924167\n",
      "Epoch: 136/500 Iteration: 1230 Train loss: 0.341347 Train acc: 0.918333\n",
      "Epoch: 137/500 Iteration: 1235 Train loss: 0.339214 Train acc: 0.901667\n",
      "Epoch: 137/500 Iteration: 1240 Train loss: 0.323531 Train acc: 0.915000\n",
      "Epoch: 138/500 Iteration: 1245 Train loss: 0.351800 Train acc: 0.916667\n",
      "Epoch: 138/500 Iteration: 1250 Train loss: 0.350124 Train acc: 0.913333\n",
      "Epoch: 138/500 Iteration: 1250 Validation loss: 0.234949 Validation acc: 0.933333\n",
      "Epoch: 139/500 Iteration: 1255 Train loss: 0.366851 Train acc: 0.918333\n",
      "Epoch: 139/500 Iteration: 1260 Train loss: 0.309579 Train acc: 0.925000\n",
      "Epoch: 140/500 Iteration: 1265 Train loss: 0.358528 Train acc: 0.908333\n",
      "Epoch: 141/500 Iteration: 1270 Train loss: 0.296591 Train acc: 0.938333\n",
      "Epoch: 141/500 Iteration: 1275 Train loss: 0.360100 Train acc: 0.910000\n",
      "Epoch: 141/500 Iteration: 1275 Validation loss: 0.230620 Validation acc: 0.933333\n",
      "Epoch: 142/500 Iteration: 1280 Train loss: 0.356832 Train acc: 0.906667\n",
      "Epoch: 142/500 Iteration: 1285 Train loss: 0.305780 Train acc: 0.925000\n",
      "Epoch: 143/500 Iteration: 1290 Train loss: 0.322810 Train acc: 0.920000\n",
      "Epoch: 143/500 Iteration: 1295 Train loss: 0.335290 Train acc: 0.908333\n",
      "Epoch: 144/500 Iteration: 1300 Train loss: 0.331748 Train acc: 0.921667\n",
      "Epoch: 144/500 Iteration: 1300 Validation loss: 0.227844 Validation acc: 0.933333\n",
      "Epoch: 144/500 Iteration: 1305 Train loss: 0.336216 Train acc: 0.915000\n",
      "Epoch: 145/500 Iteration: 1310 Train loss: 0.340947 Train acc: 0.921667\n",
      "Epoch: 146/500 Iteration: 1315 Train loss: 0.285869 Train acc: 0.933333\n",
      "Epoch: 146/500 Iteration: 1320 Train loss: 0.300745 Train acc: 0.923333\n",
      "Epoch: 147/500 Iteration: 1325 Train loss: 0.334657 Train acc: 0.923333\n",
      "Epoch: 147/500 Iteration: 1325 Validation loss: 0.226871 Validation acc: 0.932500\n",
      "Epoch: 147/500 Iteration: 1330 Train loss: 0.284844 Train acc: 0.925000\n",
      "Epoch: 148/500 Iteration: 1335 Train loss: 0.305510 Train acc: 0.925000\n",
      "Epoch: 148/500 Iteration: 1340 Train loss: 0.338871 Train acc: 0.913333\n",
      "Epoch: 149/500 Iteration: 1345 Train loss: 0.330457 Train acc: 0.915000\n",
      "Epoch: 149/500 Iteration: 1350 Train loss: 0.313130 Train acc: 0.920000\n",
      "Epoch: 149/500 Iteration: 1350 Validation loss: 0.208644 Validation acc: 0.935000\n",
      "Epoch: 150/500 Iteration: 1355 Train loss: 0.334431 Train acc: 0.911667\n",
      "Epoch: 151/500 Iteration: 1360 Train loss: 0.283512 Train acc: 0.923333\n",
      "Epoch: 151/500 Iteration: 1365 Train loss: 0.299716 Train acc: 0.916667\n",
      "Epoch: 152/500 Iteration: 1370 Train loss: 0.315019 Train acc: 0.920000\n",
      "Epoch: 152/500 Iteration: 1375 Train loss: 0.301686 Train acc: 0.930000\n",
      "Epoch: 152/500 Iteration: 1375 Validation loss: 0.229450 Validation acc: 0.926667\n",
      "Epoch: 153/500 Iteration: 1380 Train loss: 0.283361 Train acc: 0.925000\n",
      "Epoch: 153/500 Iteration: 1385 Train loss: 0.328061 Train acc: 0.915000\n",
      "Epoch: 154/500 Iteration: 1390 Train loss: 0.316047 Train acc: 0.915000\n",
      "Epoch: 154/500 Iteration: 1395 Train loss: 0.290261 Train acc: 0.936667\n",
      "Epoch: 155/500 Iteration: 1400 Train loss: 0.299775 Train acc: 0.925000\n",
      "Epoch: 155/500 Iteration: 1400 Validation loss: 0.207817 Validation acc: 0.934167\n",
      "Epoch: 156/500 Iteration: 1405 Train loss: 0.280653 Train acc: 0.931667\n",
      "Epoch: 156/500 Iteration: 1410 Train loss: 0.272597 Train acc: 0.936667\n",
      "Epoch: 157/500 Iteration: 1415 Train loss: 0.319519 Train acc: 0.925000\n",
      "Epoch: 157/500 Iteration: 1420 Train loss: 0.266888 Train acc: 0.931667\n",
      "Epoch: 158/500 Iteration: 1425 Train loss: 0.304407 Train acc: 0.921667\n",
      "Epoch: 158/500 Iteration: 1425 Validation loss: 0.201445 Validation acc: 0.938333\n",
      "Epoch: 158/500 Iteration: 1430 Train loss: 0.295087 Train acc: 0.923333\n",
      "Epoch: 159/500 Iteration: 1435 Train loss: 0.312713 Train acc: 0.921667\n",
      "Epoch: 159/500 Iteration: 1440 Train loss: 0.291348 Train acc: 0.915000\n",
      "Epoch: 160/500 Iteration: 1445 Train loss: 0.317531 Train acc: 0.918333\n",
      "Epoch: 161/500 Iteration: 1450 Train loss: 0.251031 Train acc: 0.950000\n",
      "Epoch: 161/500 Iteration: 1450 Validation loss: 0.194539 Validation acc: 0.939167\n",
      "Epoch: 161/500 Iteration: 1455 Train loss: 0.259548 Train acc: 0.931667\n",
      "Epoch: 162/500 Iteration: 1460 Train loss: 0.292954 Train acc: 0.928333\n",
      "Epoch: 162/500 Iteration: 1465 Train loss: 0.266570 Train acc: 0.933333\n",
      "Epoch: 163/500 Iteration: 1470 Train loss: 0.288688 Train acc: 0.926667\n",
      "Epoch: 163/500 Iteration: 1475 Train loss: 0.313605 Train acc: 0.916667\n",
      "Epoch: 163/500 Iteration: 1475 Validation loss: 0.186480 Validation acc: 0.943333\n",
      "Epoch: 164/500 Iteration: 1480 Train loss: 0.302103 Train acc: 0.935000\n",
      "Epoch: 164/500 Iteration: 1485 Train loss: 0.295393 Train acc: 0.931667\n",
      "Epoch: 165/500 Iteration: 1490 Train loss: 0.266786 Train acc: 0.925000\n",
      "Epoch: 166/500 Iteration: 1495 Train loss: 0.224685 Train acc: 0.948333\n",
      "Epoch: 166/500 Iteration: 1500 Train loss: 0.256187 Train acc: 0.933333\n",
      "Epoch: 166/500 Iteration: 1500 Validation loss: 0.197090 Validation acc: 0.934167\n",
      "Epoch: 167/500 Iteration: 1505 Train loss: 0.315775 Train acc: 0.923333\n",
      "Epoch: 167/500 Iteration: 1510 Train loss: 0.265421 Train acc: 0.936667\n",
      "Epoch: 168/500 Iteration: 1515 Train loss: 0.265724 Train acc: 0.926667\n",
      "Epoch: 168/500 Iteration: 1520 Train loss: 0.310344 Train acc: 0.923333\n",
      "Epoch: 169/500 Iteration: 1525 Train loss: 0.275096 Train acc: 0.931667\n",
      "Epoch: 169/500 Iteration: 1525 Validation loss: 0.178515 Validation acc: 0.944167\n",
      "Epoch: 169/500 Iteration: 1530 Train loss: 0.247696 Train acc: 0.938333\n",
      "Epoch: 170/500 Iteration: 1535 Train loss: 0.281269 Train acc: 0.926667\n",
      "Epoch: 171/500 Iteration: 1540 Train loss: 0.248229 Train acc: 0.940000\n",
      "Epoch: 171/500 Iteration: 1545 Train loss: 0.251147 Train acc: 0.943333\n",
      "Epoch: 172/500 Iteration: 1550 Train loss: 0.287536 Train acc: 0.928333\n",
      "Epoch: 172/500 Iteration: 1550 Validation loss: 0.177313 Validation acc: 0.945000\n",
      "Epoch: 172/500 Iteration: 1555 Train loss: 0.257079 Train acc: 0.931667\n",
      "Epoch: 173/500 Iteration: 1560 Train loss: 0.245767 Train acc: 0.941667\n",
      "Epoch: 173/500 Iteration: 1565 Train loss: 0.257910 Train acc: 0.931667\n",
      "Epoch: 174/500 Iteration: 1570 Train loss: 0.281708 Train acc: 0.916667\n",
      "Epoch: 174/500 Iteration: 1575 Train loss: 0.259379 Train acc: 0.933333\n",
      "Epoch: 174/500 Iteration: 1575 Validation loss: 0.181652 Validation acc: 0.941667\n",
      "Epoch: 175/500 Iteration: 1580 Train loss: 0.291819 Train acc: 0.920000\n",
      "Epoch: 176/500 Iteration: 1585 Train loss: 0.251998 Train acc: 0.938333\n",
      "Epoch: 176/500 Iteration: 1590 Train loss: 0.244365 Train acc: 0.930000\n",
      "Epoch: 177/500 Iteration: 1595 Train loss: 0.274007 Train acc: 0.918333\n",
      "Epoch: 177/500 Iteration: 1600 Train loss: 0.256481 Train acc: 0.923333\n",
      "Epoch: 177/500 Iteration: 1600 Validation loss: 0.173232 Validation acc: 0.946667\n",
      "Epoch: 178/500 Iteration: 1605 Train loss: 0.235401 Train acc: 0.936667\n",
      "Epoch: 178/500 Iteration: 1610 Train loss: 0.292349 Train acc: 0.926667\n",
      "Epoch: 179/500 Iteration: 1615 Train loss: 0.278197 Train acc: 0.925000\n",
      "Epoch: 179/500 Iteration: 1620 Train loss: 0.272010 Train acc: 0.921667\n",
      "Epoch: 180/500 Iteration: 1625 Train loss: 0.268874 Train acc: 0.928333\n",
      "Epoch: 180/500 Iteration: 1625 Validation loss: 0.174343 Validation acc: 0.945000\n",
      "Epoch: 181/500 Iteration: 1630 Train loss: 0.233784 Train acc: 0.936667\n",
      "Epoch: 181/500 Iteration: 1635 Train loss: 0.232147 Train acc: 0.940000\n",
      "Epoch: 182/500 Iteration: 1640 Train loss: 0.274209 Train acc: 0.918333\n",
      "Epoch: 182/500 Iteration: 1645 Train loss: 0.245103 Train acc: 0.928333\n",
      "Epoch: 183/500 Iteration: 1650 Train loss: 0.247137 Train acc: 0.920000\n",
      "Epoch: 183/500 Iteration: 1650 Validation loss: 0.174029 Validation acc: 0.941667\n",
      "Epoch: 183/500 Iteration: 1655 Train loss: 0.253694 Train acc: 0.933333\n",
      "Epoch: 184/500 Iteration: 1660 Train loss: 0.241337 Train acc: 0.946667\n",
      "Epoch: 184/500 Iteration: 1665 Train loss: 0.247881 Train acc: 0.926667\n",
      "Epoch: 185/500 Iteration: 1670 Train loss: 0.256527 Train acc: 0.936667\n",
      "Epoch: 186/500 Iteration: 1675 Train loss: 0.230330 Train acc: 0.945000\n",
      "Epoch: 186/500 Iteration: 1675 Validation loss: 0.168062 Validation acc: 0.946667\n",
      "Epoch: 186/500 Iteration: 1680 Train loss: 0.206661 Train acc: 0.955000\n",
      "Epoch: 187/500 Iteration: 1685 Train loss: 0.259177 Train acc: 0.923333\n",
      "Epoch: 187/500 Iteration: 1690 Train loss: 0.216176 Train acc: 0.941667\n",
      "Epoch: 188/500 Iteration: 1695 Train loss: 0.239061 Train acc: 0.931667\n",
      "Epoch: 188/500 Iteration: 1700 Train loss: 0.253781 Train acc: 0.926667\n",
      "Epoch: 188/500 Iteration: 1700 Validation loss: 0.167511 Validation acc: 0.944167\n",
      "Epoch: 189/500 Iteration: 1705 Train loss: 0.242815 Train acc: 0.940000\n",
      "Epoch: 189/500 Iteration: 1710 Train loss: 0.257156 Train acc: 0.938333\n",
      "Epoch: 190/500 Iteration: 1715 Train loss: 0.277664 Train acc: 0.923333\n",
      "Epoch: 191/500 Iteration: 1720 Train loss: 0.226048 Train acc: 0.940000\n",
      "Epoch: 191/500 Iteration: 1725 Train loss: 0.239772 Train acc: 0.948333\n",
      "Epoch: 191/500 Iteration: 1725 Validation loss: 0.167765 Validation acc: 0.943333\n",
      "Epoch: 192/500 Iteration: 1730 Train loss: 0.261771 Train acc: 0.933333\n",
      "Epoch: 192/500 Iteration: 1735 Train loss: 0.212585 Train acc: 0.943333\n",
      "Epoch: 193/500 Iteration: 1740 Train loss: 0.239979 Train acc: 0.941667\n",
      "Epoch: 193/500 Iteration: 1745 Train loss: 0.229888 Train acc: 0.933333\n",
      "Epoch: 194/500 Iteration: 1750 Train loss: 0.253908 Train acc: 0.933333\n",
      "Epoch: 194/500 Iteration: 1750 Validation loss: 0.168123 Validation acc: 0.943333\n",
      "Epoch: 194/500 Iteration: 1755 Train loss: 0.232740 Train acc: 0.940000\n",
      "Epoch: 195/500 Iteration: 1760 Train loss: 0.251269 Train acc: 0.928333\n",
      "Epoch: 196/500 Iteration: 1765 Train loss: 0.207144 Train acc: 0.961667\n",
      "Epoch: 196/500 Iteration: 1770 Train loss: 0.219424 Train acc: 0.935000\n",
      "Epoch: 197/500 Iteration: 1775 Train loss: 0.257758 Train acc: 0.935000\n",
      "Epoch: 197/500 Iteration: 1775 Validation loss: 0.157717 Validation acc: 0.948333\n",
      "Epoch: 197/500 Iteration: 1780 Train loss: 0.234060 Train acc: 0.931667\n",
      "Epoch: 198/500 Iteration: 1785 Train loss: 0.223940 Train acc: 0.940000\n",
      "Epoch: 198/500 Iteration: 1790 Train loss: 0.248448 Train acc: 0.931667\n",
      "Epoch: 199/500 Iteration: 1795 Train loss: 0.246077 Train acc: 0.936667\n",
      "Epoch: 199/500 Iteration: 1800 Train loss: 0.240232 Train acc: 0.936667\n",
      "Epoch: 199/500 Iteration: 1800 Validation loss: 0.166523 Validation acc: 0.945000\n",
      "Epoch: 200/500 Iteration: 1805 Train loss: 0.240771 Train acc: 0.935000\n",
      "Epoch: 201/500 Iteration: 1810 Train loss: 0.239112 Train acc: 0.938333\n",
      "Epoch: 201/500 Iteration: 1815 Train loss: 0.206093 Train acc: 0.948333\n",
      "Epoch: 202/500 Iteration: 1820 Train loss: 0.254831 Train acc: 0.938333\n",
      "Epoch: 202/500 Iteration: 1825 Train loss: 0.229385 Train acc: 0.943333\n",
      "Epoch: 202/500 Iteration: 1825 Validation loss: 0.171742 Validation acc: 0.942500\n",
      "Epoch: 203/500 Iteration: 1830 Train loss: 0.225660 Train acc: 0.936667\n",
      "Epoch: 203/500 Iteration: 1835 Train loss: 0.224500 Train acc: 0.943333\n",
      "Epoch: 204/500 Iteration: 1840 Train loss: 0.230797 Train acc: 0.945000\n",
      "Epoch: 204/500 Iteration: 1845 Train loss: 0.239279 Train acc: 0.936667\n",
      "Epoch: 205/500 Iteration: 1850 Train loss: 0.251736 Train acc: 0.941667\n",
      "Epoch: 205/500 Iteration: 1850 Validation loss: 0.166080 Validation acc: 0.946667\n",
      "Epoch: 206/500 Iteration: 1855 Train loss: 0.223892 Train acc: 0.933333\n",
      "Epoch: 206/500 Iteration: 1860 Train loss: 0.211778 Train acc: 0.935000\n",
      "Epoch: 207/500 Iteration: 1865 Train loss: 0.222563 Train acc: 0.938333\n",
      "Epoch: 207/500 Iteration: 1870 Train loss: 0.223698 Train acc: 0.945000\n",
      "Epoch: 208/500 Iteration: 1875 Train loss: 0.229115 Train acc: 0.941667\n",
      "Epoch: 208/500 Iteration: 1875 Validation loss: 0.158175 Validation acc: 0.946667\n",
      "Epoch: 208/500 Iteration: 1880 Train loss: 0.279313 Train acc: 0.925000\n",
      "Epoch: 209/500 Iteration: 1885 Train loss: 0.223359 Train acc: 0.941667\n",
      "Epoch: 209/500 Iteration: 1890 Train loss: 0.232228 Train acc: 0.936667\n",
      "Epoch: 210/500 Iteration: 1895 Train loss: 0.234898 Train acc: 0.928333\n",
      "Epoch: 211/500 Iteration: 1900 Train loss: 0.191933 Train acc: 0.948333\n",
      "Epoch: 211/500 Iteration: 1900 Validation loss: 0.155304 Validation acc: 0.950000\n",
      "Epoch: 211/500 Iteration: 1905 Train loss: 0.197704 Train acc: 0.938333\n",
      "Epoch: 212/500 Iteration: 1910 Train loss: 0.242650 Train acc: 0.936667\n",
      "Epoch: 212/500 Iteration: 1915 Train loss: 0.220609 Train acc: 0.933333\n",
      "Epoch: 213/500 Iteration: 1920 Train loss: 0.213598 Train acc: 0.945000\n",
      "Epoch: 213/500 Iteration: 1925 Train loss: 0.245880 Train acc: 0.935000\n",
      "Epoch: 213/500 Iteration: 1925 Validation loss: 0.158448 Validation acc: 0.946667\n",
      "Epoch: 214/500 Iteration: 1930 Train loss: 0.211843 Train acc: 0.948333\n",
      "Epoch: 214/500 Iteration: 1935 Train loss: 0.205986 Train acc: 0.936667\n",
      "Epoch: 215/500 Iteration: 1940 Train loss: 0.239481 Train acc: 0.930000\n",
      "Epoch: 216/500 Iteration: 1945 Train loss: 0.226345 Train acc: 0.935000\n",
      "Epoch: 216/500 Iteration: 1950 Train loss: 0.191676 Train acc: 0.943333\n",
      "Epoch: 216/500 Iteration: 1950 Validation loss: 0.165097 Validation acc: 0.942500\n",
      "Epoch: 217/500 Iteration: 1955 Train loss: 0.241683 Train acc: 0.928333\n",
      "Epoch: 217/500 Iteration: 1960 Train loss: 0.206241 Train acc: 0.931667\n",
      "Epoch: 218/500 Iteration: 1965 Train loss: 0.216495 Train acc: 0.946667\n",
      "Epoch: 218/500 Iteration: 1970 Train loss: 0.254689 Train acc: 0.930000\n",
      "Epoch: 219/500 Iteration: 1975 Train loss: 0.201311 Train acc: 0.953333\n",
      "Epoch: 219/500 Iteration: 1975 Validation loss: 0.154733 Validation acc: 0.945000\n",
      "Epoch: 219/500 Iteration: 1980 Train loss: 0.221690 Train acc: 0.933333\n",
      "Epoch: 220/500 Iteration: 1985 Train loss: 0.220020 Train acc: 0.940000\n",
      "Epoch: 221/500 Iteration: 1990 Train loss: 0.211465 Train acc: 0.935000\n",
      "Epoch: 221/500 Iteration: 1995 Train loss: 0.189811 Train acc: 0.953333\n",
      "Epoch: 222/500 Iteration: 2000 Train loss: 0.212843 Train acc: 0.930000\n",
      "Epoch: 222/500 Iteration: 2000 Validation loss: 0.150525 Validation acc: 0.949167\n",
      "Epoch: 222/500 Iteration: 2005 Train loss: 0.191965 Train acc: 0.940000\n",
      "Epoch: 223/500 Iteration: 2010 Train loss: 0.207644 Train acc: 0.946667\n",
      "Epoch: 223/500 Iteration: 2015 Train loss: 0.233033 Train acc: 0.925000\n",
      "Epoch: 224/500 Iteration: 2020 Train loss: 0.207537 Train acc: 0.940000\n",
      "Epoch: 224/500 Iteration: 2025 Train loss: 0.214891 Train acc: 0.938333\n",
      "Epoch: 224/500 Iteration: 2025 Validation loss: 0.161154 Validation acc: 0.945000\n",
      "Epoch: 225/500 Iteration: 2030 Train loss: 0.227156 Train acc: 0.940000\n",
      "Epoch: 226/500 Iteration: 2035 Train loss: 0.173843 Train acc: 0.965000\n",
      "Epoch: 226/500 Iteration: 2040 Train loss: 0.199354 Train acc: 0.943333\n",
      "Epoch: 227/500 Iteration: 2045 Train loss: 0.226058 Train acc: 0.940000\n",
      "Epoch: 227/500 Iteration: 2050 Train loss: 0.202849 Train acc: 0.938333\n",
      "Epoch: 227/500 Iteration: 2050 Validation loss: 0.145039 Validation acc: 0.950833\n",
      "Epoch: 228/500 Iteration: 2055 Train loss: 0.212638 Train acc: 0.935000\n",
      "Epoch: 228/500 Iteration: 2060 Train loss: 0.248628 Train acc: 0.933333\n",
      "Epoch: 229/500 Iteration: 2065 Train loss: 0.211975 Train acc: 0.953333\n",
      "Epoch: 229/500 Iteration: 2070 Train loss: 0.239648 Train acc: 0.925000\n",
      "Epoch: 230/500 Iteration: 2075 Train loss: 0.235276 Train acc: 0.930000\n",
      "Epoch: 230/500 Iteration: 2075 Validation loss: 0.142425 Validation acc: 0.952500\n",
      "Epoch: 231/500 Iteration: 2080 Train loss: 0.176544 Train acc: 0.955000\n",
      "Epoch: 231/500 Iteration: 2085 Train loss: 0.208327 Train acc: 0.940000\n",
      "Epoch: 232/500 Iteration: 2090 Train loss: 0.219827 Train acc: 0.943333\n",
      "Epoch: 232/500 Iteration: 2095 Train loss: 0.206022 Train acc: 0.955000\n",
      "Epoch: 233/500 Iteration: 2100 Train loss: 0.203484 Train acc: 0.943333\n",
      "Epoch: 233/500 Iteration: 2100 Validation loss: 0.146916 Validation acc: 0.950000\n",
      "Epoch: 233/500 Iteration: 2105 Train loss: 0.227343 Train acc: 0.935000\n",
      "Epoch: 234/500 Iteration: 2110 Train loss: 0.198731 Train acc: 0.945000\n",
      "Epoch: 234/500 Iteration: 2115 Train loss: 0.215708 Train acc: 0.945000\n",
      "Epoch: 235/500 Iteration: 2120 Train loss: 0.214780 Train acc: 0.936667\n",
      "Epoch: 236/500 Iteration: 2125 Train loss: 0.183592 Train acc: 0.953333\n",
      "Epoch: 236/500 Iteration: 2125 Validation loss: 0.146771 Validation acc: 0.949167\n",
      "Epoch: 236/500 Iteration: 2130 Train loss: 0.211889 Train acc: 0.940000\n",
      "Epoch: 237/500 Iteration: 2135 Train loss: 0.214877 Train acc: 0.935000\n",
      "Epoch: 237/500 Iteration: 2140 Train loss: 0.192824 Train acc: 0.941667\n",
      "Epoch: 238/500 Iteration: 2145 Train loss: 0.210167 Train acc: 0.951667\n",
      "Epoch: 238/500 Iteration: 2150 Train loss: 0.213588 Train acc: 0.943333\n",
      "Epoch: 238/500 Iteration: 2150 Validation loss: 0.150195 Validation acc: 0.947500\n",
      "Epoch: 239/500 Iteration: 2155 Train loss: 0.205076 Train acc: 0.941667\n",
      "Epoch: 239/500 Iteration: 2160 Train loss: 0.222113 Train acc: 0.933333\n",
      "Epoch: 240/500 Iteration: 2165 Train loss: 0.200832 Train acc: 0.948333\n",
      "Epoch: 241/500 Iteration: 2170 Train loss: 0.187315 Train acc: 0.950000\n",
      "Epoch: 241/500 Iteration: 2175 Train loss: 0.179179 Train acc: 0.948333\n",
      "Epoch: 241/500 Iteration: 2175 Validation loss: 0.141856 Validation acc: 0.952500\n",
      "Epoch: 242/500 Iteration: 2180 Train loss: 0.213044 Train acc: 0.935000\n",
      "Epoch: 242/500 Iteration: 2185 Train loss: 0.184762 Train acc: 0.948333\n",
      "Epoch: 243/500 Iteration: 2190 Train loss: 0.183992 Train acc: 0.946667\n",
      "Epoch: 243/500 Iteration: 2195 Train loss: 0.218074 Train acc: 0.943333\n",
      "Epoch: 244/500 Iteration: 2200 Train loss: 0.195546 Train acc: 0.950000\n",
      "Epoch: 244/500 Iteration: 2200 Validation loss: 0.147486 Validation acc: 0.947500\n",
      "Epoch: 244/500 Iteration: 2205 Train loss: 0.199167 Train acc: 0.936667\n",
      "Epoch: 245/500 Iteration: 2210 Train loss: 0.209383 Train acc: 0.938333\n",
      "Epoch: 246/500 Iteration: 2215 Train loss: 0.159514 Train acc: 0.951667\n",
      "Epoch: 246/500 Iteration: 2220 Train loss: 0.192919 Train acc: 0.941667\n",
      "Epoch: 247/500 Iteration: 2225 Train loss: 0.197917 Train acc: 0.943333\n",
      "Epoch: 247/500 Iteration: 2225 Validation loss: 0.148034 Validation acc: 0.950000\n",
      "Epoch: 247/500 Iteration: 2230 Train loss: 0.179618 Train acc: 0.945000\n",
      "Epoch: 248/500 Iteration: 2235 Train loss: 0.191282 Train acc: 0.943333\n",
      "Epoch: 248/500 Iteration: 2240 Train loss: 0.230929 Train acc: 0.935000\n",
      "Epoch: 249/500 Iteration: 2245 Train loss: 0.198110 Train acc: 0.945000\n",
      "Epoch: 249/500 Iteration: 2250 Train loss: 0.222551 Train acc: 0.933333\n",
      "Epoch: 249/500 Iteration: 2250 Validation loss: 0.139697 Validation acc: 0.952500\n",
      "Epoch: 250/500 Iteration: 2255 Train loss: 0.223285 Train acc: 0.933333\n",
      "Epoch: 251/500 Iteration: 2260 Train loss: 0.166962 Train acc: 0.950000\n",
      "Epoch: 251/500 Iteration: 2265 Train loss: 0.169402 Train acc: 0.950000\n",
      "Epoch: 252/500 Iteration: 2270 Train loss: 0.196963 Train acc: 0.940000\n",
      "Epoch: 252/500 Iteration: 2275 Train loss: 0.202858 Train acc: 0.940000\n",
      "Epoch: 252/500 Iteration: 2275 Validation loss: 0.146436 Validation acc: 0.949167\n",
      "Epoch: 253/500 Iteration: 2280 Train loss: 0.209984 Train acc: 0.940000\n",
      "Epoch: 253/500 Iteration: 2285 Train loss: 0.218350 Train acc: 0.930000\n",
      "Epoch: 254/500 Iteration: 2290 Train loss: 0.185648 Train acc: 0.956667\n",
      "Epoch: 254/500 Iteration: 2295 Train loss: 0.199771 Train acc: 0.945000\n",
      "Epoch: 255/500 Iteration: 2300 Train loss: 0.217552 Train acc: 0.936667\n",
      "Epoch: 255/500 Iteration: 2300 Validation loss: 0.138511 Validation acc: 0.951667\n",
      "Epoch: 256/500 Iteration: 2305 Train loss: 0.194590 Train acc: 0.941667\n",
      "Epoch: 256/500 Iteration: 2310 Train loss: 0.201241 Train acc: 0.936667\n",
      "Epoch: 257/500 Iteration: 2315 Train loss: 0.206566 Train acc: 0.936667\n",
      "Epoch: 257/500 Iteration: 2320 Train loss: 0.197806 Train acc: 0.925000\n",
      "Epoch: 258/500 Iteration: 2325 Train loss: 0.192993 Train acc: 0.940000\n",
      "Epoch: 258/500 Iteration: 2325 Validation loss: 0.143009 Validation acc: 0.950833\n",
      "Epoch: 258/500 Iteration: 2330 Train loss: 0.223911 Train acc: 0.928333\n",
      "Epoch: 259/500 Iteration: 2335 Train loss: 0.187685 Train acc: 0.950000\n",
      "Epoch: 259/500 Iteration: 2340 Train loss: 0.202685 Train acc: 0.943333\n",
      "Epoch: 260/500 Iteration: 2345 Train loss: 0.196828 Train acc: 0.940000\n",
      "Epoch: 261/500 Iteration: 2350 Train loss: 0.181324 Train acc: 0.945000\n",
      "Epoch: 261/500 Iteration: 2350 Validation loss: 0.148213 Validation acc: 0.946667\n",
      "Epoch: 261/500 Iteration: 2355 Train loss: 0.175944 Train acc: 0.951667\n",
      "Epoch: 262/500 Iteration: 2360 Train loss: 0.192463 Train acc: 0.943333\n",
      "Epoch: 262/500 Iteration: 2365 Train loss: 0.179107 Train acc: 0.950000\n",
      "Epoch: 263/500 Iteration: 2370 Train loss: 0.183078 Train acc: 0.946667\n",
      "Epoch: 263/500 Iteration: 2375 Train loss: 0.216305 Train acc: 0.930000\n",
      "Epoch: 263/500 Iteration: 2375 Validation loss: 0.147839 Validation acc: 0.949167\n",
      "Epoch: 264/500 Iteration: 2380 Train loss: 0.169513 Train acc: 0.958333\n",
      "Epoch: 264/500 Iteration: 2385 Train loss: 0.205293 Train acc: 0.940000\n",
      "Epoch: 265/500 Iteration: 2390 Train loss: 0.209627 Train acc: 0.938333\n",
      "Epoch: 266/500 Iteration: 2395 Train loss: 0.172844 Train acc: 0.945000\n",
      "Epoch: 266/500 Iteration: 2400 Train loss: 0.186647 Train acc: 0.935000\n",
      "Epoch: 266/500 Iteration: 2400 Validation loss: 0.134977 Validation acc: 0.953333\n",
      "Epoch: 267/500 Iteration: 2405 Train loss: 0.204722 Train acc: 0.940000\n",
      "Epoch: 267/500 Iteration: 2410 Train loss: 0.212705 Train acc: 0.940000\n",
      "Epoch: 268/500 Iteration: 2415 Train loss: 0.193982 Train acc: 0.950000\n",
      "Epoch: 268/500 Iteration: 2420 Train loss: 0.212611 Train acc: 0.943333\n",
      "Epoch: 269/500 Iteration: 2425 Train loss: 0.192041 Train acc: 0.938333\n",
      "Epoch: 269/500 Iteration: 2425 Validation loss: 0.136399 Validation acc: 0.951667\n",
      "Epoch: 269/500 Iteration: 2430 Train loss: 0.198756 Train acc: 0.946667\n",
      "Epoch: 270/500 Iteration: 2435 Train loss: 0.193556 Train acc: 0.938333\n",
      "Epoch: 271/500 Iteration: 2440 Train loss: 0.175233 Train acc: 0.956667\n",
      "Epoch: 271/500 Iteration: 2445 Train loss: 0.177691 Train acc: 0.948333\n",
      "Epoch: 272/500 Iteration: 2450 Train loss: 0.214631 Train acc: 0.926667\n",
      "Epoch: 272/500 Iteration: 2450 Validation loss: 0.141585 Validation acc: 0.950000\n",
      "Epoch: 272/500 Iteration: 2455 Train loss: 0.198998 Train acc: 0.936667\n",
      "Epoch: 273/500 Iteration: 2460 Train loss: 0.177270 Train acc: 0.950000\n",
      "Epoch: 273/500 Iteration: 2465 Train loss: 0.191597 Train acc: 0.958333\n",
      "Epoch: 274/500 Iteration: 2470 Train loss: 0.192162 Train acc: 0.953333\n",
      "Epoch: 274/500 Iteration: 2475 Train loss: 0.224741 Train acc: 0.940000\n",
      "Epoch: 274/500 Iteration: 2475 Validation loss: 0.141567 Validation acc: 0.950833\n",
      "Epoch: 275/500 Iteration: 2480 Train loss: 0.235989 Train acc: 0.931667\n",
      "Epoch: 276/500 Iteration: 2485 Train loss: 0.188782 Train acc: 0.948333\n",
      "Epoch: 276/500 Iteration: 2490 Train loss: 0.186608 Train acc: 0.948333\n",
      "Epoch: 277/500 Iteration: 2495 Train loss: 0.198464 Train acc: 0.941667\n",
      "Epoch: 277/500 Iteration: 2500 Train loss: 0.192987 Train acc: 0.943333\n",
      "Epoch: 277/500 Iteration: 2500 Validation loss: 0.140805 Validation acc: 0.949167\n",
      "Epoch: 278/500 Iteration: 2505 Train loss: 0.183156 Train acc: 0.953333\n",
      "Epoch: 278/500 Iteration: 2510 Train loss: 0.201525 Train acc: 0.941667\n",
      "Epoch: 279/500 Iteration: 2515 Train loss: 0.202630 Train acc: 0.941667\n",
      "Epoch: 279/500 Iteration: 2520 Train loss: 0.210970 Train acc: 0.941667\n",
      "Epoch: 280/500 Iteration: 2525 Train loss: 0.201392 Train acc: 0.936667\n",
      "Epoch: 280/500 Iteration: 2525 Validation loss: 0.132973 Validation acc: 0.953333\n",
      "Epoch: 281/500 Iteration: 2530 Train loss: 0.161837 Train acc: 0.961667\n",
      "Epoch: 281/500 Iteration: 2535 Train loss: 0.179545 Train acc: 0.948333\n",
      "Epoch: 282/500 Iteration: 2540 Train loss: 0.202958 Train acc: 0.941667\n",
      "Epoch: 282/500 Iteration: 2545 Train loss: 0.192302 Train acc: 0.935000\n",
      "Epoch: 283/500 Iteration: 2550 Train loss: 0.191652 Train acc: 0.943333\n",
      "Epoch: 283/500 Iteration: 2550 Validation loss: 0.130775 Validation acc: 0.955000\n",
      "Epoch: 283/500 Iteration: 2555 Train loss: 0.192508 Train acc: 0.938333\n",
      "Epoch: 284/500 Iteration: 2560 Train loss: 0.182927 Train acc: 0.941667\n",
      "Epoch: 284/500 Iteration: 2565 Train loss: 0.194721 Train acc: 0.941667\n",
      "Epoch: 285/500 Iteration: 2570 Train loss: 0.177563 Train acc: 0.950000\n",
      "Epoch: 286/500 Iteration: 2575 Train loss: 0.169663 Train acc: 0.955000\n",
      "Epoch: 286/500 Iteration: 2575 Validation loss: 0.138593 Validation acc: 0.949167\n",
      "Epoch: 286/500 Iteration: 2580 Train loss: 0.171744 Train acc: 0.950000\n",
      "Epoch: 287/500 Iteration: 2585 Train loss: 0.226497 Train acc: 0.933333\n",
      "Epoch: 287/500 Iteration: 2590 Train loss: 0.205799 Train acc: 0.933333\n",
      "Epoch: 288/500 Iteration: 2595 Train loss: 0.175400 Train acc: 0.943333\n",
      "Epoch: 288/500 Iteration: 2600 Train loss: 0.191565 Train acc: 0.940000\n",
      "Epoch: 288/500 Iteration: 2600 Validation loss: 0.133261 Validation acc: 0.953333\n",
      "Epoch: 289/500 Iteration: 2605 Train loss: 0.183063 Train acc: 0.951667\n",
      "Epoch: 289/500 Iteration: 2610 Train loss: 0.189671 Train acc: 0.936667\n",
      "Epoch: 290/500 Iteration: 2615 Train loss: 0.192634 Train acc: 0.941667\n",
      "Epoch: 291/500 Iteration: 2620 Train loss: 0.163601 Train acc: 0.951667\n",
      "Epoch: 291/500 Iteration: 2625 Train loss: 0.168977 Train acc: 0.951667\n",
      "Epoch: 291/500 Iteration: 2625 Validation loss: 0.131470 Validation acc: 0.951667\n",
      "Epoch: 292/500 Iteration: 2630 Train loss: 0.177833 Train acc: 0.950000\n",
      "Epoch: 292/500 Iteration: 2635 Train loss: 0.180356 Train acc: 0.936667\n",
      "Epoch: 293/500 Iteration: 2640 Train loss: 0.186937 Train acc: 0.936667\n",
      "Epoch: 293/500 Iteration: 2645 Train loss: 0.221343 Train acc: 0.938333\n",
      "Epoch: 294/500 Iteration: 2650 Train loss: 0.182693 Train acc: 0.943333\n",
      "Epoch: 294/500 Iteration: 2650 Validation loss: 0.132764 Validation acc: 0.953333\n",
      "Epoch: 294/500 Iteration: 2655 Train loss: 0.182681 Train acc: 0.941667\n",
      "Epoch: 295/500 Iteration: 2660 Train loss: 0.186832 Train acc: 0.940000\n",
      "Epoch: 296/500 Iteration: 2665 Train loss: 0.158120 Train acc: 0.961667\n",
      "Epoch: 296/500 Iteration: 2670 Train loss: 0.151123 Train acc: 0.958333\n",
      "Epoch: 297/500 Iteration: 2675 Train loss: 0.181461 Train acc: 0.945000\n",
      "Epoch: 297/500 Iteration: 2675 Validation loss: 0.137071 Validation acc: 0.952500\n",
      "Epoch: 297/500 Iteration: 2680 Train loss: 0.192559 Train acc: 0.941667\n",
      "Epoch: 298/500 Iteration: 2685 Train loss: 0.174250 Train acc: 0.946667\n",
      "Epoch: 298/500 Iteration: 2690 Train loss: 0.197343 Train acc: 0.945000\n",
      "Epoch: 299/500 Iteration: 2695 Train loss: 0.164317 Train acc: 0.955000\n",
      "Epoch: 299/500 Iteration: 2700 Train loss: 0.196778 Train acc: 0.938333\n",
      "Epoch: 299/500 Iteration: 2700 Validation loss: 0.136449 Validation acc: 0.950833\n",
      "Epoch: 300/500 Iteration: 2705 Train loss: 0.176525 Train acc: 0.945000\n",
      "Epoch: 301/500 Iteration: 2710 Train loss: 0.167446 Train acc: 0.950000\n",
      "Epoch: 301/500 Iteration: 2715 Train loss: 0.162663 Train acc: 0.950000\n",
      "Epoch: 302/500 Iteration: 2720 Train loss: 0.196332 Train acc: 0.940000\n",
      "Epoch: 302/500 Iteration: 2725 Train loss: 0.183449 Train acc: 0.946667\n",
      "Epoch: 302/500 Iteration: 2725 Validation loss: 0.147949 Validation acc: 0.950000\n",
      "Epoch: 303/500 Iteration: 2730 Train loss: 0.199059 Train acc: 0.933333\n",
      "Epoch: 303/500 Iteration: 2735 Train loss: 0.191098 Train acc: 0.945000\n",
      "Epoch: 304/500 Iteration: 2740 Train loss: 0.161572 Train acc: 0.958333\n",
      "Epoch: 304/500 Iteration: 2745 Train loss: 0.181820 Train acc: 0.941667\n",
      "Epoch: 305/500 Iteration: 2750 Train loss: 0.165249 Train acc: 0.941667\n",
      "Epoch: 305/500 Iteration: 2750 Validation loss: 0.138311 Validation acc: 0.950833\n",
      "Epoch: 306/500 Iteration: 2755 Train loss: 0.160900 Train acc: 0.958333\n",
      "Epoch: 306/500 Iteration: 2760 Train loss: 0.161730 Train acc: 0.951667\n",
      "Epoch: 307/500 Iteration: 2765 Train loss: 0.182514 Train acc: 0.943333\n",
      "Epoch: 307/500 Iteration: 2770 Train loss: 0.175954 Train acc: 0.936667\n",
      "Epoch: 308/500 Iteration: 2775 Train loss: 0.195652 Train acc: 0.941667\n",
      "Epoch: 308/500 Iteration: 2775 Validation loss: 0.121955 Validation acc: 0.956667\n",
      "Epoch: 308/500 Iteration: 2780 Train loss: 0.200067 Train acc: 0.950000\n",
      "Epoch: 309/500 Iteration: 2785 Train loss: 0.150011 Train acc: 0.960000\n",
      "Epoch: 309/500 Iteration: 2790 Train loss: 0.184978 Train acc: 0.940000\n",
      "Epoch: 310/500 Iteration: 2795 Train loss: 0.207201 Train acc: 0.936667\n",
      "Epoch: 311/500 Iteration: 2800 Train loss: 0.158402 Train acc: 0.956667\n",
      "Epoch: 311/500 Iteration: 2800 Validation loss: 0.125721 Validation acc: 0.952500\n",
      "Epoch: 311/500 Iteration: 2805 Train loss: 0.162494 Train acc: 0.951667\n",
      "Epoch: 312/500 Iteration: 2810 Train loss: 0.181919 Train acc: 0.941667\n",
      "Epoch: 312/500 Iteration: 2815 Train loss: 0.205812 Train acc: 0.923333\n",
      "Epoch: 313/500 Iteration: 2820 Train loss: 0.159807 Train acc: 0.946667\n",
      "Epoch: 313/500 Iteration: 2825 Train loss: 0.199004 Train acc: 0.946667\n",
      "Epoch: 313/500 Iteration: 2825 Validation loss: 0.130415 Validation acc: 0.953333\n",
      "Epoch: 314/500 Iteration: 2830 Train loss: 0.162877 Train acc: 0.946667\n",
      "Epoch: 314/500 Iteration: 2835 Train loss: 0.184322 Train acc: 0.943333\n",
      "Epoch: 315/500 Iteration: 2840 Train loss: 0.190017 Train acc: 0.943333\n",
      "Epoch: 316/500 Iteration: 2845 Train loss: 0.177900 Train acc: 0.948333\n",
      "Epoch: 316/500 Iteration: 2850 Train loss: 0.152263 Train acc: 0.955000\n",
      "Epoch: 316/500 Iteration: 2850 Validation loss: 0.136612 Validation acc: 0.951667\n",
      "Epoch: 317/500 Iteration: 2855 Train loss: 0.200975 Train acc: 0.933333\n",
      "Epoch: 317/500 Iteration: 2860 Train loss: 0.168584 Train acc: 0.950000\n",
      "Epoch: 318/500 Iteration: 2865 Train loss: 0.170770 Train acc: 0.940000\n",
      "Epoch: 318/500 Iteration: 2870 Train loss: 0.207678 Train acc: 0.935000\n",
      "Epoch: 319/500 Iteration: 2875 Train loss: 0.169088 Train acc: 0.956667\n",
      "Epoch: 319/500 Iteration: 2875 Validation loss: 0.125529 Validation acc: 0.954167\n",
      "Epoch: 319/500 Iteration: 2880 Train loss: 0.187024 Train acc: 0.941667\n",
      "Epoch: 320/500 Iteration: 2885 Train loss: 0.178476 Train acc: 0.948333\n",
      "Epoch: 321/500 Iteration: 2890 Train loss: 0.154952 Train acc: 0.963333\n",
      "Epoch: 321/500 Iteration: 2895 Train loss: 0.153232 Train acc: 0.955000\n",
      "Epoch: 322/500 Iteration: 2900 Train loss: 0.181635 Train acc: 0.935000\n",
      "Epoch: 322/500 Iteration: 2900 Validation loss: 0.127915 Validation acc: 0.950833\n",
      "Epoch: 322/500 Iteration: 2905 Train loss: 0.175502 Train acc: 0.943333\n",
      "Epoch: 323/500 Iteration: 2910 Train loss: 0.166959 Train acc: 0.950000\n",
      "Epoch: 323/500 Iteration: 2915 Train loss: 0.192497 Train acc: 0.950000\n",
      "Epoch: 324/500 Iteration: 2920 Train loss: 0.180366 Train acc: 0.946667\n",
      "Epoch: 324/500 Iteration: 2925 Train loss: 0.172168 Train acc: 0.951667\n",
      "Epoch: 324/500 Iteration: 2925 Validation loss: 0.133126 Validation acc: 0.950833\n",
      "Epoch: 325/500 Iteration: 2930 Train loss: 0.157406 Train acc: 0.951667\n",
      "Epoch: 326/500 Iteration: 2935 Train loss: 0.149087 Train acc: 0.955000\n",
      "Epoch: 326/500 Iteration: 2940 Train loss: 0.162879 Train acc: 0.951667\n",
      "Epoch: 327/500 Iteration: 2945 Train loss: 0.170463 Train acc: 0.940000\n",
      "Epoch: 327/500 Iteration: 2950 Train loss: 0.178298 Train acc: 0.945000\n",
      "Epoch: 327/500 Iteration: 2950 Validation loss: 0.135039 Validation acc: 0.950833\n",
      "Epoch: 328/500 Iteration: 2955 Train loss: 0.161891 Train acc: 0.946667\n",
      "Epoch: 328/500 Iteration: 2960 Train loss: 0.198429 Train acc: 0.936667\n",
      "Epoch: 329/500 Iteration: 2965 Train loss: 0.145869 Train acc: 0.958333\n",
      "Epoch: 329/500 Iteration: 2970 Train loss: 0.169611 Train acc: 0.943333\n",
      "Epoch: 330/500 Iteration: 2975 Train loss: 0.161749 Train acc: 0.941667\n",
      "Epoch: 330/500 Iteration: 2975 Validation loss: 0.131169 Validation acc: 0.951667\n",
      "Epoch: 331/500 Iteration: 2980 Train loss: 0.143136 Train acc: 0.956667\n",
      "Epoch: 331/500 Iteration: 2985 Train loss: 0.153408 Train acc: 0.958333\n",
      "Epoch: 332/500 Iteration: 2990 Train loss: 0.164091 Train acc: 0.950000\n",
      "Epoch: 332/500 Iteration: 2995 Train loss: 0.156947 Train acc: 0.941667\n",
      "Epoch: 333/500 Iteration: 3000 Train loss: 0.169183 Train acc: 0.941667\n",
      "Epoch: 333/500 Iteration: 3000 Validation loss: 0.125760 Validation acc: 0.954167\n",
      "Epoch: 333/500 Iteration: 3005 Train loss: 0.200452 Train acc: 0.943333\n",
      "Epoch: 334/500 Iteration: 3010 Train loss: 0.163556 Train acc: 0.950000\n",
      "Epoch: 334/500 Iteration: 3015 Train loss: 0.174641 Train acc: 0.948333\n",
      "Epoch: 335/500 Iteration: 3020 Train loss: 0.163370 Train acc: 0.945000\n",
      "Epoch: 336/500 Iteration: 3025 Train loss: 0.153269 Train acc: 0.953333\n",
      "Epoch: 336/500 Iteration: 3025 Validation loss: 0.140615 Validation acc: 0.951667\n",
      "Epoch: 336/500 Iteration: 3030 Train loss: 0.158813 Train acc: 0.948333\n",
      "Epoch: 337/500 Iteration: 3035 Train loss: 0.160098 Train acc: 0.940000\n",
      "Epoch: 337/500 Iteration: 3040 Train loss: 0.167762 Train acc: 0.941667\n",
      "Epoch: 338/500 Iteration: 3045 Train loss: 0.153744 Train acc: 0.948333\n",
      "Epoch: 338/500 Iteration: 3050 Train loss: 0.193402 Train acc: 0.943333\n",
      "Epoch: 338/500 Iteration: 3050 Validation loss: 0.136407 Validation acc: 0.950833\n",
      "Epoch: 339/500 Iteration: 3055 Train loss: 0.167949 Train acc: 0.951667\n",
      "Epoch: 339/500 Iteration: 3060 Train loss: 0.172368 Train acc: 0.945000\n",
      "Epoch: 340/500 Iteration: 3065 Train loss: 0.169422 Train acc: 0.945000\n",
      "Epoch: 341/500 Iteration: 3070 Train loss: 0.127318 Train acc: 0.960000\n",
      "Epoch: 341/500 Iteration: 3075 Train loss: 0.161763 Train acc: 0.948333\n",
      "Epoch: 341/500 Iteration: 3075 Validation loss: 0.129626 Validation acc: 0.954167\n",
      "Epoch: 342/500 Iteration: 3080 Train loss: 0.183781 Train acc: 0.941667\n",
      "Epoch: 342/500 Iteration: 3085 Train loss: 0.174848 Train acc: 0.945000\n",
      "Epoch: 343/500 Iteration: 3090 Train loss: 0.171569 Train acc: 0.938333\n",
      "Epoch: 343/500 Iteration: 3095 Train loss: 0.200239 Train acc: 0.935000\n",
      "Epoch: 344/500 Iteration: 3100 Train loss: 0.168921 Train acc: 0.951667\n",
      "Epoch: 344/500 Iteration: 3100 Validation loss: 0.135785 Validation acc: 0.951667\n",
      "Epoch: 344/500 Iteration: 3105 Train loss: 0.192888 Train acc: 0.935000\n",
      "Epoch: 345/500 Iteration: 3110 Train loss: 0.166847 Train acc: 0.943333\n",
      "Epoch: 346/500 Iteration: 3115 Train loss: 0.156755 Train acc: 0.951667\n",
      "Epoch: 346/500 Iteration: 3120 Train loss: 0.143917 Train acc: 0.955000\n",
      "Epoch: 347/500 Iteration: 3125 Train loss: 0.184556 Train acc: 0.953333\n",
      "Epoch: 347/500 Iteration: 3125 Validation loss: 0.136122 Validation acc: 0.951667\n",
      "Epoch: 347/500 Iteration: 3130 Train loss: 0.157065 Train acc: 0.956667\n",
      "Epoch: 348/500 Iteration: 3135 Train loss: 0.149226 Train acc: 0.943333\n",
      "Epoch: 348/500 Iteration: 3140 Train loss: 0.191171 Train acc: 0.945000\n",
      "Epoch: 349/500 Iteration: 3145 Train loss: 0.163901 Train acc: 0.945000\n",
      "Epoch: 349/500 Iteration: 3150 Train loss: 0.175227 Train acc: 0.941667\n",
      "Epoch: 349/500 Iteration: 3150 Validation loss: 0.132543 Validation acc: 0.953333\n",
      "Epoch: 350/500 Iteration: 3155 Train loss: 0.158280 Train acc: 0.943333\n",
      "Epoch: 351/500 Iteration: 3160 Train loss: 0.162509 Train acc: 0.955000\n",
      "Epoch: 351/500 Iteration: 3165 Train loss: 0.155073 Train acc: 0.953333\n",
      "Epoch: 352/500 Iteration: 3170 Train loss: 0.166307 Train acc: 0.945000\n",
      "Epoch: 352/500 Iteration: 3175 Train loss: 0.154722 Train acc: 0.958333\n",
      "Epoch: 352/500 Iteration: 3175 Validation loss: 0.125804 Validation acc: 0.952500\n",
      "Epoch: 353/500 Iteration: 3180 Train loss: 0.171364 Train acc: 0.940000\n",
      "Epoch: 353/500 Iteration: 3185 Train loss: 0.193826 Train acc: 0.943333\n",
      "Epoch: 354/500 Iteration: 3190 Train loss: 0.152475 Train acc: 0.960000\n",
      "Epoch: 354/500 Iteration: 3195 Train loss: 0.191267 Train acc: 0.935000\n",
      "Epoch: 355/500 Iteration: 3200 Train loss: 0.169152 Train acc: 0.945000\n",
      "Epoch: 355/500 Iteration: 3200 Validation loss: 0.140302 Validation acc: 0.951667\n",
      "Epoch: 356/500 Iteration: 3205 Train loss: 0.131451 Train acc: 0.958333\n",
      "Epoch: 356/500 Iteration: 3210 Train loss: 0.143015 Train acc: 0.960000\n",
      "Epoch: 357/500 Iteration: 3215 Train loss: 0.165427 Train acc: 0.938333\n",
      "Epoch: 357/500 Iteration: 3220 Train loss: 0.158442 Train acc: 0.953333\n",
      "Epoch: 358/500 Iteration: 3225 Train loss: 0.159155 Train acc: 0.953333\n",
      "Epoch: 358/500 Iteration: 3225 Validation loss: 0.144679 Validation acc: 0.949167\n",
      "Epoch: 358/500 Iteration: 3230 Train loss: 0.188196 Train acc: 0.938333\n",
      "Epoch: 359/500 Iteration: 3235 Train loss: 0.159746 Train acc: 0.958333\n",
      "Epoch: 359/500 Iteration: 3240 Train loss: 0.186672 Train acc: 0.940000\n",
      "Epoch: 360/500 Iteration: 3245 Train loss: 0.163437 Train acc: 0.951667\n",
      "Epoch: 361/500 Iteration: 3250 Train loss: 0.152700 Train acc: 0.950000\n",
      "Epoch: 361/500 Iteration: 3250 Validation loss: 0.133679 Validation acc: 0.949167\n",
      "Epoch: 361/500 Iteration: 3255 Train loss: 0.148512 Train acc: 0.950000\n",
      "Epoch: 362/500 Iteration: 3260 Train loss: 0.168440 Train acc: 0.946667\n",
      "Epoch: 362/500 Iteration: 3265 Train loss: 0.170604 Train acc: 0.943333\n",
      "Epoch: 363/500 Iteration: 3270 Train loss: 0.157377 Train acc: 0.948333\n",
      "Epoch: 363/500 Iteration: 3275 Train loss: 0.182309 Train acc: 0.943333\n",
      "Epoch: 363/500 Iteration: 3275 Validation loss: 0.143764 Validation acc: 0.951667\n",
      "Epoch: 364/500 Iteration: 3280 Train loss: 0.164984 Train acc: 0.946667\n",
      "Epoch: 364/500 Iteration: 3285 Train loss: 0.172340 Train acc: 0.941667\n",
      "Epoch: 365/500 Iteration: 3290 Train loss: 0.163728 Train acc: 0.945000\n",
      "Epoch: 366/500 Iteration: 3295 Train loss: 0.145035 Train acc: 0.951667\n",
      "Epoch: 366/500 Iteration: 3300 Train loss: 0.146644 Train acc: 0.965000\n",
      "Epoch: 366/500 Iteration: 3300 Validation loss: 0.135930 Validation acc: 0.953333\n",
      "Epoch: 367/500 Iteration: 3305 Train loss: 0.168539 Train acc: 0.941667\n",
      "Epoch: 367/500 Iteration: 3310 Train loss: 0.171184 Train acc: 0.955000\n",
      "Epoch: 368/500 Iteration: 3315 Train loss: 0.156166 Train acc: 0.946667\n",
      "Epoch: 368/500 Iteration: 3320 Train loss: 0.191751 Train acc: 0.936667\n",
      "Epoch: 369/500 Iteration: 3325 Train loss: 0.153635 Train acc: 0.955000\n",
      "Epoch: 369/500 Iteration: 3325 Validation loss: 0.135087 Validation acc: 0.950000\n",
      "Epoch: 369/500 Iteration: 3330 Train loss: 0.176569 Train acc: 0.943333\n",
      "Epoch: 370/500 Iteration: 3335 Train loss: 0.166779 Train acc: 0.943333\n",
      "Epoch: 371/500 Iteration: 3340 Train loss: 0.135884 Train acc: 0.961667\n",
      "Epoch: 371/500 Iteration: 3345 Train loss: 0.133209 Train acc: 0.958333\n",
      "Epoch: 372/500 Iteration: 3350 Train loss: 0.173668 Train acc: 0.940000\n",
      "Epoch: 372/500 Iteration: 3350 Validation loss: 0.136422 Validation acc: 0.952500\n",
      "Epoch: 372/500 Iteration: 3355 Train loss: 0.154004 Train acc: 0.950000\n",
      "Epoch: 373/500 Iteration: 3360 Train loss: 0.149336 Train acc: 0.951667\n",
      "Epoch: 373/500 Iteration: 3365 Train loss: 0.195299 Train acc: 0.935000\n",
      "Epoch: 374/500 Iteration: 3370 Train loss: 0.159635 Train acc: 0.950000\n",
      "Epoch: 374/500 Iteration: 3375 Train loss: 0.168439 Train acc: 0.941667\n",
      "Epoch: 374/500 Iteration: 3375 Validation loss: 0.135647 Validation acc: 0.951667\n",
      "Epoch: 375/500 Iteration: 3380 Train loss: 0.167899 Train acc: 0.943333\n",
      "Epoch: 376/500 Iteration: 3385 Train loss: 0.148441 Train acc: 0.943333\n",
      "Epoch: 376/500 Iteration: 3390 Train loss: 0.154575 Train acc: 0.950000\n",
      "Epoch: 377/500 Iteration: 3395 Train loss: 0.169144 Train acc: 0.938333\n",
      "Epoch: 377/500 Iteration: 3400 Train loss: 0.149173 Train acc: 0.951667\n",
      "Epoch: 377/500 Iteration: 3400 Validation loss: 0.130749 Validation acc: 0.951667\n",
      "Epoch: 378/500 Iteration: 3405 Train loss: 0.155933 Train acc: 0.940000\n",
      "Epoch: 378/500 Iteration: 3410 Train loss: 0.196155 Train acc: 0.946667\n",
      "Epoch: 379/500 Iteration: 3415 Train loss: 0.150883 Train acc: 0.956667\n",
      "Epoch: 379/500 Iteration: 3420 Train loss: 0.184965 Train acc: 0.943333\n",
      "Epoch: 380/500 Iteration: 3425 Train loss: 0.161198 Train acc: 0.946667\n",
      "Epoch: 380/500 Iteration: 3425 Validation loss: 0.131206 Validation acc: 0.949167\n",
      "Epoch: 381/500 Iteration: 3430 Train loss: 0.138713 Train acc: 0.956667\n",
      "Epoch: 381/500 Iteration: 3435 Train loss: 0.148209 Train acc: 0.946667\n",
      "Epoch: 382/500 Iteration: 3440 Train loss: 0.178240 Train acc: 0.935000\n",
      "Epoch: 382/500 Iteration: 3445 Train loss: 0.158416 Train acc: 0.950000\n",
      "Epoch: 383/500 Iteration: 3450 Train loss: 0.147750 Train acc: 0.946667\n",
      "Epoch: 383/500 Iteration: 3450 Validation loss: 0.124351 Validation acc: 0.952500\n",
      "Epoch: 383/500 Iteration: 3455 Train loss: 0.181631 Train acc: 0.946667\n",
      "Epoch: 384/500 Iteration: 3460 Train loss: 0.153521 Train acc: 0.955000\n",
      "Epoch: 384/500 Iteration: 3465 Train loss: 0.165283 Train acc: 0.943333\n",
      "Epoch: 385/500 Iteration: 3470 Train loss: 0.154917 Train acc: 0.946667\n",
      "Epoch: 386/500 Iteration: 3475 Train loss: 0.128283 Train acc: 0.951667\n",
      "Epoch: 386/500 Iteration: 3475 Validation loss: 0.133930 Validation acc: 0.953333\n",
      "Epoch: 386/500 Iteration: 3480 Train loss: 0.131089 Train acc: 0.960000\n",
      "Epoch: 387/500 Iteration: 3485 Train loss: 0.167448 Train acc: 0.941667\n",
      "Epoch: 387/500 Iteration: 3490 Train loss: 0.159410 Train acc: 0.948333\n",
      "Epoch: 388/500 Iteration: 3495 Train loss: 0.139314 Train acc: 0.953333\n",
      "Epoch: 388/500 Iteration: 3500 Train loss: 0.187868 Train acc: 0.945000\n",
      "Epoch: 388/500 Iteration: 3500 Validation loss: 0.136468 Validation acc: 0.952500\n",
      "Epoch: 389/500 Iteration: 3505 Train loss: 0.153723 Train acc: 0.956667\n",
      "Epoch: 389/500 Iteration: 3510 Train loss: 0.151378 Train acc: 0.956667\n",
      "Epoch: 390/500 Iteration: 3515 Train loss: 0.146467 Train acc: 0.953333\n",
      "Epoch: 391/500 Iteration: 3520 Train loss: 0.135431 Train acc: 0.960000\n",
      "Epoch: 391/500 Iteration: 3525 Train loss: 0.152831 Train acc: 0.955000\n",
      "Epoch: 391/500 Iteration: 3525 Validation loss: 0.138547 Validation acc: 0.952500\n",
      "Epoch: 392/500 Iteration: 3530 Train loss: 0.153007 Train acc: 0.943333\n",
      "Epoch: 392/500 Iteration: 3535 Train loss: 0.176408 Train acc: 0.941667\n",
      "Epoch: 393/500 Iteration: 3540 Train loss: 0.150833 Train acc: 0.948333\n",
      "Epoch: 393/500 Iteration: 3545 Train loss: 0.177936 Train acc: 0.936667\n",
      "Epoch: 394/500 Iteration: 3550 Train loss: 0.154187 Train acc: 0.956667\n",
      "Epoch: 394/500 Iteration: 3550 Validation loss: 0.135175 Validation acc: 0.951667\n",
      "Epoch: 394/500 Iteration: 3555 Train loss: 0.172047 Train acc: 0.936667\n",
      "Epoch: 395/500 Iteration: 3560 Train loss: 0.161564 Train acc: 0.946667\n",
      "Epoch: 396/500 Iteration: 3565 Train loss: 0.137285 Train acc: 0.966667\n",
      "Epoch: 396/500 Iteration: 3570 Train loss: 0.150634 Train acc: 0.958333\n",
      "Epoch: 397/500 Iteration: 3575 Train loss: 0.160456 Train acc: 0.951667\n",
      "Epoch: 397/500 Iteration: 3575 Validation loss: 0.136294 Validation acc: 0.951667\n",
      "Epoch: 397/500 Iteration: 3580 Train loss: 0.169657 Train acc: 0.943333\n",
      "Epoch: 398/500 Iteration: 3585 Train loss: 0.157040 Train acc: 0.945000\n",
      "Epoch: 398/500 Iteration: 3590 Train loss: 0.175274 Train acc: 0.941667\n",
      "Epoch: 399/500 Iteration: 3595 Train loss: 0.146019 Train acc: 0.953333\n",
      "Epoch: 399/500 Iteration: 3600 Train loss: 0.151750 Train acc: 0.951667\n",
      "Epoch: 399/500 Iteration: 3600 Validation loss: 0.137583 Validation acc: 0.949167\n",
      "Epoch: 400/500 Iteration: 3605 Train loss: 0.175992 Train acc: 0.941667\n",
      "Epoch: 401/500 Iteration: 3610 Train loss: 0.130061 Train acc: 0.960000\n",
      "Epoch: 401/500 Iteration: 3615 Train loss: 0.166641 Train acc: 0.943333\n",
      "Epoch: 402/500 Iteration: 3620 Train loss: 0.170421 Train acc: 0.936667\n",
      "Epoch: 402/500 Iteration: 3625 Train loss: 0.149424 Train acc: 0.955000\n",
      "Epoch: 402/500 Iteration: 3625 Validation loss: 0.124574 Validation acc: 0.950833\n",
      "Epoch: 403/500 Iteration: 3630 Train loss: 0.156505 Train acc: 0.945000\n",
      "Epoch: 403/500 Iteration: 3635 Train loss: 0.177552 Train acc: 0.940000\n",
      "Epoch: 404/500 Iteration: 3640 Train loss: 0.166870 Train acc: 0.950000\n",
      "Epoch: 404/500 Iteration: 3645 Train loss: 0.167425 Train acc: 0.936667\n",
      "Epoch: 405/500 Iteration: 3650 Train loss: 0.159568 Train acc: 0.946667\n",
      "Epoch: 405/500 Iteration: 3650 Validation loss: 0.144927 Validation acc: 0.947500\n",
      "Epoch: 406/500 Iteration: 3655 Train loss: 0.145278 Train acc: 0.946667\n",
      "Epoch: 406/500 Iteration: 3660 Train loss: 0.155449 Train acc: 0.945000\n",
      "Epoch: 407/500 Iteration: 3665 Train loss: 0.163637 Train acc: 0.943333\n",
      "Epoch: 407/500 Iteration: 3670 Train loss: 0.179592 Train acc: 0.933333\n",
      "Epoch: 408/500 Iteration: 3675 Train loss: 0.144691 Train acc: 0.946667\n",
      "Epoch: 408/500 Iteration: 3675 Validation loss: 0.132737 Validation acc: 0.951667\n",
      "Epoch: 408/500 Iteration: 3680 Train loss: 0.179969 Train acc: 0.948333\n",
      "Epoch: 409/500 Iteration: 3685 Train loss: 0.140830 Train acc: 0.963333\n",
      "Epoch: 409/500 Iteration: 3690 Train loss: 0.178188 Train acc: 0.941667\n",
      "Epoch: 410/500 Iteration: 3695 Train loss: 0.148742 Train acc: 0.940000\n",
      "Epoch: 411/500 Iteration: 3700 Train loss: 0.131722 Train acc: 0.958333\n",
      "Epoch: 411/500 Iteration: 3700 Validation loss: 0.130998 Validation acc: 0.951667\n",
      "Epoch: 411/500 Iteration: 3705 Train loss: 0.140199 Train acc: 0.950000\n",
      "Epoch: 412/500 Iteration: 3710 Train loss: 0.151698 Train acc: 0.955000\n",
      "Epoch: 412/500 Iteration: 3715 Train loss: 0.158925 Train acc: 0.950000\n",
      "Epoch: 413/500 Iteration: 3720 Train loss: 0.142463 Train acc: 0.953333\n",
      "Epoch: 413/500 Iteration: 3725 Train loss: 0.165066 Train acc: 0.956667\n",
      "Epoch: 413/500 Iteration: 3725 Validation loss: 0.132979 Validation acc: 0.951667\n",
      "Epoch: 414/500 Iteration: 3730 Train loss: 0.142877 Train acc: 0.951667\n",
      "Epoch: 414/500 Iteration: 3735 Train loss: 0.160756 Train acc: 0.946667\n",
      "Epoch: 415/500 Iteration: 3740 Train loss: 0.151826 Train acc: 0.948333\n",
      "Epoch: 416/500 Iteration: 3745 Train loss: 0.135249 Train acc: 0.953333\n",
      "Epoch: 416/500 Iteration: 3750 Train loss: 0.149997 Train acc: 0.951667\n",
      "Epoch: 416/500 Iteration: 3750 Validation loss: 0.127913 Validation acc: 0.950833\n",
      "Epoch: 417/500 Iteration: 3755 Train loss: 0.177456 Train acc: 0.941667\n",
      "Epoch: 417/500 Iteration: 3760 Train loss: 0.147974 Train acc: 0.950000\n",
      "Epoch: 418/500 Iteration: 3765 Train loss: 0.154967 Train acc: 0.943333\n",
      "Epoch: 418/500 Iteration: 3770 Train loss: 0.176845 Train acc: 0.940000\n",
      "Epoch: 419/500 Iteration: 3775 Train loss: 0.133189 Train acc: 0.955000\n",
      "Epoch: 419/500 Iteration: 3775 Validation loss: 0.130723 Validation acc: 0.952500\n",
      "Epoch: 419/500 Iteration: 3780 Train loss: 0.176852 Train acc: 0.945000\n",
      "Epoch: 420/500 Iteration: 3785 Train loss: 0.139649 Train acc: 0.945000\n",
      "Epoch: 421/500 Iteration: 3790 Train loss: 0.123345 Train acc: 0.960000\n",
      "Epoch: 421/500 Iteration: 3795 Train loss: 0.159895 Train acc: 0.946667\n",
      "Epoch: 422/500 Iteration: 3800 Train loss: 0.161662 Train acc: 0.948333\n",
      "Epoch: 422/500 Iteration: 3800 Validation loss: 0.135178 Validation acc: 0.950833\n",
      "Epoch: 422/500 Iteration: 3805 Train loss: 0.153012 Train acc: 0.953333\n",
      "Epoch: 423/500 Iteration: 3810 Train loss: 0.141460 Train acc: 0.946667\n",
      "Epoch: 423/500 Iteration: 3815 Train loss: 0.196908 Train acc: 0.945000\n",
      "Epoch: 424/500 Iteration: 3820 Train loss: 0.164352 Train acc: 0.951667\n",
      "Epoch: 424/500 Iteration: 3825 Train loss: 0.167117 Train acc: 0.940000\n",
      "Epoch: 424/500 Iteration: 3825 Validation loss: 0.135941 Validation acc: 0.952500\n",
      "Epoch: 425/500 Iteration: 3830 Train loss: 0.148924 Train acc: 0.946667\n",
      "Epoch: 426/500 Iteration: 3835 Train loss: 0.129374 Train acc: 0.955000\n",
      "Epoch: 426/500 Iteration: 3840 Train loss: 0.147118 Train acc: 0.953333\n",
      "Epoch: 427/500 Iteration: 3845 Train loss: 0.146448 Train acc: 0.951667\n",
      "Epoch: 427/500 Iteration: 3850 Train loss: 0.157177 Train acc: 0.940000\n",
      "Epoch: 427/500 Iteration: 3850 Validation loss: 0.136880 Validation acc: 0.950000\n",
      "Epoch: 428/500 Iteration: 3855 Train loss: 0.150516 Train acc: 0.951667\n",
      "Epoch: 428/500 Iteration: 3860 Train loss: 0.165139 Train acc: 0.945000\n",
      "Epoch: 429/500 Iteration: 3865 Train loss: 0.147002 Train acc: 0.953333\n",
      "Epoch: 429/500 Iteration: 3870 Train loss: 0.161176 Train acc: 0.948333\n",
      "Epoch: 430/500 Iteration: 3875 Train loss: 0.147361 Train acc: 0.953333\n",
      "Epoch: 430/500 Iteration: 3875 Validation loss: 0.138351 Validation acc: 0.951667\n",
      "Epoch: 431/500 Iteration: 3880 Train loss: 0.136264 Train acc: 0.955000\n",
      "Epoch: 431/500 Iteration: 3885 Train loss: 0.130134 Train acc: 0.950000\n",
      "Epoch: 432/500 Iteration: 3890 Train loss: 0.139504 Train acc: 0.955000\n",
      "Epoch: 432/500 Iteration: 3895 Train loss: 0.140985 Train acc: 0.943333\n",
      "Epoch: 433/500 Iteration: 3900 Train loss: 0.145720 Train acc: 0.953333\n",
      "Epoch: 433/500 Iteration: 3900 Validation loss: 0.139360 Validation acc: 0.953333\n",
      "Epoch: 433/500 Iteration: 3905 Train loss: 0.180729 Train acc: 0.928333\n",
      "Epoch: 434/500 Iteration: 3910 Train loss: 0.147435 Train acc: 0.955000\n",
      "Epoch: 434/500 Iteration: 3915 Train loss: 0.176589 Train acc: 0.938333\n",
      "Epoch: 435/500 Iteration: 3920 Train loss: 0.168454 Train acc: 0.940000\n",
      "Epoch: 436/500 Iteration: 3925 Train loss: 0.132372 Train acc: 0.958333\n",
      "Epoch: 436/500 Iteration: 3925 Validation loss: 0.137223 Validation acc: 0.951667\n",
      "Epoch: 436/500 Iteration: 3930 Train loss: 0.140250 Train acc: 0.948333\n",
      "Epoch: 437/500 Iteration: 3935 Train loss: 0.163625 Train acc: 0.940000\n",
      "Epoch: 437/500 Iteration: 3940 Train loss: 0.150127 Train acc: 0.953333\n",
      "Epoch: 438/500 Iteration: 3945 Train loss: 0.167683 Train acc: 0.936667\n",
      "Epoch: 438/500 Iteration: 3950 Train loss: 0.171659 Train acc: 0.938333\n",
      "Epoch: 438/500 Iteration: 3950 Validation loss: 0.132077 Validation acc: 0.953333\n",
      "Epoch: 439/500 Iteration: 3955 Train loss: 0.140348 Train acc: 0.955000\n",
      "Epoch: 439/500 Iteration: 3960 Train loss: 0.158739 Train acc: 0.938333\n",
      "Epoch: 440/500 Iteration: 3965 Train loss: 0.143782 Train acc: 0.940000\n",
      "Epoch: 441/500 Iteration: 3970 Train loss: 0.126385 Train acc: 0.963333\n",
      "Epoch: 441/500 Iteration: 3975 Train loss: 0.134373 Train acc: 0.953333\n",
      "Epoch: 441/500 Iteration: 3975 Validation loss: 0.129950 Validation acc: 0.955000\n",
      "Epoch: 442/500 Iteration: 3980 Train loss: 0.139724 Train acc: 0.951667\n",
      "Epoch: 442/500 Iteration: 3985 Train loss: 0.153366 Train acc: 0.950000\n",
      "Epoch: 443/500 Iteration: 3990 Train loss: 0.145130 Train acc: 0.948333\n",
      "Epoch: 443/500 Iteration: 3995 Train loss: 0.174803 Train acc: 0.935000\n",
      "Epoch: 444/500 Iteration: 4000 Train loss: 0.142235 Train acc: 0.951667\n",
      "Epoch: 444/500 Iteration: 4000 Validation loss: 0.130072 Validation acc: 0.953333\n",
      "Epoch: 444/500 Iteration: 4005 Train loss: 0.163968 Train acc: 0.943333\n",
      "Epoch: 445/500 Iteration: 4010 Train loss: 0.135392 Train acc: 0.955000\n",
      "Epoch: 446/500 Iteration: 4015 Train loss: 0.123466 Train acc: 0.961667\n",
      "Epoch: 446/500 Iteration: 4020 Train loss: 0.140473 Train acc: 0.951667\n",
      "Epoch: 447/500 Iteration: 4025 Train loss: 0.136213 Train acc: 0.951667\n",
      "Epoch: 447/500 Iteration: 4025 Validation loss: 0.131079 Validation acc: 0.951667\n",
      "Epoch: 447/500 Iteration: 4030 Train loss: 0.146195 Train acc: 0.946667\n",
      "Epoch: 448/500 Iteration: 4035 Train loss: 0.164503 Train acc: 0.936667\n",
      "Epoch: 448/500 Iteration: 4040 Train loss: 0.172513 Train acc: 0.930000\n",
      "Epoch: 449/500 Iteration: 4045 Train loss: 0.144647 Train acc: 0.956667\n",
      "Epoch: 449/500 Iteration: 4050 Train loss: 0.146882 Train acc: 0.950000\n",
      "Epoch: 449/500 Iteration: 4050 Validation loss: 0.132580 Validation acc: 0.951667\n",
      "Epoch: 450/500 Iteration: 4055 Train loss: 0.151545 Train acc: 0.945000\n",
      "Epoch: 451/500 Iteration: 4060 Train loss: 0.128001 Train acc: 0.953333\n",
      "Epoch: 451/500 Iteration: 4065 Train loss: 0.144559 Train acc: 0.933333\n",
      "Epoch: 452/500 Iteration: 4070 Train loss: 0.141513 Train acc: 0.953333\n",
      "Epoch: 452/500 Iteration: 4075 Train loss: 0.157490 Train acc: 0.933333\n",
      "Epoch: 452/500 Iteration: 4075 Validation loss: 0.126675 Validation acc: 0.951667\n",
      "Epoch: 453/500 Iteration: 4080 Train loss: 0.148272 Train acc: 0.951667\n",
      "Epoch: 453/500 Iteration: 4085 Train loss: 0.179554 Train acc: 0.938333\n",
      "Epoch: 454/500 Iteration: 4090 Train loss: 0.143991 Train acc: 0.950000\n",
      "Epoch: 454/500 Iteration: 4095 Train loss: 0.173107 Train acc: 0.946667\n",
      "Epoch: 455/500 Iteration: 4100 Train loss: 0.139086 Train acc: 0.945000\n",
      "Epoch: 455/500 Iteration: 4100 Validation loss: 0.125245 Validation acc: 0.952500\n",
      "Epoch: 456/500 Iteration: 4105 Train loss: 0.143599 Train acc: 0.946667\n",
      "Epoch: 456/500 Iteration: 4110 Train loss: 0.152124 Train acc: 0.948333\n",
      "Epoch: 457/500 Iteration: 4115 Train loss: 0.158016 Train acc: 0.938333\n",
      "Epoch: 457/500 Iteration: 4120 Train loss: 0.151935 Train acc: 0.951667\n",
      "Epoch: 458/500 Iteration: 4125 Train loss: 0.136166 Train acc: 0.951667\n",
      "Epoch: 458/500 Iteration: 4125 Validation loss: 0.129943 Validation acc: 0.952500\n",
      "Epoch: 458/500 Iteration: 4130 Train loss: 0.168732 Train acc: 0.938333\n",
      "Epoch: 459/500 Iteration: 4135 Train loss: 0.131996 Train acc: 0.958333\n",
      "Epoch: 459/500 Iteration: 4140 Train loss: 0.160664 Train acc: 0.943333\n",
      "Epoch: 460/500 Iteration: 4145 Train loss: 0.143726 Train acc: 0.943333\n",
      "Epoch: 461/500 Iteration: 4150 Train loss: 0.140209 Train acc: 0.956667\n",
      "Epoch: 461/500 Iteration: 4150 Validation loss: 0.126653 Validation acc: 0.952500\n",
      "Epoch: 461/500 Iteration: 4155 Train loss: 0.140985 Train acc: 0.960000\n",
      "Epoch: 462/500 Iteration: 4160 Train loss: 0.151328 Train acc: 0.940000\n",
      "Epoch: 462/500 Iteration: 4165 Train loss: 0.151796 Train acc: 0.943333\n",
      "Epoch: 463/500 Iteration: 4170 Train loss: 0.147801 Train acc: 0.948333\n",
      "Epoch: 463/500 Iteration: 4175 Train loss: 0.174367 Train acc: 0.946667\n",
      "Epoch: 463/500 Iteration: 4175 Validation loss: 0.122380 Validation acc: 0.951667\n",
      "Epoch: 464/500 Iteration: 4180 Train loss: 0.130834 Train acc: 0.951667\n",
      "Epoch: 464/500 Iteration: 4185 Train loss: 0.145838 Train acc: 0.943333\n",
      "Epoch: 465/500 Iteration: 4190 Train loss: 0.118442 Train acc: 0.953333\n",
      "Epoch: 466/500 Iteration: 4195 Train loss: 0.135212 Train acc: 0.956667\n",
      "Epoch: 466/500 Iteration: 4200 Train loss: 0.144288 Train acc: 0.941667\n",
      "Epoch: 466/500 Iteration: 4200 Validation loss: 0.129287 Validation acc: 0.951667\n",
      "Epoch: 467/500 Iteration: 4205 Train loss: 0.164148 Train acc: 0.941667\n",
      "Epoch: 467/500 Iteration: 4210 Train loss: 0.148422 Train acc: 0.948333\n",
      "Epoch: 468/500 Iteration: 4215 Train loss: 0.134184 Train acc: 0.958333\n",
      "Epoch: 468/500 Iteration: 4220 Train loss: 0.172447 Train acc: 0.931667\n",
      "Epoch: 469/500 Iteration: 4225 Train loss: 0.130864 Train acc: 0.961667\n",
      "Epoch: 469/500 Iteration: 4225 Validation loss: 0.121906 Validation acc: 0.952500\n",
      "Epoch: 469/500 Iteration: 4230 Train loss: 0.153515 Train acc: 0.943333\n",
      "Epoch: 470/500 Iteration: 4235 Train loss: 0.140573 Train acc: 0.941667\n",
      "Epoch: 471/500 Iteration: 4240 Train loss: 0.119546 Train acc: 0.956667\n",
      "Epoch: 471/500 Iteration: 4245 Train loss: 0.141132 Train acc: 0.956667\n",
      "Epoch: 472/500 Iteration: 4250 Train loss: 0.155248 Train acc: 0.941667\n",
      "Epoch: 472/500 Iteration: 4250 Validation loss: 0.127685 Validation acc: 0.950833\n",
      "Epoch: 472/500 Iteration: 4255 Train loss: 0.149928 Train acc: 0.950000\n",
      "Epoch: 473/500 Iteration: 4260 Train loss: 0.146028 Train acc: 0.950000\n",
      "Epoch: 473/500 Iteration: 4265 Train loss: 0.172669 Train acc: 0.938333\n",
      "Epoch: 474/500 Iteration: 4270 Train loss: 0.151573 Train acc: 0.943333\n",
      "Epoch: 474/500 Iteration: 4275 Train loss: 0.162087 Train acc: 0.938333\n",
      "Epoch: 474/500 Iteration: 4275 Validation loss: 0.139016 Validation acc: 0.952500\n",
      "Epoch: 475/500 Iteration: 4280 Train loss: 0.143114 Train acc: 0.946667\n",
      "Epoch: 476/500 Iteration: 4285 Train loss: 0.137144 Train acc: 0.951667\n",
      "Epoch: 476/500 Iteration: 4290 Train loss: 0.145383 Train acc: 0.953333\n",
      "Epoch: 477/500 Iteration: 4295 Train loss: 0.157692 Train acc: 0.941667\n",
      "Epoch: 477/500 Iteration: 4300 Train loss: 0.147304 Train acc: 0.935000\n",
      "Epoch: 477/500 Iteration: 4300 Validation loss: 0.128424 Validation acc: 0.950000\n",
      "Epoch: 478/500 Iteration: 4305 Train loss: 0.148649 Train acc: 0.945000\n",
      "Epoch: 478/500 Iteration: 4310 Train loss: 0.168702 Train acc: 0.948333\n",
      "Epoch: 479/500 Iteration: 4315 Train loss: 0.145722 Train acc: 0.956667\n",
      "Epoch: 479/500 Iteration: 4320 Train loss: 0.148472 Train acc: 0.941667\n",
      "Epoch: 480/500 Iteration: 4325 Train loss: 0.146011 Train acc: 0.950000\n",
      "Epoch: 480/500 Iteration: 4325 Validation loss: 0.135707 Validation acc: 0.949167\n",
      "Epoch: 481/500 Iteration: 4330 Train loss: 0.143378 Train acc: 0.946667\n",
      "Epoch: 481/500 Iteration: 4335 Train loss: 0.132961 Train acc: 0.956667\n",
      "Epoch: 482/500 Iteration: 4340 Train loss: 0.153396 Train acc: 0.945000\n",
      "Epoch: 482/500 Iteration: 4345 Train loss: 0.148807 Train acc: 0.940000\n",
      "Epoch: 483/500 Iteration: 4350 Train loss: 0.149002 Train acc: 0.941667\n",
      "Epoch: 483/500 Iteration: 4350 Validation loss: 0.131879 Validation acc: 0.950000\n",
      "Epoch: 483/500 Iteration: 4355 Train loss: 0.165247 Train acc: 0.950000\n",
      "Epoch: 484/500 Iteration: 4360 Train loss: 0.161280 Train acc: 0.948333\n",
      "Epoch: 484/500 Iteration: 4365 Train loss: 0.159268 Train acc: 0.936667\n",
      "Epoch: 485/500 Iteration: 4370 Train loss: 0.137370 Train acc: 0.948333\n",
      "Epoch: 486/500 Iteration: 4375 Train loss: 0.133982 Train acc: 0.953333\n",
      "Epoch: 486/500 Iteration: 4375 Validation loss: 0.128488 Validation acc: 0.952500\n",
      "Epoch: 486/500 Iteration: 4380 Train loss: 0.118111 Train acc: 0.958333\n",
      "Epoch: 487/500 Iteration: 4385 Train loss: 0.157829 Train acc: 0.948333\n",
      "Epoch: 487/500 Iteration: 4390 Train loss: 0.157921 Train acc: 0.948333\n",
      "Epoch: 488/500 Iteration: 4395 Train loss: 0.133325 Train acc: 0.955000\n",
      "Epoch: 488/500 Iteration: 4400 Train loss: 0.174786 Train acc: 0.940000\n",
      "Epoch: 488/500 Iteration: 4400 Validation loss: 0.123101 Validation acc: 0.955833\n",
      "Epoch: 489/500 Iteration: 4405 Train loss: 0.132842 Train acc: 0.960000\n",
      "Epoch: 489/500 Iteration: 4410 Train loss: 0.149602 Train acc: 0.945000\n",
      "Epoch: 490/500 Iteration: 4415 Train loss: 0.136989 Train acc: 0.941667\n",
      "Epoch: 491/500 Iteration: 4420 Train loss: 0.135751 Train acc: 0.955000\n",
      "Epoch: 491/500 Iteration: 4425 Train loss: 0.132973 Train acc: 0.960000\n",
      "Epoch: 491/500 Iteration: 4425 Validation loss: 0.125526 Validation acc: 0.954167\n",
      "Epoch: 492/500 Iteration: 4430 Train loss: 0.143378 Train acc: 0.950000\n",
      "Epoch: 492/500 Iteration: 4435 Train loss: 0.143614 Train acc: 0.948333\n",
      "Epoch: 493/500 Iteration: 4440 Train loss: 0.136418 Train acc: 0.955000\n",
      "Epoch: 493/500 Iteration: 4445 Train loss: 0.165111 Train acc: 0.940000\n",
      "Epoch: 494/500 Iteration: 4450 Train loss: 0.134566 Train acc: 0.958333\n",
      "Epoch: 494/500 Iteration: 4450 Validation loss: 0.127359 Validation acc: 0.954167\n",
      "Epoch: 494/500 Iteration: 4455 Train loss: 0.154653 Train acc: 0.951667\n",
      "Epoch: 495/500 Iteration: 4460 Train loss: 0.145791 Train acc: 0.951667\n",
      "Epoch: 496/500 Iteration: 4465 Train loss: 0.111799 Train acc: 0.958333\n",
      "Epoch: 496/500 Iteration: 4470 Train loss: 0.135121 Train acc: 0.943333\n",
      "Epoch: 497/500 Iteration: 4475 Train loss: 0.151621 Train acc: 0.945000\n",
      "Epoch: 497/500 Iteration: 4475 Validation loss: 0.137521 Validation acc: 0.950833\n",
      "Epoch: 497/500 Iteration: 4480 Train loss: 0.152551 Train acc: 0.955000\n",
      "Epoch: 498/500 Iteration: 4485 Train loss: 0.159508 Train acc: 0.943333\n",
      "Epoch: 498/500 Iteration: 4490 Train loss: 0.155656 Train acc: 0.948333\n",
      "Epoch: 499/500 Iteration: 4495 Train loss: 0.133004 Train acc: 0.958333\n",
      "Epoch: 499/500 Iteration: 4500 Train loss: 0.159998 Train acc: 0.943333\n",
      "Epoch: 499/500 Iteration: 4500 Validation loss: 0.121794 Validation acc: 0.954167\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%25 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "    saver.save(sess,\"checkpoints/eeg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAF3CAYAAAC2bHyQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRARkSNMGERE5AgTBhEROfL/AaFkHWXByScTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8294dcaba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 25 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8294dca7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 25 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.854167\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
