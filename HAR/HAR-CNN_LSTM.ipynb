{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# HAR CNN + LSTM training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.utilities import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, labels_train, list_ch_train = read_data(data_path=\"./data/\", split=\"train\") # train\n",
    "X_test, labels_test, list_ch_test = read_data(data_path=\"./data/\", split=\"test\") # test\n",
    "\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Standardize ?\n",
    "#X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, test_size = 0.2,\n",
    "                                                stratify = labels_train, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 27         # 3 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 600       # Batch size\n",
    "seq_len = 128          # Number of steps\n",
    "learning_rate = 0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 500\n",
    "\n",
    "# Fixed\n",
    "n_classes = 6\n",
    "n_channels = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construct the graph\n",
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build Convolutional Layer(s)\n",
    "\n",
    "Questions: \n",
    "* Should we use a different activation? Like tf.nn.tanh?\n",
    "* Should we use pooling? average or max?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convolutional layers\n",
    "with graph.as_default():\n",
    "    # (batch, 128, 9) --> (batch, 128, 18)\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=2, strides=1, \n",
    "                             padding='same', activation = tf.nn.relu)\n",
    "    n_ch = n_channels *2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, pass to LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(conv1, [1,0,2]) # reshape into (seq_len, batch, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_ch]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define forward pass and cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints-crnn') == False):\n",
    "    !mkdir checkpoints-crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 Iteration: 5 Train loss: 1.786813 Train acc: 0.168333\n",
      "Epoch: 1/500 Iteration: 10 Train loss: 1.782435 Train acc: 0.203333\n",
      "Epoch: 1/500 Iteration: 15 Train loss: 1.772435 Train acc: 0.208333\n",
      "Epoch: 2/500 Iteration: 20 Train loss: 1.761753 Train acc: 0.261667\n",
      "Epoch: 2/500 Iteration: 25 Train loss: 1.768160 Train acc: 0.255000\n",
      "Epoch: 2/500 Iteration: 25 Validation loss: 1.756156 Validation acc: 0.313333\n",
      "Epoch: 3/500 Iteration: 30 Train loss: 1.750669 Train acc: 0.295000\n",
      "Epoch: 3/500 Iteration: 35 Train loss: 1.746308 Train acc: 0.275000\n",
      "Epoch: 4/500 Iteration: 40 Train loss: 1.741839 Train acc: 0.303333\n",
      "Epoch: 4/500 Iteration: 45 Train loss: 1.727514 Train acc: 0.306667\n",
      "Epoch: 5/500 Iteration: 50 Train loss: 1.729037 Train acc: 0.291667\n",
      "Epoch: 5/500 Iteration: 50 Validation loss: 1.718354 Validation acc: 0.305833\n",
      "Epoch: 6/500 Iteration: 55 Train loss: 1.709624 Train acc: 0.320000\n",
      "Epoch: 6/500 Iteration: 60 Train loss: 1.713140 Train acc: 0.290000\n",
      "Epoch: 7/500 Iteration: 65 Train loss: 1.692579 Train acc: 0.345000\n",
      "Epoch: 7/500 Iteration: 70 Train loss: 1.695004 Train acc: 0.331667\n",
      "Epoch: 8/500 Iteration: 75 Train loss: 1.673282 Train acc: 0.346667\n",
      "Epoch: 8/500 Iteration: 75 Validation loss: 1.663238 Validation acc: 0.294167\n",
      "Epoch: 8/500 Iteration: 80 Train loss: 1.674729 Train acc: 0.333333\n",
      "Epoch: 9/500 Iteration: 85 Train loss: 1.656362 Train acc: 0.335000\n",
      "Epoch: 9/500 Iteration: 90 Train loss: 1.641298 Train acc: 0.328333\n",
      "Epoch: 10/500 Iteration: 95 Train loss: 1.640090 Train acc: 0.340000\n",
      "Epoch: 11/500 Iteration: 100 Train loss: 1.604738 Train acc: 0.351667\n",
      "Epoch: 11/500 Iteration: 100 Validation loss: 1.589706 Validation acc: 0.280000\n",
      "Epoch: 11/500 Iteration: 105 Train loss: 1.601102 Train acc: 0.340000\n",
      "Epoch: 12/500 Iteration: 110 Train loss: 1.585456 Train acc: 0.376667\n",
      "Epoch: 12/500 Iteration: 115 Train loss: 1.565159 Train acc: 0.353333\n",
      "Epoch: 13/500 Iteration: 120 Train loss: 1.547574 Train acc: 0.383333\n",
      "Epoch: 13/500 Iteration: 125 Train loss: 1.570497 Train acc: 0.330000\n",
      "Epoch: 13/500 Iteration: 125 Validation loss: 1.517324 Validation acc: 0.419167\n",
      "Epoch: 14/500 Iteration: 130 Train loss: 1.540770 Train acc: 0.340000\n",
      "Epoch: 14/500 Iteration: 135 Train loss: 1.512159 Train acc: 0.355000\n",
      "Epoch: 15/500 Iteration: 140 Train loss: 1.512564 Train acc: 0.365000\n",
      "Epoch: 16/500 Iteration: 145 Train loss: 1.507026 Train acc: 0.368333\n",
      "Epoch: 16/500 Iteration: 150 Train loss: 1.491215 Train acc: 0.368333\n",
      "Epoch: 16/500 Iteration: 150 Validation loss: 1.461152 Validation acc: 0.470000\n",
      "Epoch: 17/500 Iteration: 155 Train loss: 1.477371 Train acc: 0.380000\n",
      "Epoch: 17/500 Iteration: 160 Train loss: 1.459150 Train acc: 0.368333\n",
      "Epoch: 18/500 Iteration: 165 Train loss: 1.476402 Train acc: 0.398333\n",
      "Epoch: 18/500 Iteration: 170 Train loss: 1.494730 Train acc: 0.368333\n",
      "Epoch: 19/500 Iteration: 175 Train loss: 1.466691 Train acc: 0.371667\n",
      "Epoch: 19/500 Iteration: 175 Validation loss: 1.413627 Validation acc: 0.487500\n",
      "Epoch: 19/500 Iteration: 180 Train loss: 1.455683 Train acc: 0.385000\n",
      "Epoch: 20/500 Iteration: 185 Train loss: 1.453691 Train acc: 0.383333\n",
      "Epoch: 21/500 Iteration: 190 Train loss: 1.414842 Train acc: 0.428333\n",
      "Epoch: 21/500 Iteration: 195 Train loss: 1.401475 Train acc: 0.405000\n",
      "Epoch: 22/500 Iteration: 200 Train loss: 1.391353 Train acc: 0.433333\n",
      "Epoch: 22/500 Iteration: 200 Validation loss: 1.358196 Validation acc: 0.535000\n",
      "Epoch: 22/500 Iteration: 205 Train loss: 1.364548 Train acc: 0.461667\n",
      "Epoch: 23/500 Iteration: 210 Train loss: 1.364788 Train acc: 0.481667\n",
      "Epoch: 23/500 Iteration: 215 Train loss: 1.385074 Train acc: 0.448333\n",
      "Epoch: 24/500 Iteration: 220 Train loss: 1.370149 Train acc: 0.438333\n",
      "Epoch: 24/500 Iteration: 225 Train loss: 1.344873 Train acc: 0.465000\n",
      "Epoch: 24/500 Iteration: 225 Validation loss: 1.270972 Validation acc: 0.589167\n",
      "Epoch: 25/500 Iteration: 230 Train loss: 1.324153 Train acc: 0.495000\n",
      "Epoch: 26/500 Iteration: 235 Train loss: 1.279809 Train acc: 0.490000\n",
      "Epoch: 26/500 Iteration: 240 Train loss: 1.267301 Train acc: 0.500000\n",
      "Epoch: 27/500 Iteration: 245 Train loss: 1.231116 Train acc: 0.510000\n",
      "Epoch: 27/500 Iteration: 250 Train loss: 1.188056 Train acc: 0.516667\n",
      "Epoch: 27/500 Iteration: 250 Validation loss: 1.151480 Validation acc: 0.569167\n",
      "Epoch: 28/500 Iteration: 255 Train loss: 1.199112 Train acc: 0.508333\n",
      "Epoch: 28/500 Iteration: 260 Train loss: 1.216055 Train acc: 0.540000\n",
      "Epoch: 29/500 Iteration: 265 Train loss: 1.171170 Train acc: 0.546667\n",
      "Epoch: 29/500 Iteration: 270 Train loss: 1.152099 Train acc: 0.553333\n",
      "Epoch: 30/500 Iteration: 275 Train loss: 1.148704 Train acc: 0.558333\n",
      "Epoch: 30/500 Iteration: 275 Validation loss: 1.044295 Validation acc: 0.597500\n",
      "Epoch: 31/500 Iteration: 280 Train loss: 1.076149 Train acc: 0.610000\n",
      "Epoch: 31/500 Iteration: 285 Train loss: 1.069856 Train acc: 0.580000\n",
      "Epoch: 32/500 Iteration: 290 Train loss: 1.061670 Train acc: 0.581667\n",
      "Epoch: 32/500 Iteration: 295 Train loss: 1.036460 Train acc: 0.575000\n",
      "Epoch: 33/500 Iteration: 300 Train loss: 1.043528 Train acc: 0.556667\n",
      "Epoch: 33/500 Iteration: 300 Validation loss: 0.958460 Validation acc: 0.632500\n",
      "Epoch: 33/500 Iteration: 305 Train loss: 1.037849 Train acc: 0.576667\n",
      "Epoch: 34/500 Iteration: 310 Train loss: 1.022459 Train acc: 0.611667\n",
      "Epoch: 34/500 Iteration: 315 Train loss: 1.032460 Train acc: 0.580000\n",
      "Epoch: 35/500 Iteration: 320 Train loss: 1.004152 Train acc: 0.596667\n",
      "Epoch: 36/500 Iteration: 325 Train loss: 0.984724 Train acc: 0.655000\n",
      "Epoch: 36/500 Iteration: 325 Validation loss: 0.892671 Validation acc: 0.652500\n",
      "Epoch: 36/500 Iteration: 330 Train loss: 0.969467 Train acc: 0.623333\n",
      "Epoch: 37/500 Iteration: 335 Train loss: 0.962347 Train acc: 0.596667\n",
      "Epoch: 37/500 Iteration: 340 Train loss: 0.929430 Train acc: 0.648333\n",
      "Epoch: 38/500 Iteration: 345 Train loss: 0.943623 Train acc: 0.608333\n",
      "Epoch: 38/500 Iteration: 350 Train loss: 0.965858 Train acc: 0.598333\n",
      "Epoch: 38/500 Iteration: 350 Validation loss: 0.852110 Validation acc: 0.646667\n",
      "Epoch: 39/500 Iteration: 355 Train loss: 0.946865 Train acc: 0.630000\n",
      "Epoch: 39/500 Iteration: 360 Train loss: 0.956704 Train acc: 0.598333\n",
      "Epoch: 40/500 Iteration: 365 Train loss: 0.922363 Train acc: 0.626667\n",
      "Epoch: 41/500 Iteration: 370 Train loss: 0.898465 Train acc: 0.620000\n",
      "Epoch: 41/500 Iteration: 375 Train loss: 0.899461 Train acc: 0.633333\n",
      "Epoch: 41/500 Iteration: 375 Validation loss: 0.815347 Validation acc: 0.656667\n",
      "Epoch: 42/500 Iteration: 380 Train loss: 0.929385 Train acc: 0.610000\n",
      "Epoch: 42/500 Iteration: 385 Train loss: 0.871637 Train acc: 0.621667\n",
      "Epoch: 43/500 Iteration: 390 Train loss: 0.888725 Train acc: 0.641667\n",
      "Epoch: 43/500 Iteration: 395 Train loss: 0.872460 Train acc: 0.646667\n",
      "Epoch: 44/500 Iteration: 400 Train loss: 0.893593 Train acc: 0.620000\n",
      "Epoch: 44/500 Iteration: 400 Validation loss: 0.787209 Validation acc: 0.665000\n",
      "Epoch: 44/500 Iteration: 405 Train loss: 0.926267 Train acc: 0.598333\n",
      "Epoch: 45/500 Iteration: 410 Train loss: 0.895932 Train acc: 0.638333\n",
      "Epoch: 46/500 Iteration: 415 Train loss: 0.815901 Train acc: 0.686667\n",
      "Epoch: 46/500 Iteration: 420 Train loss: 0.837263 Train acc: 0.645000\n",
      "Epoch: 47/500 Iteration: 425 Train loss: 0.856122 Train acc: 0.636667\n",
      "Epoch: 47/500 Iteration: 425 Validation loss: 0.762824 Validation acc: 0.669167\n",
      "Epoch: 47/500 Iteration: 430 Train loss: 0.829782 Train acc: 0.655000\n",
      "Epoch: 48/500 Iteration: 435 Train loss: 0.824893 Train acc: 0.630000\n",
      "Epoch: 48/500 Iteration: 440 Train loss: 0.846911 Train acc: 0.650000\n",
      "Epoch: 49/500 Iteration: 445 Train loss: 0.868874 Train acc: 0.653333\n",
      "Epoch: 49/500 Iteration: 450 Train loss: 0.873242 Train acc: 0.620000\n",
      "Epoch: 49/500 Iteration: 450 Validation loss: 0.743204 Validation acc: 0.670000\n",
      "Epoch: 50/500 Iteration: 455 Train loss: 0.837232 Train acc: 0.646667\n",
      "Epoch: 51/500 Iteration: 460 Train loss: 0.810108 Train acc: 0.678333\n",
      "Epoch: 51/500 Iteration: 465 Train loss: 0.816517 Train acc: 0.653333\n",
      "Epoch: 52/500 Iteration: 470 Train loss: 0.810992 Train acc: 0.651667\n",
      "Epoch: 52/500 Iteration: 475 Train loss: 0.794118 Train acc: 0.665000\n",
      "Epoch: 52/500 Iteration: 475 Validation loss: 0.731244 Validation acc: 0.670833\n",
      "Epoch: 53/500 Iteration: 480 Train loss: 0.808471 Train acc: 0.641667\n",
      "Epoch: 53/500 Iteration: 485 Train loss: 0.825515 Train acc: 0.666667\n",
      "Epoch: 54/500 Iteration: 490 Train loss: 0.821724 Train acc: 0.651667\n",
      "Epoch: 54/500 Iteration: 495 Train loss: 0.846362 Train acc: 0.631667\n",
      "Epoch: 55/500 Iteration: 500 Train loss: 0.838198 Train acc: 0.615000\n",
      "Epoch: 55/500 Iteration: 500 Validation loss: 0.712596 Validation acc: 0.671667\n",
      "Epoch: 56/500 Iteration: 505 Train loss: 0.768863 Train acc: 0.700000\n",
      "Epoch: 56/500 Iteration: 510 Train loss: 0.776990 Train acc: 0.655000\n",
      "Epoch: 57/500 Iteration: 515 Train loss: 0.813737 Train acc: 0.630000\n",
      "Epoch: 57/500 Iteration: 520 Train loss: 0.783098 Train acc: 0.636667\n",
      "Epoch: 58/500 Iteration: 525 Train loss: 0.802067 Train acc: 0.666667\n",
      "Epoch: 58/500 Iteration: 525 Validation loss: 0.701156 Validation acc: 0.671667\n",
      "Epoch: 58/500 Iteration: 530 Train loss: 0.810741 Train acc: 0.648333\n",
      "Epoch: 59/500 Iteration: 535 Train loss: 0.805862 Train acc: 0.655000\n",
      "Epoch: 59/500 Iteration: 540 Train loss: 0.842757 Train acc: 0.616667\n",
      "Epoch: 60/500 Iteration: 545 Train loss: 0.780939 Train acc: 0.673333\n",
      "Epoch: 61/500 Iteration: 550 Train loss: 0.742030 Train acc: 0.690000\n",
      "Epoch: 61/500 Iteration: 550 Validation loss: 0.693159 Validation acc: 0.673333\n",
      "Epoch: 61/500 Iteration: 555 Train loss: 0.766582 Train acc: 0.676667\n",
      "Epoch: 62/500 Iteration: 560 Train loss: 0.777793 Train acc: 0.671667\n",
      "Epoch: 62/500 Iteration: 565 Train loss: 0.774834 Train acc: 0.665000\n",
      "Epoch: 63/500 Iteration: 570 Train loss: 0.782343 Train acc: 0.650000\n",
      "Epoch: 63/500 Iteration: 575 Train loss: 0.774282 Train acc: 0.663333\n",
      "Epoch: 63/500 Iteration: 575 Validation loss: 0.682109 Validation acc: 0.680833\n",
      "Epoch: 64/500 Iteration: 580 Train loss: 0.761641 Train acc: 0.690000\n",
      "Epoch: 64/500 Iteration: 585 Train loss: 0.821152 Train acc: 0.620000\n",
      "Epoch: 65/500 Iteration: 590 Train loss: 0.784079 Train acc: 0.678333\n",
      "Epoch: 66/500 Iteration: 595 Train loss: 0.720835 Train acc: 0.673333\n",
      "Epoch: 66/500 Iteration: 600 Train loss: 0.732888 Train acc: 0.666667\n",
      "Epoch: 66/500 Iteration: 600 Validation loss: 0.673120 Validation acc: 0.690000\n",
      "Epoch: 67/500 Iteration: 605 Train loss: 0.757082 Train acc: 0.658333\n",
      "Epoch: 67/500 Iteration: 610 Train loss: 0.743141 Train acc: 0.663333\n",
      "Epoch: 68/500 Iteration: 615 Train loss: 0.752599 Train acc: 0.651667\n",
      "Epoch: 68/500 Iteration: 620 Train loss: 0.757418 Train acc: 0.686667\n",
      "Epoch: 69/500 Iteration: 625 Train loss: 0.759797 Train acc: 0.661667\n",
      "Epoch: 69/500 Iteration: 625 Validation loss: 0.660662 Validation acc: 0.695833\n",
      "Epoch: 69/500 Iteration: 630 Train loss: 0.783641 Train acc: 0.645000\n",
      "Epoch: 70/500 Iteration: 635 Train loss: 0.739794 Train acc: 0.686667\n",
      "Epoch: 71/500 Iteration: 640 Train loss: 0.699818 Train acc: 0.693333\n",
      "Epoch: 71/500 Iteration: 645 Train loss: 0.737699 Train acc: 0.655000\n",
      "Epoch: 72/500 Iteration: 650 Train loss: 0.726930 Train acc: 0.693333\n",
      "Epoch: 72/500 Iteration: 650 Validation loss: 0.637857 Validation acc: 0.720833\n",
      "Epoch: 72/500 Iteration: 655 Train loss: 0.700828 Train acc: 0.696667\n",
      "Epoch: 73/500 Iteration: 660 Train loss: 0.743541 Train acc: 0.645000\n",
      "Epoch: 73/500 Iteration: 665 Train loss: 0.735502 Train acc: 0.655000\n",
      "Epoch: 74/500 Iteration: 670 Train loss: 0.702434 Train acc: 0.698333\n",
      "Epoch: 74/500 Iteration: 675 Train loss: 0.775794 Train acc: 0.658333\n",
      "Epoch: 74/500 Iteration: 675 Validation loss: 0.621296 Validation acc: 0.736667\n",
      "Epoch: 75/500 Iteration: 680 Train loss: 0.711964 Train acc: 0.673333\n",
      "Epoch: 76/500 Iteration: 685 Train loss: 0.659261 Train acc: 0.725000\n",
      "Epoch: 76/500 Iteration: 690 Train loss: 0.680474 Train acc: 0.726667\n",
      "Epoch: 77/500 Iteration: 695 Train loss: 0.732538 Train acc: 0.661667\n",
      "Epoch: 77/500 Iteration: 700 Train loss: 0.690272 Train acc: 0.711667\n",
      "Epoch: 77/500 Iteration: 700 Validation loss: 0.611451 Validation acc: 0.740000\n",
      "Epoch: 78/500 Iteration: 705 Train loss: 0.721284 Train acc: 0.660000\n",
      "Epoch: 78/500 Iteration: 710 Train loss: 0.706522 Train acc: 0.705000\n",
      "Epoch: 79/500 Iteration: 715 Train loss: 0.692027 Train acc: 0.703333\n",
      "Epoch: 79/500 Iteration: 720 Train loss: 0.733084 Train acc: 0.673333\n",
      "Epoch: 80/500 Iteration: 725 Train loss: 0.682919 Train acc: 0.693333\n",
      "Epoch: 80/500 Iteration: 725 Validation loss: 0.594525 Validation acc: 0.753333\n",
      "Epoch: 81/500 Iteration: 730 Train loss: 0.654084 Train acc: 0.710000\n",
      "Epoch: 81/500 Iteration: 735 Train loss: 0.660784 Train acc: 0.723333\n",
      "Epoch: 82/500 Iteration: 740 Train loss: 0.673484 Train acc: 0.695000\n",
      "Epoch: 82/500 Iteration: 745 Train loss: 0.654715 Train acc: 0.721667\n",
      "Epoch: 83/500 Iteration: 750 Train loss: 0.698409 Train acc: 0.673333\n",
      "Epoch: 83/500 Iteration: 750 Validation loss: 0.586516 Validation acc: 0.749167\n",
      "Epoch: 83/500 Iteration: 755 Train loss: 0.693511 Train acc: 0.696667\n",
      "Epoch: 84/500 Iteration: 760 Train loss: 0.671030 Train acc: 0.708333\n",
      "Epoch: 84/500 Iteration: 765 Train loss: 0.691956 Train acc: 0.695000\n",
      "Epoch: 85/500 Iteration: 770 Train loss: 0.654293 Train acc: 0.731667\n",
      "Epoch: 86/500 Iteration: 775 Train loss: 0.618973 Train acc: 0.750000\n",
      "Epoch: 86/500 Iteration: 775 Validation loss: 0.568731 Validation acc: 0.761667\n",
      "Epoch: 86/500 Iteration: 780 Train loss: 0.657397 Train acc: 0.738333\n",
      "Epoch: 87/500 Iteration: 785 Train loss: 0.654633 Train acc: 0.703333\n",
      "Epoch: 87/500 Iteration: 790 Train loss: 0.630783 Train acc: 0.715000\n",
      "Epoch: 88/500 Iteration: 795 Train loss: 0.671903 Train acc: 0.720000\n",
      "Epoch: 88/500 Iteration: 800 Train loss: 0.663883 Train acc: 0.725000\n",
      "Epoch: 88/500 Iteration: 800 Validation loss: 0.557635 Validation acc: 0.765000\n",
      "Epoch: 89/500 Iteration: 805 Train loss: 0.655063 Train acc: 0.730000\n",
      "Epoch: 89/500 Iteration: 810 Train loss: 0.681632 Train acc: 0.715000\n",
      "Epoch: 90/500 Iteration: 815 Train loss: 0.634008 Train acc: 0.741667\n",
      "Epoch: 91/500 Iteration: 820 Train loss: 0.625257 Train acc: 0.730000\n",
      "Epoch: 91/500 Iteration: 825 Train loss: 0.636903 Train acc: 0.735000\n",
      "Epoch: 91/500 Iteration: 825 Validation loss: 0.540798 Validation acc: 0.775000\n",
      "Epoch: 92/500 Iteration: 830 Train loss: 0.628636 Train acc: 0.741667\n",
      "Epoch: 92/500 Iteration: 835 Train loss: 0.598658 Train acc: 0.751667\n",
      "Epoch: 93/500 Iteration: 840 Train loss: 0.619309 Train acc: 0.728333\n",
      "Epoch: 93/500 Iteration: 845 Train loss: 0.633228 Train acc: 0.741667\n",
      "Epoch: 94/500 Iteration: 850 Train loss: 0.582475 Train acc: 0.756667\n",
      "Epoch: 94/500 Iteration: 850 Validation loss: 0.528571 Validation acc: 0.777500\n",
      "Epoch: 94/500 Iteration: 855 Train loss: 0.642242 Train acc: 0.726667\n",
      "Epoch: 95/500 Iteration: 860 Train loss: 0.607230 Train acc: 0.748333\n",
      "Epoch: 96/500 Iteration: 865 Train loss: 0.563937 Train acc: 0.763333\n",
      "Epoch: 96/500 Iteration: 870 Train loss: 0.602813 Train acc: 0.753333\n",
      "Epoch: 97/500 Iteration: 875 Train loss: 0.606449 Train acc: 0.750000\n",
      "Epoch: 97/500 Iteration: 875 Validation loss: 0.523369 Validation acc: 0.777500\n",
      "Epoch: 97/500 Iteration: 880 Train loss: 0.582360 Train acc: 0.758333\n",
      "Epoch: 98/500 Iteration: 885 Train loss: 0.612169 Train acc: 0.751667\n",
      "Epoch: 98/500 Iteration: 890 Train loss: 0.632710 Train acc: 0.738333\n",
      "Epoch: 99/500 Iteration: 895 Train loss: 0.582598 Train acc: 0.766667\n",
      "Epoch: 99/500 Iteration: 900 Train loss: 0.619099 Train acc: 0.733333\n",
      "Epoch: 99/500 Iteration: 900 Validation loss: 0.501148 Validation acc: 0.790833\n",
      "Epoch: 100/500 Iteration: 905 Train loss: 0.609031 Train acc: 0.753333\n",
      "Epoch: 101/500 Iteration: 910 Train loss: 0.541086 Train acc: 0.775000\n",
      "Epoch: 101/500 Iteration: 915 Train loss: 0.581336 Train acc: 0.771667\n",
      "Epoch: 102/500 Iteration: 920 Train loss: 0.579965 Train acc: 0.745000\n",
      "Epoch: 102/500 Iteration: 925 Train loss: 0.554240 Train acc: 0.780000\n",
      "Epoch: 102/500 Iteration: 925 Validation loss: 0.502499 Validation acc: 0.783333\n",
      "Epoch: 103/500 Iteration: 930 Train loss: 0.604340 Train acc: 0.753333\n",
      "Epoch: 103/500 Iteration: 935 Train loss: 0.600273 Train acc: 0.770000\n",
      "Epoch: 104/500 Iteration: 940 Train loss: 0.556060 Train acc: 0.766667\n",
      "Epoch: 104/500 Iteration: 945 Train loss: 0.589718 Train acc: 0.746667\n",
      "Epoch: 105/500 Iteration: 950 Train loss: 0.548898 Train acc: 0.818333\n",
      "Epoch: 105/500 Iteration: 950 Validation loss: 0.471968 Validation acc: 0.797500\n",
      "Epoch: 106/500 Iteration: 955 Train loss: 0.531777 Train acc: 0.770000\n",
      "Epoch: 106/500 Iteration: 960 Train loss: 0.550177 Train acc: 0.785000\n",
      "Epoch: 107/500 Iteration: 965 Train loss: 0.584848 Train acc: 0.750000\n",
      "Epoch: 107/500 Iteration: 970 Train loss: 0.529136 Train acc: 0.785000\n",
      "Epoch: 108/500 Iteration: 975 Train loss: 0.561351 Train acc: 0.753333\n",
      "Epoch: 108/500 Iteration: 975 Validation loss: 0.453327 Validation acc: 0.800833\n",
      "Epoch: 108/500 Iteration: 980 Train loss: 0.553818 Train acc: 0.756667\n",
      "Epoch: 109/500 Iteration: 985 Train loss: 0.550745 Train acc: 0.773333\n",
      "Epoch: 109/500 Iteration: 990 Train loss: 0.546608 Train acc: 0.771667\n",
      "Epoch: 110/500 Iteration: 995 Train loss: 0.533869 Train acc: 0.786667\n",
      "Epoch: 111/500 Iteration: 1000 Train loss: 0.493687 Train acc: 0.803333\n",
      "Epoch: 111/500 Iteration: 1000 Validation loss: 0.446043 Validation acc: 0.804167\n",
      "Epoch: 111/500 Iteration: 1005 Train loss: 0.492862 Train acc: 0.798333\n",
      "Epoch: 112/500 Iteration: 1010 Train loss: 0.529616 Train acc: 0.780000\n",
      "Epoch: 112/500 Iteration: 1015 Train loss: 0.512941 Train acc: 0.775000\n",
      "Epoch: 113/500 Iteration: 1020 Train loss: 0.513655 Train acc: 0.776667\n",
      "Epoch: 113/500 Iteration: 1025 Train loss: 0.526387 Train acc: 0.780000\n",
      "Epoch: 113/500 Iteration: 1025 Validation loss: 0.439393 Validation acc: 0.793333\n",
      "Epoch: 114/500 Iteration: 1030 Train loss: 0.518940 Train acc: 0.781667\n",
      "Epoch: 114/500 Iteration: 1035 Train loss: 0.529811 Train acc: 0.755000\n",
      "Epoch: 115/500 Iteration: 1040 Train loss: 0.480018 Train acc: 0.770000\n",
      "Epoch: 116/500 Iteration: 1045 Train loss: 0.444420 Train acc: 0.816667\n",
      "Epoch: 116/500 Iteration: 1050 Train loss: 0.484595 Train acc: 0.811667\n",
      "Epoch: 116/500 Iteration: 1050 Validation loss: 0.415926 Validation acc: 0.800833\n",
      "Epoch: 117/500 Iteration: 1055 Train loss: 0.513611 Train acc: 0.775000\n",
      "Epoch: 117/500 Iteration: 1060 Train loss: 0.471126 Train acc: 0.806667\n",
      "Epoch: 118/500 Iteration: 1065 Train loss: 0.500816 Train acc: 0.790000\n",
      "Epoch: 118/500 Iteration: 1070 Train loss: 0.494348 Train acc: 0.796667\n",
      "Epoch: 119/500 Iteration: 1075 Train loss: 0.503994 Train acc: 0.806667\n",
      "Epoch: 119/500 Iteration: 1075 Validation loss: 0.410876 Validation acc: 0.799167\n",
      "Epoch: 119/500 Iteration: 1080 Train loss: 0.508216 Train acc: 0.771667\n",
      "Epoch: 120/500 Iteration: 1085 Train loss: 0.470308 Train acc: 0.808333\n",
      "Epoch: 121/500 Iteration: 1090 Train loss: 0.457027 Train acc: 0.818333\n",
      "Epoch: 121/500 Iteration: 1095 Train loss: 0.490558 Train acc: 0.793333\n",
      "Epoch: 122/500 Iteration: 1100 Train loss: 0.483393 Train acc: 0.780000\n",
      "Epoch: 122/500 Iteration: 1100 Validation loss: 0.400707 Validation acc: 0.805833\n",
      "Epoch: 122/500 Iteration: 1105 Train loss: 0.464363 Train acc: 0.801667\n",
      "Epoch: 123/500 Iteration: 1110 Train loss: 0.506420 Train acc: 0.760000\n",
      "Epoch: 123/500 Iteration: 1115 Train loss: 0.490925 Train acc: 0.793333\n",
      "Epoch: 124/500 Iteration: 1120 Train loss: 0.459290 Train acc: 0.790000\n",
      "Epoch: 124/500 Iteration: 1125 Train loss: 0.523718 Train acc: 0.761667\n",
      "Epoch: 124/500 Iteration: 1125 Validation loss: 0.395449 Validation acc: 0.806667\n",
      "Epoch: 125/500 Iteration: 1130 Train loss: 0.475867 Train acc: 0.796667\n",
      "Epoch: 126/500 Iteration: 1135 Train loss: 0.421245 Train acc: 0.811667\n",
      "Epoch: 126/500 Iteration: 1140 Train loss: 0.490454 Train acc: 0.790000\n",
      "Epoch: 127/500 Iteration: 1145 Train loss: 0.501403 Train acc: 0.791667\n",
      "Epoch: 127/500 Iteration: 1150 Train loss: 0.477026 Train acc: 0.796667\n",
      "Epoch: 127/500 Iteration: 1150 Validation loss: 0.395199 Validation acc: 0.805833\n",
      "Epoch: 128/500 Iteration: 1155 Train loss: 0.487834 Train acc: 0.776667\n",
      "Epoch: 128/500 Iteration: 1160 Train loss: 0.474191 Train acc: 0.796667\n",
      "Epoch: 129/500 Iteration: 1165 Train loss: 0.450527 Train acc: 0.808333\n",
      "Epoch: 129/500 Iteration: 1170 Train loss: 0.498980 Train acc: 0.758333\n",
      "Epoch: 130/500 Iteration: 1175 Train loss: 0.450488 Train acc: 0.801667\n",
      "Epoch: 130/500 Iteration: 1175 Validation loss: 0.395724 Validation acc: 0.803333\n",
      "Epoch: 131/500 Iteration: 1180 Train loss: 0.440916 Train acc: 0.791667\n",
      "Epoch: 131/500 Iteration: 1185 Train loss: 0.448310 Train acc: 0.823333\n",
      "Epoch: 132/500 Iteration: 1190 Train loss: 0.470198 Train acc: 0.788333\n",
      "Epoch: 132/500 Iteration: 1195 Train loss: 0.445826 Train acc: 0.786667\n",
      "Epoch: 133/500 Iteration: 1200 Train loss: 0.466951 Train acc: 0.790000\n",
      "Epoch: 133/500 Iteration: 1200 Validation loss: 0.386854 Validation acc: 0.805000\n",
      "Epoch: 133/500 Iteration: 1205 Train loss: 0.483090 Train acc: 0.793333\n",
      "Epoch: 134/500 Iteration: 1210 Train loss: 0.445145 Train acc: 0.810000\n",
      "Epoch: 134/500 Iteration: 1215 Train loss: 0.475535 Train acc: 0.790000\n",
      "Epoch: 135/500 Iteration: 1220 Train loss: 0.457215 Train acc: 0.796667\n",
      "Epoch: 136/500 Iteration: 1225 Train loss: 0.435040 Train acc: 0.805000\n",
      "Epoch: 136/500 Iteration: 1225 Validation loss: 0.386796 Validation acc: 0.805000\n",
      "Epoch: 136/500 Iteration: 1230 Train loss: 0.453587 Train acc: 0.805000\n",
      "Epoch: 137/500 Iteration: 1235 Train loss: 0.476439 Train acc: 0.778333\n",
      "Epoch: 137/500 Iteration: 1240 Train loss: 0.420325 Train acc: 0.821667\n",
      "Epoch: 138/500 Iteration: 1245 Train loss: 0.466625 Train acc: 0.796667\n",
      "Epoch: 138/500 Iteration: 1250 Train loss: 0.477966 Train acc: 0.783333\n",
      "Epoch: 138/500 Iteration: 1250 Validation loss: 0.385526 Validation acc: 0.810833\n",
      "Epoch: 139/500 Iteration: 1255 Train loss: 0.485335 Train acc: 0.783333\n",
      "Epoch: 139/500 Iteration: 1260 Train loss: 0.445342 Train acc: 0.811667\n",
      "Epoch: 140/500 Iteration: 1265 Train loss: 0.449427 Train acc: 0.798333\n",
      "Epoch: 141/500 Iteration: 1270 Train loss: 0.409867 Train acc: 0.811667\n",
      "Epoch: 141/500 Iteration: 1275 Train loss: 0.431342 Train acc: 0.820000\n",
      "Epoch: 141/500 Iteration: 1275 Validation loss: 0.378677 Validation acc: 0.809167\n",
      "Epoch: 142/500 Iteration: 1280 Train loss: 0.458076 Train acc: 0.783333\n",
      "Epoch: 142/500 Iteration: 1285 Train loss: 0.451424 Train acc: 0.791667\n",
      "Epoch: 143/500 Iteration: 1290 Train loss: 0.466410 Train acc: 0.781667\n",
      "Epoch: 143/500 Iteration: 1295 Train loss: 0.471995 Train acc: 0.793333\n",
      "Epoch: 144/500 Iteration: 1300 Train loss: 0.438899 Train acc: 0.801667\n",
      "Epoch: 144/500 Iteration: 1300 Validation loss: 0.383418 Validation acc: 0.805000\n",
      "Epoch: 144/500 Iteration: 1305 Train loss: 0.473815 Train acc: 0.780000\n",
      "Epoch: 145/500 Iteration: 1310 Train loss: 0.436562 Train acc: 0.805000\n",
      "Epoch: 146/500 Iteration: 1315 Train loss: 0.400865 Train acc: 0.826667\n",
      "Epoch: 146/500 Iteration: 1320 Train loss: 0.426602 Train acc: 0.823333\n",
      "Epoch: 147/500 Iteration: 1325 Train loss: 0.448460 Train acc: 0.790000\n",
      "Epoch: 147/500 Iteration: 1325 Validation loss: 0.377612 Validation acc: 0.811667\n",
      "Epoch: 147/500 Iteration: 1330 Train loss: 0.412550 Train acc: 0.810000\n",
      "Epoch: 148/500 Iteration: 1335 Train loss: 0.438112 Train acc: 0.795000\n",
      "Epoch: 148/500 Iteration: 1340 Train loss: 0.476436 Train acc: 0.803333\n",
      "Epoch: 149/500 Iteration: 1345 Train loss: 0.461069 Train acc: 0.786667\n",
      "Epoch: 149/500 Iteration: 1350 Train loss: 0.450274 Train acc: 0.793333\n",
      "Epoch: 149/500 Iteration: 1350 Validation loss: 0.379222 Validation acc: 0.807500\n",
      "Epoch: 150/500 Iteration: 1355 Train loss: 0.442595 Train acc: 0.796667\n",
      "Epoch: 151/500 Iteration: 1360 Train loss: 0.393227 Train acc: 0.831667\n",
      "Epoch: 151/500 Iteration: 1365 Train loss: 0.404506 Train acc: 0.840000\n",
      "Epoch: 152/500 Iteration: 1370 Train loss: 0.448292 Train acc: 0.800000\n",
      "Epoch: 152/500 Iteration: 1375 Train loss: 0.434826 Train acc: 0.806667\n",
      "Epoch: 152/500 Iteration: 1375 Validation loss: 0.375693 Validation acc: 0.809167\n",
      "Epoch: 153/500 Iteration: 1380 Train loss: 0.474085 Train acc: 0.785000\n",
      "Epoch: 153/500 Iteration: 1385 Train loss: 0.469664 Train acc: 0.811667\n",
      "Epoch: 154/500 Iteration: 1390 Train loss: 0.434413 Train acc: 0.805000\n",
      "Epoch: 154/500 Iteration: 1395 Train loss: 0.450877 Train acc: 0.801667\n",
      "Epoch: 155/500 Iteration: 1400 Train loss: 0.422540 Train acc: 0.803333\n",
      "Epoch: 155/500 Iteration: 1400 Validation loss: 0.371183 Validation acc: 0.810000\n",
      "Epoch: 156/500 Iteration: 1405 Train loss: 0.392025 Train acc: 0.836667\n",
      "Epoch: 156/500 Iteration: 1410 Train loss: 0.423038 Train acc: 0.813333\n",
      "Epoch: 157/500 Iteration: 1415 Train loss: 0.456686 Train acc: 0.783333\n",
      "Epoch: 157/500 Iteration: 1420 Train loss: 0.431791 Train acc: 0.785000\n",
      "Epoch: 158/500 Iteration: 1425 Train loss: 0.440844 Train acc: 0.790000\n",
      "Epoch: 158/500 Iteration: 1425 Validation loss: 0.369418 Validation acc: 0.810833\n",
      "Epoch: 158/500 Iteration: 1430 Train loss: 0.453029 Train acc: 0.800000\n",
      "Epoch: 159/500 Iteration: 1435 Train loss: 0.448317 Train acc: 0.808333\n",
      "Epoch: 159/500 Iteration: 1440 Train loss: 0.440871 Train acc: 0.786667\n",
      "Epoch: 160/500 Iteration: 1445 Train loss: 0.417528 Train acc: 0.801667\n",
      "Epoch: 161/500 Iteration: 1450 Train loss: 0.378355 Train acc: 0.833333\n",
      "Epoch: 161/500 Iteration: 1450 Validation loss: 0.362773 Validation acc: 0.817500\n",
      "Epoch: 161/500 Iteration: 1455 Train loss: 0.439198 Train acc: 0.800000\n",
      "Epoch: 162/500 Iteration: 1460 Train loss: 0.444908 Train acc: 0.800000\n",
      "Epoch: 162/500 Iteration: 1465 Train loss: 0.421884 Train acc: 0.785000\n",
      "Epoch: 163/500 Iteration: 1470 Train loss: 0.438341 Train acc: 0.796667\n",
      "Epoch: 163/500 Iteration: 1475 Train loss: 0.461866 Train acc: 0.801667\n",
      "Epoch: 163/500 Iteration: 1475 Validation loss: 0.370930 Validation acc: 0.815833\n",
      "Epoch: 164/500 Iteration: 1480 Train loss: 0.418540 Train acc: 0.810000\n",
      "Epoch: 164/500 Iteration: 1485 Train loss: 0.481447 Train acc: 0.781667\n",
      "Epoch: 165/500 Iteration: 1490 Train loss: 0.428840 Train acc: 0.811667\n",
      "Epoch: 166/500 Iteration: 1495 Train loss: 0.413768 Train acc: 0.800000\n",
      "Epoch: 166/500 Iteration: 1500 Train loss: 0.420666 Train acc: 0.830000\n",
      "Epoch: 166/500 Iteration: 1500 Validation loss: 0.359144 Validation acc: 0.830000\n",
      "Epoch: 167/500 Iteration: 1505 Train loss: 0.427158 Train acc: 0.800000\n",
      "Epoch: 167/500 Iteration: 1510 Train loss: 0.417950 Train acc: 0.815000\n",
      "Epoch: 168/500 Iteration: 1515 Train loss: 0.462703 Train acc: 0.803333\n",
      "Epoch: 168/500 Iteration: 1520 Train loss: 0.451464 Train acc: 0.826667\n",
      "Epoch: 169/500 Iteration: 1525 Train loss: 0.425774 Train acc: 0.803333\n",
      "Epoch: 169/500 Iteration: 1525 Validation loss: 0.362716 Validation acc: 0.832500\n",
      "Epoch: 169/500 Iteration: 1530 Train loss: 0.443949 Train acc: 0.788333\n",
      "Epoch: 170/500 Iteration: 1535 Train loss: 0.414510 Train acc: 0.808333\n",
      "Epoch: 171/500 Iteration: 1540 Train loss: 0.373395 Train acc: 0.848333\n",
      "Epoch: 171/500 Iteration: 1545 Train loss: 0.411164 Train acc: 0.826667\n",
      "Epoch: 172/500 Iteration: 1550 Train loss: 0.416509 Train acc: 0.803333\n",
      "Epoch: 172/500 Iteration: 1550 Validation loss: 0.357106 Validation acc: 0.834167\n",
      "Epoch: 172/500 Iteration: 1555 Train loss: 0.379928 Train acc: 0.806667\n",
      "Epoch: 173/500 Iteration: 1560 Train loss: 0.434805 Train acc: 0.796667\n",
      "Epoch: 173/500 Iteration: 1565 Train loss: 0.453755 Train acc: 0.803333\n",
      "Epoch: 174/500 Iteration: 1570 Train loss: 0.408886 Train acc: 0.835000\n",
      "Epoch: 174/500 Iteration: 1575 Train loss: 0.430982 Train acc: 0.806667\n",
      "Epoch: 174/500 Iteration: 1575 Validation loss: 0.356245 Validation acc: 0.830833\n",
      "Epoch: 175/500 Iteration: 1580 Train loss: 0.411504 Train acc: 0.801667\n",
      "Epoch: 176/500 Iteration: 1585 Train loss: 0.392714 Train acc: 0.808333\n",
      "Epoch: 176/500 Iteration: 1590 Train loss: 0.396168 Train acc: 0.823333\n",
      "Epoch: 177/500 Iteration: 1595 Train loss: 0.419757 Train acc: 0.808333\n",
      "Epoch: 177/500 Iteration: 1600 Train loss: 0.412527 Train acc: 0.818333\n",
      "Epoch: 177/500 Iteration: 1600 Validation loss: 0.352642 Validation acc: 0.835000\n",
      "Epoch: 178/500 Iteration: 1605 Train loss: 0.459076 Train acc: 0.786667\n",
      "Epoch: 178/500 Iteration: 1610 Train loss: 0.434858 Train acc: 0.808333\n",
      "Epoch: 179/500 Iteration: 1615 Train loss: 0.421897 Train acc: 0.811667\n",
      "Epoch: 179/500 Iteration: 1620 Train loss: 0.439151 Train acc: 0.793333\n",
      "Epoch: 180/500 Iteration: 1625 Train loss: 0.412584 Train acc: 0.803333\n",
      "Epoch: 180/500 Iteration: 1625 Validation loss: 0.351717 Validation acc: 0.837500\n",
      "Epoch: 181/500 Iteration: 1630 Train loss: 0.366220 Train acc: 0.841667\n",
      "Epoch: 181/500 Iteration: 1635 Train loss: 0.404297 Train acc: 0.826667\n",
      "Epoch: 182/500 Iteration: 1640 Train loss: 0.414555 Train acc: 0.806667\n",
      "Epoch: 182/500 Iteration: 1645 Train loss: 0.409811 Train acc: 0.798333\n",
      "Epoch: 183/500 Iteration: 1650 Train loss: 0.435423 Train acc: 0.798333\n",
      "Epoch: 183/500 Iteration: 1650 Validation loss: 0.349112 Validation acc: 0.845000\n",
      "Epoch: 183/500 Iteration: 1655 Train loss: 0.451991 Train acc: 0.803333\n",
      "Epoch: 184/500 Iteration: 1660 Train loss: 0.392753 Train acc: 0.831667\n",
      "Epoch: 184/500 Iteration: 1665 Train loss: 0.440115 Train acc: 0.795000\n",
      "Epoch: 185/500 Iteration: 1670 Train loss: 0.397694 Train acc: 0.818333\n",
      "Epoch: 186/500 Iteration: 1675 Train loss: 0.359026 Train acc: 0.848333\n",
      "Epoch: 186/500 Iteration: 1675 Validation loss: 0.345041 Validation acc: 0.847500\n",
      "Epoch: 186/500 Iteration: 1680 Train loss: 0.393105 Train acc: 0.830000\n",
      "Epoch: 187/500 Iteration: 1685 Train loss: 0.420718 Train acc: 0.796667\n",
      "Epoch: 187/500 Iteration: 1690 Train loss: 0.385373 Train acc: 0.823333\n",
      "Epoch: 188/500 Iteration: 1695 Train loss: 0.417653 Train acc: 0.805000\n",
      "Epoch: 188/500 Iteration: 1700 Train loss: 0.428345 Train acc: 0.803333\n",
      "Epoch: 188/500 Iteration: 1700 Validation loss: 0.343506 Validation acc: 0.846667\n",
      "Epoch: 189/500 Iteration: 1705 Train loss: 0.416847 Train acc: 0.791667\n",
      "Epoch: 189/500 Iteration: 1710 Train loss: 0.438828 Train acc: 0.800000\n",
      "Epoch: 190/500 Iteration: 1715 Train loss: 0.383060 Train acc: 0.835000\n",
      "Epoch: 191/500 Iteration: 1720 Train loss: 0.382177 Train acc: 0.820000\n",
      "Epoch: 191/500 Iteration: 1725 Train loss: 0.388061 Train acc: 0.841667\n",
      "Epoch: 191/500 Iteration: 1725 Validation loss: 0.342220 Validation acc: 0.850000\n",
      "Epoch: 192/500 Iteration: 1730 Train loss: 0.411995 Train acc: 0.801667\n",
      "Epoch: 192/500 Iteration: 1735 Train loss: 0.390355 Train acc: 0.831667\n",
      "Epoch: 193/500 Iteration: 1740 Train loss: 0.420532 Train acc: 0.808333\n",
      "Epoch: 193/500 Iteration: 1745 Train loss: 0.430869 Train acc: 0.790000\n",
      "Epoch: 194/500 Iteration: 1750 Train loss: 0.431347 Train acc: 0.801667\n",
      "Epoch: 194/500 Iteration: 1750 Validation loss: 0.340308 Validation acc: 0.849167\n",
      "Epoch: 194/500 Iteration: 1755 Train loss: 0.418452 Train acc: 0.816667\n",
      "Epoch: 195/500 Iteration: 1760 Train loss: 0.402374 Train acc: 0.813333\n",
      "Epoch: 196/500 Iteration: 1765 Train loss: 0.352023 Train acc: 0.843333\n",
      "Epoch: 196/500 Iteration: 1770 Train loss: 0.379925 Train acc: 0.841667\n",
      "Epoch: 197/500 Iteration: 1775 Train loss: 0.388356 Train acc: 0.818333\n",
      "Epoch: 197/500 Iteration: 1775 Validation loss: 0.342493 Validation acc: 0.846667\n",
      "Epoch: 197/500 Iteration: 1780 Train loss: 0.367206 Train acc: 0.833333\n",
      "Epoch: 198/500 Iteration: 1785 Train loss: 0.424497 Train acc: 0.798333\n",
      "Epoch: 198/500 Iteration: 1790 Train loss: 0.439323 Train acc: 0.811667\n",
      "Epoch: 199/500 Iteration: 1795 Train loss: 0.393564 Train acc: 0.833333\n",
      "Epoch: 199/500 Iteration: 1800 Train loss: 0.421825 Train acc: 0.791667\n",
      "Epoch: 199/500 Iteration: 1800 Validation loss: 0.340752 Validation acc: 0.848333\n",
      "Epoch: 200/500 Iteration: 1805 Train loss: 0.388042 Train acc: 0.823333\n",
      "Epoch: 201/500 Iteration: 1810 Train loss: 0.360608 Train acc: 0.850000\n",
      "Epoch: 201/500 Iteration: 1815 Train loss: 0.379645 Train acc: 0.853333\n",
      "Epoch: 202/500 Iteration: 1820 Train loss: 0.396653 Train acc: 0.816667\n",
      "Epoch: 202/500 Iteration: 1825 Train loss: 0.369376 Train acc: 0.826667\n",
      "Epoch: 202/500 Iteration: 1825 Validation loss: 0.334579 Validation acc: 0.850833\n",
      "Epoch: 203/500 Iteration: 1830 Train loss: 0.411081 Train acc: 0.808333\n",
      "Epoch: 203/500 Iteration: 1835 Train loss: 0.404923 Train acc: 0.815000\n",
      "Epoch: 204/500 Iteration: 1840 Train loss: 0.399209 Train acc: 0.816667\n",
      "Epoch: 204/500 Iteration: 1845 Train loss: 0.435475 Train acc: 0.815000\n",
      "Epoch: 205/500 Iteration: 1850 Train loss: 0.370042 Train acc: 0.853333\n",
      "Epoch: 205/500 Iteration: 1850 Validation loss: 0.333050 Validation acc: 0.854167\n",
      "Epoch: 206/500 Iteration: 1855 Train loss: 0.348449 Train acc: 0.856667\n",
      "Epoch: 206/500 Iteration: 1860 Train loss: 0.385878 Train acc: 0.843333\n",
      "Epoch: 207/500 Iteration: 1865 Train loss: 0.398101 Train acc: 0.840000\n",
      "Epoch: 207/500 Iteration: 1870 Train loss: 0.351206 Train acc: 0.848333\n",
      "Epoch: 208/500 Iteration: 1875 Train loss: 0.422493 Train acc: 0.811667\n",
      "Epoch: 208/500 Iteration: 1875 Validation loss: 0.330843 Validation acc: 0.855000\n",
      "Epoch: 208/500 Iteration: 1880 Train loss: 0.419711 Train acc: 0.806667\n",
      "Epoch: 209/500 Iteration: 1885 Train loss: 0.379254 Train acc: 0.836667\n",
      "Epoch: 209/500 Iteration: 1890 Train loss: 0.410186 Train acc: 0.820000\n",
      "Epoch: 210/500 Iteration: 1895 Train loss: 0.380461 Train acc: 0.841667\n",
      "Epoch: 211/500 Iteration: 1900 Train loss: 0.337994 Train acc: 0.851667\n",
      "Epoch: 211/500 Iteration: 1900 Validation loss: 0.329168 Validation acc: 0.858333\n",
      "Epoch: 211/500 Iteration: 1905 Train loss: 0.392488 Train acc: 0.851667\n",
      "Epoch: 212/500 Iteration: 1910 Train loss: 0.383828 Train acc: 0.835000\n",
      "Epoch: 212/500 Iteration: 1915 Train loss: 0.364028 Train acc: 0.836667\n",
      "Epoch: 213/500 Iteration: 1920 Train loss: 0.409068 Train acc: 0.816667\n",
      "Epoch: 213/500 Iteration: 1925 Train loss: 0.419394 Train acc: 0.836667\n",
      "Epoch: 213/500 Iteration: 1925 Validation loss: 0.326547 Validation acc: 0.856667\n",
      "Epoch: 214/500 Iteration: 1930 Train loss: 0.386036 Train acc: 0.841667\n",
      "Epoch: 214/500 Iteration: 1935 Train loss: 0.385923 Train acc: 0.821667\n",
      "Epoch: 215/500 Iteration: 1940 Train loss: 0.380476 Train acc: 0.833333\n",
      "Epoch: 216/500 Iteration: 1945 Train loss: 0.336176 Train acc: 0.853333\n",
      "Epoch: 216/500 Iteration: 1950 Train loss: 0.349496 Train acc: 0.880000\n",
      "Epoch: 216/500 Iteration: 1950 Validation loss: 0.324356 Validation acc: 0.865000\n",
      "Epoch: 217/500 Iteration: 1955 Train loss: 0.389078 Train acc: 0.828333\n",
      "Epoch: 217/500 Iteration: 1960 Train loss: 0.361901 Train acc: 0.853333\n",
      "Epoch: 218/500 Iteration: 1965 Train loss: 0.397189 Train acc: 0.825000\n",
      "Epoch: 218/500 Iteration: 1970 Train loss: 0.398776 Train acc: 0.836667\n",
      "Epoch: 219/500 Iteration: 1975 Train loss: 0.384605 Train acc: 0.843333\n",
      "Epoch: 219/500 Iteration: 1975 Validation loss: 0.319928 Validation acc: 0.866667\n",
      "Epoch: 219/500 Iteration: 1980 Train loss: 0.391838 Train acc: 0.840000\n",
      "Epoch: 220/500 Iteration: 1985 Train loss: 0.371475 Train acc: 0.851667\n",
      "Epoch: 221/500 Iteration: 1990 Train loss: 0.336111 Train acc: 0.865000\n",
      "Epoch: 221/500 Iteration: 1995 Train loss: 0.365721 Train acc: 0.853333\n",
      "Epoch: 222/500 Iteration: 2000 Train loss: 0.378017 Train acc: 0.823333\n",
      "Epoch: 222/500 Iteration: 2000 Validation loss: 0.321514 Validation acc: 0.866667\n",
      "Epoch: 222/500 Iteration: 2005 Train loss: 0.351099 Train acc: 0.835000\n",
      "Epoch: 223/500 Iteration: 2010 Train loss: 0.380802 Train acc: 0.846667\n",
      "Epoch: 223/500 Iteration: 2015 Train loss: 0.394828 Train acc: 0.833333\n",
      "Epoch: 224/500 Iteration: 2020 Train loss: 0.390201 Train acc: 0.836667\n",
      "Epoch: 224/500 Iteration: 2025 Train loss: 0.394249 Train acc: 0.816667\n",
      "Epoch: 224/500 Iteration: 2025 Validation loss: 0.323302 Validation acc: 0.855000\n",
      "Epoch: 225/500 Iteration: 2030 Train loss: 0.348623 Train acc: 0.865000\n",
      "Epoch: 226/500 Iteration: 2035 Train loss: 0.314787 Train acc: 0.891667\n",
      "Epoch: 226/500 Iteration: 2040 Train loss: 0.349460 Train acc: 0.870000\n",
      "Epoch: 227/500 Iteration: 2045 Train loss: 0.380142 Train acc: 0.838333\n",
      "Epoch: 227/500 Iteration: 2050 Train loss: 0.366664 Train acc: 0.855000\n",
      "Epoch: 227/500 Iteration: 2050 Validation loss: 0.313226 Validation acc: 0.861667\n",
      "Epoch: 228/500 Iteration: 2055 Train loss: 0.380500 Train acc: 0.838333\n",
      "Epoch: 228/500 Iteration: 2060 Train loss: 0.380528 Train acc: 0.843333\n",
      "Epoch: 229/500 Iteration: 2065 Train loss: 0.352225 Train acc: 0.868333\n",
      "Epoch: 229/500 Iteration: 2070 Train loss: 0.374108 Train acc: 0.846667\n",
      "Epoch: 230/500 Iteration: 2075 Train loss: 0.365580 Train acc: 0.860000\n",
      "Epoch: 230/500 Iteration: 2075 Validation loss: 0.312982 Validation acc: 0.866667\n",
      "Epoch: 231/500 Iteration: 2080 Train loss: 0.326687 Train acc: 0.865000\n",
      "Epoch: 231/500 Iteration: 2085 Train loss: 0.331955 Train acc: 0.885000\n",
      "Epoch: 232/500 Iteration: 2090 Train loss: 0.372668 Train acc: 0.833333\n",
      "Epoch: 232/500 Iteration: 2095 Train loss: 0.341334 Train acc: 0.860000\n",
      "Epoch: 233/500 Iteration: 2100 Train loss: 0.375662 Train acc: 0.845000\n",
      "Epoch: 233/500 Iteration: 2100 Validation loss: 0.298518 Validation acc: 0.894167\n",
      "Epoch: 233/500 Iteration: 2105 Train loss: 0.377692 Train acc: 0.850000\n",
      "Epoch: 234/500 Iteration: 2110 Train loss: 0.357952 Train acc: 0.856667\n",
      "Epoch: 234/500 Iteration: 2115 Train loss: 0.391254 Train acc: 0.855000\n",
      "Epoch: 235/500 Iteration: 2120 Train loss: 0.317714 Train acc: 0.878333\n",
      "Epoch: 236/500 Iteration: 2125 Train loss: 0.342705 Train acc: 0.850000\n",
      "Epoch: 236/500 Iteration: 2125 Validation loss: 0.303366 Validation acc: 0.878333\n",
      "Epoch: 236/500 Iteration: 2130 Train loss: 0.322353 Train acc: 0.875000\n",
      "Epoch: 237/500 Iteration: 2135 Train loss: 0.373267 Train acc: 0.851667\n",
      "Epoch: 237/500 Iteration: 2140 Train loss: 0.330356 Train acc: 0.881667\n",
      "Epoch: 238/500 Iteration: 2145 Train loss: 0.384468 Train acc: 0.846667\n",
      "Epoch: 238/500 Iteration: 2150 Train loss: 0.376977 Train acc: 0.860000\n",
      "Epoch: 238/500 Iteration: 2150 Validation loss: 0.297916 Validation acc: 0.889167\n",
      "Epoch: 239/500 Iteration: 2155 Train loss: 0.376371 Train acc: 0.855000\n",
      "Epoch: 239/500 Iteration: 2160 Train loss: 0.350828 Train acc: 0.871667\n",
      "Epoch: 240/500 Iteration: 2165 Train loss: 0.335576 Train acc: 0.876667\n",
      "Epoch: 241/500 Iteration: 2170 Train loss: 0.297342 Train acc: 0.883333\n",
      "Epoch: 241/500 Iteration: 2175 Train loss: 0.350396 Train acc: 0.886667\n",
      "Epoch: 241/500 Iteration: 2175 Validation loss: 0.302444 Validation acc: 0.874167\n",
      "Epoch: 242/500 Iteration: 2180 Train loss: 0.355848 Train acc: 0.858333\n",
      "Epoch: 242/500 Iteration: 2185 Train loss: 0.321401 Train acc: 0.875000\n",
      "Epoch: 243/500 Iteration: 2190 Train loss: 0.357405 Train acc: 0.855000\n",
      "Epoch: 243/500 Iteration: 2195 Train loss: 0.371903 Train acc: 0.868333\n",
      "Epoch: 244/500 Iteration: 2200 Train loss: 0.361112 Train acc: 0.853333\n",
      "Epoch: 244/500 Iteration: 2200 Validation loss: 0.280787 Validation acc: 0.907500\n",
      "Epoch: 244/500 Iteration: 2205 Train loss: 0.342010 Train acc: 0.870000\n",
      "Epoch: 245/500 Iteration: 2210 Train loss: 0.317697 Train acc: 0.886667\n",
      "Epoch: 246/500 Iteration: 2215 Train loss: 0.302681 Train acc: 0.906667\n",
      "Epoch: 246/500 Iteration: 2220 Train loss: 0.311633 Train acc: 0.891667\n",
      "Epoch: 247/500 Iteration: 2225 Train loss: 0.345047 Train acc: 0.875000\n",
      "Epoch: 247/500 Iteration: 2225 Validation loss: 0.280105 Validation acc: 0.902500\n",
      "Epoch: 247/500 Iteration: 2230 Train loss: 0.342983 Train acc: 0.883333\n",
      "Epoch: 248/500 Iteration: 2235 Train loss: 0.354828 Train acc: 0.858333\n",
      "Epoch: 248/500 Iteration: 2240 Train loss: 0.347845 Train acc: 0.876667\n",
      "Epoch: 249/500 Iteration: 2245 Train loss: 0.341209 Train acc: 0.888333\n",
      "Epoch: 249/500 Iteration: 2250 Train loss: 0.335260 Train acc: 0.878333\n",
      "Epoch: 249/500 Iteration: 2250 Validation loss: 0.276004 Validation acc: 0.907500\n",
      "Epoch: 250/500 Iteration: 2255 Train loss: 0.307009 Train acc: 0.893333\n",
      "Epoch: 251/500 Iteration: 2260 Train loss: 0.290254 Train acc: 0.921667\n",
      "Epoch: 251/500 Iteration: 2265 Train loss: 0.301486 Train acc: 0.913333\n",
      "Epoch: 252/500 Iteration: 2270 Train loss: 0.341442 Train acc: 0.876667\n",
      "Epoch: 252/500 Iteration: 2275 Train loss: 0.319125 Train acc: 0.900000\n",
      "Epoch: 252/500 Iteration: 2275 Validation loss: 0.278524 Validation acc: 0.902500\n",
      "Epoch: 253/500 Iteration: 2280 Train loss: 0.343999 Train acc: 0.881667\n",
      "Epoch: 253/500 Iteration: 2285 Train loss: 0.359135 Train acc: 0.868333\n",
      "Epoch: 254/500 Iteration: 2290 Train loss: 0.319741 Train acc: 0.901667\n",
      "Epoch: 254/500 Iteration: 2295 Train loss: 0.338255 Train acc: 0.876667\n",
      "Epoch: 255/500 Iteration: 2300 Train loss: 0.305955 Train acc: 0.900000\n",
      "Epoch: 255/500 Iteration: 2300 Validation loss: 0.266935 Validation acc: 0.915833\n",
      "Epoch: 256/500 Iteration: 2305 Train loss: 0.286245 Train acc: 0.916667\n",
      "Epoch: 256/500 Iteration: 2310 Train loss: 0.312446 Train acc: 0.886667\n",
      "Epoch: 257/500 Iteration: 2315 Train loss: 0.351613 Train acc: 0.861667\n",
      "Epoch: 257/500 Iteration: 2320 Train loss: 0.305080 Train acc: 0.903333\n",
      "Epoch: 258/500 Iteration: 2325 Train loss: 0.332096 Train acc: 0.881667\n",
      "Epoch: 258/500 Iteration: 2325 Validation loss: 0.256554 Validation acc: 0.921667\n",
      "Epoch: 258/500 Iteration: 2330 Train loss: 0.326085 Train acc: 0.886667\n",
      "Epoch: 259/500 Iteration: 2335 Train loss: 0.304964 Train acc: 0.901667\n",
      "Epoch: 259/500 Iteration: 2340 Train loss: 0.343079 Train acc: 0.880000\n",
      "Epoch: 260/500 Iteration: 2345 Train loss: 0.281475 Train acc: 0.926667\n",
      "Epoch: 261/500 Iteration: 2350 Train loss: 0.275119 Train acc: 0.918333\n",
      "Epoch: 261/500 Iteration: 2350 Validation loss: 0.255217 Validation acc: 0.920833\n",
      "Epoch: 261/500 Iteration: 2355 Train loss: 0.307244 Train acc: 0.895000\n",
      "Epoch: 262/500 Iteration: 2360 Train loss: 0.321300 Train acc: 0.890000\n",
      "Epoch: 262/500 Iteration: 2365 Train loss: 0.295716 Train acc: 0.905000\n",
      "Epoch: 263/500 Iteration: 2370 Train loss: 0.349225 Train acc: 0.885000\n",
      "Epoch: 263/500 Iteration: 2375 Train loss: 0.328616 Train acc: 0.895000\n",
      "Epoch: 263/500 Iteration: 2375 Validation loss: 0.251105 Validation acc: 0.922500\n",
      "Epoch: 264/500 Iteration: 2380 Train loss: 0.317467 Train acc: 0.891667\n",
      "Epoch: 264/500 Iteration: 2385 Train loss: 0.337782 Train acc: 0.886667\n",
      "Epoch: 265/500 Iteration: 2390 Train loss: 0.286119 Train acc: 0.908333\n",
      "Epoch: 266/500 Iteration: 2395 Train loss: 0.265064 Train acc: 0.925000\n",
      "Epoch: 266/500 Iteration: 2400 Train loss: 0.286991 Train acc: 0.916667\n",
      "Epoch: 266/500 Iteration: 2400 Validation loss: 0.247295 Validation acc: 0.920833\n",
      "Epoch: 267/500 Iteration: 2405 Train loss: 0.320433 Train acc: 0.901667\n",
      "Epoch: 267/500 Iteration: 2410 Train loss: 0.300563 Train acc: 0.905000\n",
      "Epoch: 268/500 Iteration: 2415 Train loss: 0.313813 Train acc: 0.903333\n",
      "Epoch: 268/500 Iteration: 2420 Train loss: 0.303177 Train acc: 0.893333\n",
      "Epoch: 269/500 Iteration: 2425 Train loss: 0.281320 Train acc: 0.921667\n",
      "Epoch: 269/500 Iteration: 2425 Validation loss: 0.249901 Validation acc: 0.914167\n",
      "Epoch: 269/500 Iteration: 2430 Train loss: 0.311055 Train acc: 0.896667\n",
      "Epoch: 270/500 Iteration: 2435 Train loss: 0.275327 Train acc: 0.913333\n",
      "Epoch: 271/500 Iteration: 2440 Train loss: 0.234804 Train acc: 0.948333\n",
      "Epoch: 271/500 Iteration: 2445 Train loss: 0.284922 Train acc: 0.923333\n",
      "Epoch: 272/500 Iteration: 2450 Train loss: 0.296928 Train acc: 0.913333\n",
      "Epoch: 272/500 Iteration: 2450 Validation loss: 0.235493 Validation acc: 0.928333\n",
      "Epoch: 272/500 Iteration: 2455 Train loss: 0.262716 Train acc: 0.923333\n",
      "Epoch: 273/500 Iteration: 2460 Train loss: 0.308443 Train acc: 0.905000\n",
      "Epoch: 273/500 Iteration: 2465 Train loss: 0.314805 Train acc: 0.901667\n",
      "Epoch: 274/500 Iteration: 2470 Train loss: 0.293409 Train acc: 0.920000\n",
      "Epoch: 274/500 Iteration: 2475 Train loss: 0.294043 Train acc: 0.910000\n",
      "Epoch: 274/500 Iteration: 2475 Validation loss: 0.236223 Validation acc: 0.920833\n",
      "Epoch: 275/500 Iteration: 2480 Train loss: 0.275420 Train acc: 0.915000\n",
      "Epoch: 276/500 Iteration: 2485 Train loss: 0.227200 Train acc: 0.933333\n",
      "Epoch: 276/500 Iteration: 2490 Train loss: 0.261346 Train acc: 0.925000\n",
      "Epoch: 277/500 Iteration: 2495 Train loss: 0.310861 Train acc: 0.901667\n",
      "Epoch: 277/500 Iteration: 2500 Train loss: 0.244986 Train acc: 0.921667\n",
      "Epoch: 277/500 Iteration: 2500 Validation loss: 0.236508 Validation acc: 0.919167\n",
      "Epoch: 278/500 Iteration: 2505 Train loss: 0.281558 Train acc: 0.915000\n",
      "Epoch: 278/500 Iteration: 2510 Train loss: 0.317800 Train acc: 0.883333\n",
      "Epoch: 279/500 Iteration: 2515 Train loss: 0.290532 Train acc: 0.915000\n",
      "Epoch: 279/500 Iteration: 2520 Train loss: 0.269293 Train acc: 0.920000\n",
      "Epoch: 280/500 Iteration: 2525 Train loss: 0.259355 Train acc: 0.923333\n",
      "Epoch: 280/500 Iteration: 2525 Validation loss: 0.224772 Validation acc: 0.927500\n",
      "Epoch: 281/500 Iteration: 2530 Train loss: 0.226367 Train acc: 0.943333\n",
      "Epoch: 281/500 Iteration: 2535 Train loss: 0.252560 Train acc: 0.928333\n",
      "Epoch: 282/500 Iteration: 2540 Train loss: 0.299558 Train acc: 0.901667\n",
      "Epoch: 282/500 Iteration: 2545 Train loss: 0.259823 Train acc: 0.928333\n",
      "Epoch: 283/500 Iteration: 2550 Train loss: 0.271347 Train acc: 0.915000\n",
      "Epoch: 283/500 Iteration: 2550 Validation loss: 0.216214 Validation acc: 0.931667\n",
      "Epoch: 283/500 Iteration: 2555 Train loss: 0.299751 Train acc: 0.903333\n",
      "Epoch: 284/500 Iteration: 2560 Train loss: 0.269681 Train acc: 0.928333\n",
      "Epoch: 284/500 Iteration: 2565 Train loss: 0.260199 Train acc: 0.925000\n",
      "Epoch: 285/500 Iteration: 2570 Train loss: 0.243858 Train acc: 0.931667\n",
      "Epoch: 286/500 Iteration: 2575 Train loss: 0.184518 Train acc: 0.958333\n",
      "Epoch: 286/500 Iteration: 2575 Validation loss: 0.215048 Validation acc: 0.930833\n",
      "Epoch: 286/500 Iteration: 2580 Train loss: 0.257533 Train acc: 0.941667\n",
      "Epoch: 287/500 Iteration: 2585 Train loss: 0.252800 Train acc: 0.928333\n",
      "Epoch: 287/500 Iteration: 2590 Train loss: 0.260921 Train acc: 0.915000\n",
      "Epoch: 288/500 Iteration: 2595 Train loss: 0.278771 Train acc: 0.911667\n",
      "Epoch: 288/500 Iteration: 2600 Train loss: 0.284092 Train acc: 0.918333\n",
      "Epoch: 288/500 Iteration: 2600 Validation loss: 0.212946 Validation acc: 0.926667\n",
      "Epoch: 289/500 Iteration: 2605 Train loss: 0.259152 Train acc: 0.930000\n",
      "Epoch: 289/500 Iteration: 2610 Train loss: 0.286082 Train acc: 0.910000\n",
      "Epoch: 290/500 Iteration: 2615 Train loss: 0.256388 Train acc: 0.925000\n",
      "Epoch: 291/500 Iteration: 2620 Train loss: 0.211186 Train acc: 0.946667\n",
      "Epoch: 291/500 Iteration: 2625 Train loss: 0.276973 Train acc: 0.926667\n",
      "Epoch: 291/500 Iteration: 2625 Validation loss: 0.209572 Validation acc: 0.929167\n",
      "Epoch: 292/500 Iteration: 2630 Train loss: 0.275358 Train acc: 0.911667\n",
      "Epoch: 292/500 Iteration: 2635 Train loss: 0.237950 Train acc: 0.921667\n",
      "Epoch: 293/500 Iteration: 2640 Train loss: 0.253224 Train acc: 0.915000\n",
      "Epoch: 293/500 Iteration: 2645 Train loss: 0.275706 Train acc: 0.908333\n",
      "Epoch: 294/500 Iteration: 2650 Train loss: 0.236479 Train acc: 0.938333\n",
      "Epoch: 294/500 Iteration: 2650 Validation loss: 0.207979 Validation acc: 0.933333\n",
      "Epoch: 294/500 Iteration: 2655 Train loss: 0.256929 Train acc: 0.933333\n",
      "Epoch: 295/500 Iteration: 2660 Train loss: 0.224299 Train acc: 0.945000\n",
      "Epoch: 296/500 Iteration: 2665 Train loss: 0.213733 Train acc: 0.945000\n",
      "Epoch: 296/500 Iteration: 2670 Train loss: 0.239286 Train acc: 0.940000\n",
      "Epoch: 297/500 Iteration: 2675 Train loss: 0.264260 Train acc: 0.925000\n",
      "Epoch: 297/500 Iteration: 2675 Validation loss: 0.200462 Validation acc: 0.931667\n",
      "Epoch: 297/500 Iteration: 2680 Train loss: 0.235195 Train acc: 0.925000\n",
      "Epoch: 298/500 Iteration: 2685 Train loss: 0.255526 Train acc: 0.923333\n",
      "Epoch: 298/500 Iteration: 2690 Train loss: 0.284440 Train acc: 0.911667\n",
      "Epoch: 299/500 Iteration: 2695 Train loss: 0.251972 Train acc: 0.943333\n",
      "Epoch: 299/500 Iteration: 2700 Train loss: 0.235238 Train acc: 0.936667\n",
      "Epoch: 299/500 Iteration: 2700 Validation loss: 0.197659 Validation acc: 0.933333\n",
      "Epoch: 300/500 Iteration: 2705 Train loss: 0.226874 Train acc: 0.938333\n",
      "Epoch: 301/500 Iteration: 2710 Train loss: 0.213302 Train acc: 0.948333\n",
      "Epoch: 301/500 Iteration: 2715 Train loss: 0.243856 Train acc: 0.933333\n",
      "Epoch: 302/500 Iteration: 2720 Train loss: 0.264908 Train acc: 0.921667\n",
      "Epoch: 302/500 Iteration: 2725 Train loss: 0.243520 Train acc: 0.925000\n",
      "Epoch: 302/500 Iteration: 2725 Validation loss: 0.204266 Validation acc: 0.930000\n",
      "Epoch: 303/500 Iteration: 2730 Train loss: 0.260823 Train acc: 0.918333\n",
      "Epoch: 303/500 Iteration: 2735 Train loss: 0.261543 Train acc: 0.920000\n",
      "Epoch: 304/500 Iteration: 2740 Train loss: 0.232475 Train acc: 0.938333\n",
      "Epoch: 304/500 Iteration: 2745 Train loss: 0.230620 Train acc: 0.930000\n",
      "Epoch: 305/500 Iteration: 2750 Train loss: 0.225841 Train acc: 0.946667\n",
      "Epoch: 305/500 Iteration: 2750 Validation loss: 0.194053 Validation acc: 0.930833\n",
      "Epoch: 306/500 Iteration: 2755 Train loss: 0.195673 Train acc: 0.963333\n",
      "Epoch: 306/500 Iteration: 2760 Train loss: 0.236075 Train acc: 0.920000\n",
      "Epoch: 307/500 Iteration: 2765 Train loss: 0.255568 Train acc: 0.923333\n",
      "Epoch: 307/500 Iteration: 2770 Train loss: 0.247421 Train acc: 0.925000\n",
      "Epoch: 308/500 Iteration: 2775 Train loss: 0.252641 Train acc: 0.925000\n",
      "Epoch: 308/500 Iteration: 2775 Validation loss: 0.194694 Validation acc: 0.937500\n",
      "Epoch: 308/500 Iteration: 2780 Train loss: 0.258166 Train acc: 0.916667\n",
      "Epoch: 309/500 Iteration: 2785 Train loss: 0.236362 Train acc: 0.931667\n",
      "Epoch: 309/500 Iteration: 2790 Train loss: 0.241448 Train acc: 0.930000\n",
      "Epoch: 310/500 Iteration: 2795 Train loss: 0.194417 Train acc: 0.956667\n",
      "Epoch: 311/500 Iteration: 2800 Train loss: 0.187485 Train acc: 0.960000\n",
      "Epoch: 311/500 Iteration: 2800 Validation loss: 0.194971 Validation acc: 0.933333\n",
      "Epoch: 311/500 Iteration: 2805 Train loss: 0.245487 Train acc: 0.931667\n",
      "Epoch: 312/500 Iteration: 2810 Train loss: 0.246310 Train acc: 0.926667\n",
      "Epoch: 312/500 Iteration: 2815 Train loss: 0.206786 Train acc: 0.941667\n",
      "Epoch: 313/500 Iteration: 2820 Train loss: 0.231547 Train acc: 0.936667\n",
      "Epoch: 313/500 Iteration: 2825 Train loss: 0.248366 Train acc: 0.920000\n",
      "Epoch: 313/500 Iteration: 2825 Validation loss: 0.183976 Validation acc: 0.940000\n",
      "Epoch: 314/500 Iteration: 2830 Train loss: 0.244880 Train acc: 0.941667\n",
      "Epoch: 314/500 Iteration: 2835 Train loss: 0.237540 Train acc: 0.923333\n",
      "Epoch: 315/500 Iteration: 2840 Train loss: 0.191194 Train acc: 0.958333\n",
      "Epoch: 316/500 Iteration: 2845 Train loss: 0.181597 Train acc: 0.946667\n",
      "Epoch: 316/500 Iteration: 2850 Train loss: 0.253448 Train acc: 0.928333\n",
      "Epoch: 316/500 Iteration: 2850 Validation loss: 0.205713 Validation acc: 0.926667\n",
      "Epoch: 317/500 Iteration: 2855 Train loss: 0.234561 Train acc: 0.923333\n",
      "Epoch: 317/500 Iteration: 2860 Train loss: 0.218712 Train acc: 0.935000\n",
      "Epoch: 318/500 Iteration: 2865 Train loss: 0.237302 Train acc: 0.926667\n",
      "Epoch: 318/500 Iteration: 2870 Train loss: 0.260921 Train acc: 0.908333\n",
      "Epoch: 319/500 Iteration: 2875 Train loss: 0.212651 Train acc: 0.935000\n",
      "Epoch: 319/500 Iteration: 2875 Validation loss: 0.184911 Validation acc: 0.936667\n",
      "Epoch: 319/500 Iteration: 2880 Train loss: 0.215383 Train acc: 0.940000\n",
      "Epoch: 320/500 Iteration: 2885 Train loss: 0.179250 Train acc: 0.955000\n",
      "Epoch: 321/500 Iteration: 2890 Train loss: 0.171393 Train acc: 0.956667\n",
      "Epoch: 321/500 Iteration: 2895 Train loss: 0.221885 Train acc: 0.926667\n",
      "Epoch: 322/500 Iteration: 2900 Train loss: 0.236922 Train acc: 0.938333\n",
      "Epoch: 322/500 Iteration: 2900 Validation loss: 0.186674 Validation acc: 0.934167\n",
      "Epoch: 322/500 Iteration: 2905 Train loss: 0.189699 Train acc: 0.956667\n",
      "Epoch: 323/500 Iteration: 2910 Train loss: 0.239847 Train acc: 0.931667\n",
      "Epoch: 323/500 Iteration: 2915 Train loss: 0.245565 Train acc: 0.930000\n",
      "Epoch: 324/500 Iteration: 2920 Train loss: 0.207327 Train acc: 0.943333\n",
      "Epoch: 324/500 Iteration: 2925 Train loss: 0.212771 Train acc: 0.938333\n",
      "Epoch: 324/500 Iteration: 2925 Validation loss: 0.181015 Validation acc: 0.936667\n",
      "Epoch: 325/500 Iteration: 2930 Train loss: 0.204739 Train acc: 0.951667\n",
      "Epoch: 326/500 Iteration: 2935 Train loss: 0.179788 Train acc: 0.956667\n",
      "Epoch: 326/500 Iteration: 2940 Train loss: 0.206603 Train acc: 0.946667\n",
      "Epoch: 327/500 Iteration: 2945 Train loss: 0.248557 Train acc: 0.928333\n",
      "Epoch: 327/500 Iteration: 2950 Train loss: 0.206332 Train acc: 0.943333\n",
      "Epoch: 327/500 Iteration: 2950 Validation loss: 0.186745 Validation acc: 0.935000\n",
      "Epoch: 328/500 Iteration: 2955 Train loss: 0.221562 Train acc: 0.935000\n",
      "Epoch: 328/500 Iteration: 2960 Train loss: 0.234772 Train acc: 0.935000\n",
      "Epoch: 329/500 Iteration: 2965 Train loss: 0.202036 Train acc: 0.943333\n",
      "Epoch: 329/500 Iteration: 2970 Train loss: 0.212473 Train acc: 0.946667\n",
      "Epoch: 330/500 Iteration: 2975 Train loss: 0.192678 Train acc: 0.940000\n",
      "Epoch: 330/500 Iteration: 2975 Validation loss: 0.180261 Validation acc: 0.936667\n",
      "Epoch: 331/500 Iteration: 2980 Train loss: 0.161714 Train acc: 0.955000\n",
      "Epoch: 331/500 Iteration: 2985 Train loss: 0.216765 Train acc: 0.933333\n",
      "Epoch: 332/500 Iteration: 2990 Train loss: 0.215379 Train acc: 0.926667\n",
      "Epoch: 332/500 Iteration: 2995 Train loss: 0.182220 Train acc: 0.940000\n",
      "Epoch: 333/500 Iteration: 3000 Train loss: 0.234420 Train acc: 0.916667\n",
      "Epoch: 333/500 Iteration: 3000 Validation loss: 0.176266 Validation acc: 0.934167\n",
      "Epoch: 333/500 Iteration: 3005 Train loss: 0.233518 Train acc: 0.921667\n",
      "Epoch: 334/500 Iteration: 3010 Train loss: 0.197731 Train acc: 0.938333\n",
      "Epoch: 334/500 Iteration: 3015 Train loss: 0.197705 Train acc: 0.950000\n",
      "Epoch: 335/500 Iteration: 3020 Train loss: 0.173321 Train acc: 0.951667\n",
      "Epoch: 336/500 Iteration: 3025 Train loss: 0.176947 Train acc: 0.953333\n",
      "Epoch: 336/500 Iteration: 3025 Validation loss: 0.175601 Validation acc: 0.940000\n",
      "Epoch: 336/500 Iteration: 3030 Train loss: 0.199746 Train acc: 0.950000\n",
      "Epoch: 337/500 Iteration: 3035 Train loss: 0.220769 Train acc: 0.935000\n",
      "Epoch: 337/500 Iteration: 3040 Train loss: 0.209933 Train acc: 0.938333\n",
      "Epoch: 338/500 Iteration: 3045 Train loss: 0.212279 Train acc: 0.936667\n",
      "Epoch: 338/500 Iteration: 3050 Train loss: 0.234807 Train acc: 0.930000\n",
      "Epoch: 338/500 Iteration: 3050 Validation loss: 0.179269 Validation acc: 0.935000\n",
      "Epoch: 339/500 Iteration: 3055 Train loss: 0.184539 Train acc: 0.950000\n",
      "Epoch: 339/500 Iteration: 3060 Train loss: 0.199667 Train acc: 0.950000\n",
      "Epoch: 340/500 Iteration: 3065 Train loss: 0.168885 Train acc: 0.943333\n",
      "Epoch: 341/500 Iteration: 3070 Train loss: 0.183413 Train acc: 0.950000\n",
      "Epoch: 341/500 Iteration: 3075 Train loss: 0.210789 Train acc: 0.931667\n",
      "Epoch: 341/500 Iteration: 3075 Validation loss: 0.169054 Validation acc: 0.937500\n",
      "Epoch: 342/500 Iteration: 3080 Train loss: 0.225390 Train acc: 0.926667\n",
      "Epoch: 342/500 Iteration: 3085 Train loss: 0.170655 Train acc: 0.953333\n",
      "Epoch: 343/500 Iteration: 3090 Train loss: 0.207334 Train acc: 0.935000\n",
      "Epoch: 343/500 Iteration: 3095 Train loss: 0.221525 Train acc: 0.931667\n",
      "Epoch: 344/500 Iteration: 3100 Train loss: 0.189736 Train acc: 0.950000\n",
      "Epoch: 344/500 Iteration: 3100 Validation loss: 0.179271 Validation acc: 0.935833\n",
      "Epoch: 344/500 Iteration: 3105 Train loss: 0.194329 Train acc: 0.951667\n",
      "Epoch: 345/500 Iteration: 3110 Train loss: 0.181760 Train acc: 0.946667\n",
      "Epoch: 346/500 Iteration: 3115 Train loss: 0.165711 Train acc: 0.948333\n",
      "Epoch: 346/500 Iteration: 3120 Train loss: 0.204899 Train acc: 0.936667\n",
      "Epoch: 347/500 Iteration: 3125 Train loss: 0.218232 Train acc: 0.941667\n",
      "Epoch: 347/500 Iteration: 3125 Validation loss: 0.168447 Validation acc: 0.937500\n",
      "Epoch: 347/500 Iteration: 3130 Train loss: 0.174806 Train acc: 0.951667\n",
      "Epoch: 348/500 Iteration: 3135 Train loss: 0.208452 Train acc: 0.935000\n",
      "Epoch: 348/500 Iteration: 3140 Train loss: 0.206739 Train acc: 0.931667\n",
      "Epoch: 349/500 Iteration: 3145 Train loss: 0.210446 Train acc: 0.948333\n",
      "Epoch: 349/500 Iteration: 3150 Train loss: 0.202530 Train acc: 0.946667\n",
      "Epoch: 349/500 Iteration: 3150 Validation loss: 0.175083 Validation acc: 0.936667\n",
      "Epoch: 350/500 Iteration: 3155 Train loss: 0.164855 Train acc: 0.953333\n",
      "Epoch: 351/500 Iteration: 3160 Train loss: 0.158634 Train acc: 0.951667\n",
      "Epoch: 351/500 Iteration: 3165 Train loss: 0.219862 Train acc: 0.940000\n",
      "Epoch: 352/500 Iteration: 3170 Train loss: 0.211092 Train acc: 0.930000\n",
      "Epoch: 352/500 Iteration: 3175 Train loss: 0.185738 Train acc: 0.946667\n",
      "Epoch: 352/500 Iteration: 3175 Validation loss: 0.176121 Validation acc: 0.937500\n",
      "Epoch: 353/500 Iteration: 3180 Train loss: 0.215003 Train acc: 0.940000\n",
      "Epoch: 353/500 Iteration: 3185 Train loss: 0.218640 Train acc: 0.926667\n",
      "Epoch: 354/500 Iteration: 3190 Train loss: 0.197782 Train acc: 0.943333\n",
      "Epoch: 354/500 Iteration: 3195 Train loss: 0.196930 Train acc: 0.946667\n",
      "Epoch: 355/500 Iteration: 3200 Train loss: 0.165029 Train acc: 0.953333\n",
      "Epoch: 355/500 Iteration: 3200 Validation loss: 0.160366 Validation acc: 0.943333\n",
      "Epoch: 356/500 Iteration: 3205 Train loss: 0.154543 Train acc: 0.966667\n",
      "Epoch: 356/500 Iteration: 3210 Train loss: 0.188174 Train acc: 0.946667\n",
      "Epoch: 357/500 Iteration: 3215 Train loss: 0.201331 Train acc: 0.945000\n",
      "Epoch: 357/500 Iteration: 3220 Train loss: 0.166694 Train acc: 0.955000\n",
      "Epoch: 358/500 Iteration: 3225 Train loss: 0.216708 Train acc: 0.935000\n",
      "Epoch: 358/500 Iteration: 3225 Validation loss: 0.161682 Validation acc: 0.942500\n",
      "Epoch: 358/500 Iteration: 3230 Train loss: 0.226055 Train acc: 0.928333\n",
      "Epoch: 359/500 Iteration: 3235 Train loss: 0.175275 Train acc: 0.948333\n",
      "Epoch: 359/500 Iteration: 3240 Train loss: 0.193859 Train acc: 0.950000\n",
      "Epoch: 360/500 Iteration: 3245 Train loss: 0.174342 Train acc: 0.953333\n",
      "Epoch: 361/500 Iteration: 3250 Train loss: 0.163458 Train acc: 0.955000\n",
      "Epoch: 361/500 Iteration: 3250 Validation loss: 0.170288 Validation acc: 0.941667\n",
      "Epoch: 361/500 Iteration: 3255 Train loss: 0.201665 Train acc: 0.941667\n",
      "Epoch: 362/500 Iteration: 3260 Train loss: 0.208065 Train acc: 0.933333\n",
      "Epoch: 362/500 Iteration: 3265 Train loss: 0.192174 Train acc: 0.938333\n",
      "Epoch: 363/500 Iteration: 3270 Train loss: 0.197970 Train acc: 0.943333\n",
      "Epoch: 363/500 Iteration: 3275 Train loss: 0.220469 Train acc: 0.933333\n",
      "Epoch: 363/500 Iteration: 3275 Validation loss: 0.167227 Validation acc: 0.939167\n",
      "Epoch: 364/500 Iteration: 3280 Train loss: 0.191252 Train acc: 0.951667\n",
      "Epoch: 364/500 Iteration: 3285 Train loss: 0.193004 Train acc: 0.936667\n",
      "Epoch: 365/500 Iteration: 3290 Train loss: 0.169099 Train acc: 0.953333\n",
      "Epoch: 366/500 Iteration: 3295 Train loss: 0.159128 Train acc: 0.958333\n",
      "Epoch: 366/500 Iteration: 3300 Train loss: 0.180043 Train acc: 0.941667\n",
      "Epoch: 366/500 Iteration: 3300 Validation loss: 0.156563 Validation acc: 0.942500\n",
      "Epoch: 367/500 Iteration: 3305 Train loss: 0.230508 Train acc: 0.925000\n",
      "Epoch: 367/500 Iteration: 3310 Train loss: 0.192678 Train acc: 0.951667\n",
      "Epoch: 368/500 Iteration: 3315 Train loss: 0.237423 Train acc: 0.931667\n",
      "Epoch: 368/500 Iteration: 3320 Train loss: 0.227886 Train acc: 0.933333\n",
      "Epoch: 369/500 Iteration: 3325 Train loss: 0.183956 Train acc: 0.955000\n",
      "Epoch: 369/500 Iteration: 3325 Validation loss: 0.158693 Validation acc: 0.940833\n",
      "Epoch: 369/500 Iteration: 3330 Train loss: 0.180507 Train acc: 0.945000\n",
      "Epoch: 370/500 Iteration: 3335 Train loss: 0.168312 Train acc: 0.948333\n",
      "Epoch: 371/500 Iteration: 3340 Train loss: 0.147695 Train acc: 0.963333\n",
      "Epoch: 371/500 Iteration: 3345 Train loss: 0.180416 Train acc: 0.938333\n",
      "Epoch: 372/500 Iteration: 3350 Train loss: 0.209217 Train acc: 0.933333\n",
      "Epoch: 372/500 Iteration: 3350 Validation loss: 0.170717 Validation acc: 0.936667\n",
      "Epoch: 372/500 Iteration: 3355 Train loss: 0.175596 Train acc: 0.945000\n",
      "Epoch: 373/500 Iteration: 3360 Train loss: 0.192556 Train acc: 0.940000\n",
      "Epoch: 373/500 Iteration: 3365 Train loss: 0.201399 Train acc: 0.946667\n",
      "Epoch: 374/500 Iteration: 3370 Train loss: 0.180859 Train acc: 0.955000\n",
      "Epoch: 374/500 Iteration: 3375 Train loss: 0.173568 Train acc: 0.951667\n",
      "Epoch: 374/500 Iteration: 3375 Validation loss: 0.161926 Validation acc: 0.942500\n",
      "Epoch: 375/500 Iteration: 3380 Train loss: 0.156149 Train acc: 0.945000\n",
      "Epoch: 376/500 Iteration: 3385 Train loss: 0.149509 Train acc: 0.955000\n",
      "Epoch: 376/500 Iteration: 3390 Train loss: 0.194572 Train acc: 0.945000\n",
      "Epoch: 377/500 Iteration: 3395 Train loss: 0.183339 Train acc: 0.943333\n",
      "Epoch: 377/500 Iteration: 3400 Train loss: 0.179371 Train acc: 0.946667\n",
      "Epoch: 377/500 Iteration: 3400 Validation loss: 0.162228 Validation acc: 0.940833\n",
      "Epoch: 378/500 Iteration: 3405 Train loss: 0.206476 Train acc: 0.935000\n",
      "Epoch: 378/500 Iteration: 3410 Train loss: 0.209547 Train acc: 0.933333\n",
      "Epoch: 379/500 Iteration: 3415 Train loss: 0.175424 Train acc: 0.945000\n",
      "Epoch: 379/500 Iteration: 3420 Train loss: 0.176560 Train acc: 0.950000\n",
      "Epoch: 380/500 Iteration: 3425 Train loss: 0.166581 Train acc: 0.951667\n",
      "Epoch: 380/500 Iteration: 3425 Validation loss: 0.160780 Validation acc: 0.939167\n",
      "Epoch: 381/500 Iteration: 3430 Train loss: 0.164304 Train acc: 0.958333\n",
      "Epoch: 381/500 Iteration: 3435 Train loss: 0.170784 Train acc: 0.951667\n",
      "Epoch: 382/500 Iteration: 3440 Train loss: 0.180064 Train acc: 0.943333\n",
      "Epoch: 382/500 Iteration: 3445 Train loss: 0.180183 Train acc: 0.945000\n",
      "Epoch: 383/500 Iteration: 3450 Train loss: 0.185066 Train acc: 0.941667\n",
      "Epoch: 383/500 Iteration: 3450 Validation loss: 0.151066 Validation acc: 0.944167\n",
      "Epoch: 383/500 Iteration: 3455 Train loss: 0.197787 Train acc: 0.926667\n",
      "Epoch: 384/500 Iteration: 3460 Train loss: 0.157137 Train acc: 0.956667\n",
      "Epoch: 384/500 Iteration: 3465 Train loss: 0.180230 Train acc: 0.950000\n",
      "Epoch: 385/500 Iteration: 3470 Train loss: 0.163314 Train acc: 0.948333\n",
      "Epoch: 386/500 Iteration: 3475 Train loss: 0.133571 Train acc: 0.961667\n",
      "Epoch: 386/500 Iteration: 3475 Validation loss: 0.154624 Validation acc: 0.945833\n",
      "Epoch: 386/500 Iteration: 3480 Train loss: 0.178274 Train acc: 0.938333\n",
      "Epoch: 387/500 Iteration: 3485 Train loss: 0.177349 Train acc: 0.950000\n",
      "Epoch: 387/500 Iteration: 3490 Train loss: 0.173138 Train acc: 0.945000\n",
      "Epoch: 388/500 Iteration: 3495 Train loss: 0.203008 Train acc: 0.935000\n",
      "Epoch: 388/500 Iteration: 3500 Train loss: 0.203363 Train acc: 0.940000\n",
      "Epoch: 388/500 Iteration: 3500 Validation loss: 0.156724 Validation acc: 0.942500\n",
      "Epoch: 389/500 Iteration: 3505 Train loss: 0.161917 Train acc: 0.961667\n",
      "Epoch: 389/500 Iteration: 3510 Train loss: 0.177365 Train acc: 0.948333\n",
      "Epoch: 390/500 Iteration: 3515 Train loss: 0.157489 Train acc: 0.951667\n",
      "Epoch: 391/500 Iteration: 3520 Train loss: 0.142837 Train acc: 0.960000\n",
      "Epoch: 391/500 Iteration: 3525 Train loss: 0.183864 Train acc: 0.938333\n",
      "Epoch: 391/500 Iteration: 3525 Validation loss: 0.162354 Validation acc: 0.940833\n",
      "Epoch: 392/500 Iteration: 3530 Train loss: 0.185747 Train acc: 0.943333\n",
      "Epoch: 392/500 Iteration: 3535 Train loss: 0.159663 Train acc: 0.960000\n",
      "Epoch: 393/500 Iteration: 3540 Train loss: 0.204136 Train acc: 0.938333\n",
      "Epoch: 393/500 Iteration: 3545 Train loss: 0.193818 Train acc: 0.940000\n",
      "Epoch: 394/500 Iteration: 3550 Train loss: 0.168069 Train acc: 0.953333\n",
      "Epoch: 394/500 Iteration: 3550 Validation loss: 0.155621 Validation acc: 0.940833\n",
      "Epoch: 394/500 Iteration: 3555 Train loss: 0.172204 Train acc: 0.946667\n",
      "Epoch: 395/500 Iteration: 3560 Train loss: 0.158612 Train acc: 0.956667\n",
      "Epoch: 396/500 Iteration: 3565 Train loss: 0.151305 Train acc: 0.953333\n",
      "Epoch: 396/500 Iteration: 3570 Train loss: 0.170126 Train acc: 0.950000\n",
      "Epoch: 397/500 Iteration: 3575 Train loss: 0.191250 Train acc: 0.953333\n",
      "Epoch: 397/500 Iteration: 3575 Validation loss: 0.159776 Validation acc: 0.944167\n",
      "Epoch: 397/500 Iteration: 3580 Train loss: 0.161310 Train acc: 0.950000\n",
      "Epoch: 398/500 Iteration: 3585 Train loss: 0.178760 Train acc: 0.945000\n",
      "Epoch: 398/500 Iteration: 3590 Train loss: 0.197265 Train acc: 0.938333\n",
      "Epoch: 399/500 Iteration: 3595 Train loss: 0.159002 Train acc: 0.951667\n",
      "Epoch: 399/500 Iteration: 3600 Train loss: 0.155088 Train acc: 0.956667\n",
      "Epoch: 399/500 Iteration: 3600 Validation loss: 0.150819 Validation acc: 0.945833\n",
      "Epoch: 400/500 Iteration: 3605 Train loss: 0.161640 Train acc: 0.948333\n",
      "Epoch: 401/500 Iteration: 3610 Train loss: 0.136562 Train acc: 0.958333\n",
      "Epoch: 401/500 Iteration: 3615 Train loss: 0.164590 Train acc: 0.953333\n",
      "Epoch: 402/500 Iteration: 3620 Train loss: 0.171096 Train acc: 0.948333\n",
      "Epoch: 402/500 Iteration: 3625 Train loss: 0.161886 Train acc: 0.953333\n",
      "Epoch: 402/500 Iteration: 3625 Validation loss: 0.170478 Validation acc: 0.936667\n",
      "Epoch: 403/500 Iteration: 3630 Train loss: 0.172607 Train acc: 0.948333\n",
      "Epoch: 403/500 Iteration: 3635 Train loss: 0.196709 Train acc: 0.935000\n",
      "Epoch: 404/500 Iteration: 3640 Train loss: 0.155568 Train acc: 0.960000\n",
      "Epoch: 404/500 Iteration: 3645 Train loss: 0.158394 Train acc: 0.951667\n",
      "Epoch: 405/500 Iteration: 3650 Train loss: 0.146646 Train acc: 0.950000\n",
      "Epoch: 405/500 Iteration: 3650 Validation loss: 0.164306 Validation acc: 0.942500\n",
      "Epoch: 406/500 Iteration: 3655 Train loss: 0.151636 Train acc: 0.961667\n",
      "Epoch: 406/500 Iteration: 3660 Train loss: 0.164830 Train acc: 0.946667\n",
      "Epoch: 407/500 Iteration: 3665 Train loss: 0.171975 Train acc: 0.943333\n",
      "Epoch: 407/500 Iteration: 3670 Train loss: 0.146675 Train acc: 0.953333\n",
      "Epoch: 408/500 Iteration: 3675 Train loss: 0.169048 Train acc: 0.950000\n",
      "Epoch: 408/500 Iteration: 3675 Validation loss: 0.155069 Validation acc: 0.944167\n",
      "Epoch: 408/500 Iteration: 3680 Train loss: 0.179320 Train acc: 0.941667\n",
      "Epoch: 409/500 Iteration: 3685 Train loss: 0.159528 Train acc: 0.955000\n",
      "Epoch: 409/500 Iteration: 3690 Train loss: 0.154924 Train acc: 0.961667\n",
      "Epoch: 410/500 Iteration: 3695 Train loss: 0.127854 Train acc: 0.960000\n",
      "Epoch: 411/500 Iteration: 3700 Train loss: 0.127918 Train acc: 0.968333\n",
      "Epoch: 411/500 Iteration: 3700 Validation loss: 0.158708 Validation acc: 0.944167\n",
      "Epoch: 411/500 Iteration: 3705 Train loss: 0.167036 Train acc: 0.946667\n",
      "Epoch: 412/500 Iteration: 3710 Train loss: 0.167073 Train acc: 0.953333\n",
      "Epoch: 412/500 Iteration: 3715 Train loss: 0.160247 Train acc: 0.960000\n",
      "Epoch: 413/500 Iteration: 3720 Train loss: 0.167113 Train acc: 0.940000\n",
      "Epoch: 413/500 Iteration: 3725 Train loss: 0.191106 Train acc: 0.940000\n",
      "Epoch: 413/500 Iteration: 3725 Validation loss: 0.170630 Validation acc: 0.937500\n",
      "Epoch: 414/500 Iteration: 3730 Train loss: 0.151679 Train acc: 0.960000\n",
      "Epoch: 414/500 Iteration: 3735 Train loss: 0.179276 Train acc: 0.943333\n",
      "Epoch: 415/500 Iteration: 3740 Train loss: 0.157610 Train acc: 0.956667\n",
      "Epoch: 416/500 Iteration: 3745 Train loss: 0.126380 Train acc: 0.960000\n",
      "Epoch: 416/500 Iteration: 3750 Train loss: 0.167576 Train acc: 0.940000\n",
      "Epoch: 416/500 Iteration: 3750 Validation loss: 0.148076 Validation acc: 0.946667\n",
      "Epoch: 417/500 Iteration: 3755 Train loss: 0.197239 Train acc: 0.943333\n",
      "Epoch: 417/500 Iteration: 3760 Train loss: 0.161086 Train acc: 0.948333\n",
      "Epoch: 418/500 Iteration: 3765 Train loss: 0.154717 Train acc: 0.951667\n",
      "Epoch: 418/500 Iteration: 3770 Train loss: 0.178968 Train acc: 0.936667\n",
      "Epoch: 419/500 Iteration: 3775 Train loss: 0.147174 Train acc: 0.953333\n",
      "Epoch: 419/500 Iteration: 3775 Validation loss: 0.158769 Validation acc: 0.941667\n",
      "Epoch: 419/500 Iteration: 3780 Train loss: 0.165368 Train acc: 0.951667\n",
      "Epoch: 420/500 Iteration: 3785 Train loss: 0.140651 Train acc: 0.956667\n",
      "Epoch: 421/500 Iteration: 3790 Train loss: 0.143683 Train acc: 0.965000\n",
      "Epoch: 421/500 Iteration: 3795 Train loss: 0.163146 Train acc: 0.958333\n",
      "Epoch: 422/500 Iteration: 3800 Train loss: 0.157765 Train acc: 0.948333\n",
      "Epoch: 422/500 Iteration: 3800 Validation loss: 0.158649 Validation acc: 0.940833\n",
      "Epoch: 422/500 Iteration: 3805 Train loss: 0.161076 Train acc: 0.951667\n",
      "Epoch: 423/500 Iteration: 3810 Train loss: 0.186669 Train acc: 0.946667\n",
      "Epoch: 423/500 Iteration: 3815 Train loss: 0.182666 Train acc: 0.940000\n",
      "Epoch: 424/500 Iteration: 3820 Train loss: 0.158238 Train acc: 0.960000\n",
      "Epoch: 424/500 Iteration: 3825 Train loss: 0.158367 Train acc: 0.953333\n",
      "Epoch: 424/500 Iteration: 3825 Validation loss: 0.140519 Validation acc: 0.946667\n",
      "Epoch: 425/500 Iteration: 3830 Train loss: 0.139354 Train acc: 0.956667\n",
      "Epoch: 426/500 Iteration: 3835 Train loss: 0.120777 Train acc: 0.965000\n",
      "Epoch: 426/500 Iteration: 3840 Train loss: 0.159780 Train acc: 0.953333\n",
      "Epoch: 427/500 Iteration: 3845 Train loss: 0.162300 Train acc: 0.950000\n",
      "Epoch: 427/500 Iteration: 3850 Train loss: 0.131974 Train acc: 0.963333\n",
      "Epoch: 427/500 Iteration: 3850 Validation loss: 0.154550 Validation acc: 0.944167\n",
      "Epoch: 428/500 Iteration: 3855 Train loss: 0.178912 Train acc: 0.943333\n",
      "Epoch: 428/500 Iteration: 3860 Train loss: 0.191204 Train acc: 0.930000\n",
      "Epoch: 429/500 Iteration: 3865 Train loss: 0.147855 Train acc: 0.958333\n",
      "Epoch: 429/500 Iteration: 3870 Train loss: 0.159396 Train acc: 0.956667\n",
      "Epoch: 430/500 Iteration: 3875 Train loss: 0.134700 Train acc: 0.953333\n",
      "Epoch: 430/500 Iteration: 3875 Validation loss: 0.153115 Validation acc: 0.943333\n",
      "Epoch: 431/500 Iteration: 3880 Train loss: 0.130251 Train acc: 0.960000\n",
      "Epoch: 431/500 Iteration: 3885 Train loss: 0.177487 Train acc: 0.945000\n",
      "Epoch: 432/500 Iteration: 3890 Train loss: 0.170792 Train acc: 0.950000\n",
      "Epoch: 432/500 Iteration: 3895 Train loss: 0.153120 Train acc: 0.960000\n",
      "Epoch: 433/500 Iteration: 3900 Train loss: 0.155114 Train acc: 0.951667\n",
      "Epoch: 433/500 Iteration: 3900 Validation loss: 0.147971 Validation acc: 0.945000\n",
      "Epoch: 433/500 Iteration: 3905 Train loss: 0.184853 Train acc: 0.931667\n",
      "Epoch: 434/500 Iteration: 3910 Train loss: 0.145910 Train acc: 0.956667\n",
      "Epoch: 434/500 Iteration: 3915 Train loss: 0.143362 Train acc: 0.953333\n",
      "Epoch: 435/500 Iteration: 3920 Train loss: 0.147634 Train acc: 0.953333\n",
      "Epoch: 436/500 Iteration: 3925 Train loss: 0.140540 Train acc: 0.961667\n",
      "Epoch: 436/500 Iteration: 3925 Validation loss: 0.157438 Validation acc: 0.943333\n",
      "Epoch: 436/500 Iteration: 3930 Train loss: 0.152224 Train acc: 0.945000\n",
      "Epoch: 437/500 Iteration: 3935 Train loss: 0.163535 Train acc: 0.941667\n",
      "Epoch: 437/500 Iteration: 3940 Train loss: 0.158354 Train acc: 0.953333\n",
      "Epoch: 438/500 Iteration: 3945 Train loss: 0.152729 Train acc: 0.953333\n",
      "Epoch: 438/500 Iteration: 3950 Train loss: 0.174564 Train acc: 0.936667\n",
      "Epoch: 438/500 Iteration: 3950 Validation loss: 0.168779 Validation acc: 0.940000\n",
      "Epoch: 439/500 Iteration: 3955 Train loss: 0.150341 Train acc: 0.960000\n",
      "Epoch: 439/500 Iteration: 3960 Train loss: 0.167960 Train acc: 0.945000\n",
      "Epoch: 440/500 Iteration: 3965 Train loss: 0.146671 Train acc: 0.953333\n",
      "Epoch: 441/500 Iteration: 3970 Train loss: 0.135314 Train acc: 0.966667\n",
      "Epoch: 441/500 Iteration: 3975 Train loss: 0.163817 Train acc: 0.948333\n",
      "Epoch: 441/500 Iteration: 3975 Validation loss: 0.144250 Validation acc: 0.944167\n",
      "Epoch: 442/500 Iteration: 3980 Train loss: 0.159883 Train acc: 0.946667\n",
      "Epoch: 442/500 Iteration: 3985 Train loss: 0.157425 Train acc: 0.943333\n",
      "Epoch: 443/500 Iteration: 3990 Train loss: 0.165203 Train acc: 0.950000\n",
      "Epoch: 443/500 Iteration: 3995 Train loss: 0.181631 Train acc: 0.943333\n",
      "Epoch: 444/500 Iteration: 4000 Train loss: 0.141224 Train acc: 0.960000\n",
      "Epoch: 444/500 Iteration: 4000 Validation loss: 0.159561 Validation acc: 0.940833\n",
      "Epoch: 444/500 Iteration: 4005 Train loss: 0.151873 Train acc: 0.946667\n",
      "Epoch: 445/500 Iteration: 4010 Train loss: 0.137028 Train acc: 0.960000\n",
      "Epoch: 446/500 Iteration: 4015 Train loss: 0.121860 Train acc: 0.958333\n",
      "Epoch: 446/500 Iteration: 4020 Train loss: 0.155831 Train acc: 0.955000\n",
      "Epoch: 447/500 Iteration: 4025 Train loss: 0.179346 Train acc: 0.943333\n",
      "Epoch: 447/500 Iteration: 4025 Validation loss: 0.152899 Validation acc: 0.943333\n",
      "Epoch: 447/500 Iteration: 4030 Train loss: 0.155596 Train acc: 0.958333\n",
      "Epoch: 448/500 Iteration: 4035 Train loss: 0.171438 Train acc: 0.941667\n",
      "Epoch: 448/500 Iteration: 4040 Train loss: 0.182532 Train acc: 0.940000\n",
      "Epoch: 449/500 Iteration: 4045 Train loss: 0.134674 Train acc: 0.961667\n",
      "Epoch: 449/500 Iteration: 4050 Train loss: 0.131660 Train acc: 0.955000\n",
      "Epoch: 449/500 Iteration: 4050 Validation loss: 0.135403 Validation acc: 0.950000\n",
      "Epoch: 450/500 Iteration: 4055 Train loss: 0.134001 Train acc: 0.951667\n",
      "Epoch: 451/500 Iteration: 4060 Train loss: 0.122812 Train acc: 0.968333\n",
      "Epoch: 451/500 Iteration: 4065 Train loss: 0.156027 Train acc: 0.953333\n",
      "Epoch: 452/500 Iteration: 4070 Train loss: 0.172832 Train acc: 0.936667\n",
      "Epoch: 452/500 Iteration: 4075 Train loss: 0.150255 Train acc: 0.951667\n",
      "Epoch: 452/500 Iteration: 4075 Validation loss: 0.150664 Validation acc: 0.943333\n",
      "Epoch: 453/500 Iteration: 4080 Train loss: 0.153930 Train acc: 0.953333\n",
      "Epoch: 453/500 Iteration: 4085 Train loss: 0.187406 Train acc: 0.931667\n",
      "Epoch: 454/500 Iteration: 4090 Train loss: 0.131789 Train acc: 0.961667\n",
      "Epoch: 454/500 Iteration: 4095 Train loss: 0.159469 Train acc: 0.956667\n",
      "Epoch: 455/500 Iteration: 4100 Train loss: 0.135565 Train acc: 0.965000\n",
      "Epoch: 455/500 Iteration: 4100 Validation loss: 0.154048 Validation acc: 0.943333\n",
      "Epoch: 456/500 Iteration: 4105 Train loss: 0.124315 Train acc: 0.963333\n",
      "Epoch: 456/500 Iteration: 4110 Train loss: 0.153916 Train acc: 0.946667\n",
      "Epoch: 457/500 Iteration: 4115 Train loss: 0.168309 Train acc: 0.945000\n",
      "Epoch: 457/500 Iteration: 4120 Train loss: 0.155688 Train acc: 0.955000\n",
      "Epoch: 458/500 Iteration: 4125 Train loss: 0.146279 Train acc: 0.941667\n",
      "Epoch: 458/500 Iteration: 4125 Validation loss: 0.139464 Validation acc: 0.946667\n",
      "Epoch: 458/500 Iteration: 4130 Train loss: 0.163870 Train acc: 0.950000\n",
      "Epoch: 459/500 Iteration: 4135 Train loss: 0.130395 Train acc: 0.956667\n",
      "Epoch: 459/500 Iteration: 4140 Train loss: 0.148350 Train acc: 0.951667\n",
      "Epoch: 460/500 Iteration: 4145 Train loss: 0.132265 Train acc: 0.958333\n",
      "Epoch: 461/500 Iteration: 4150 Train loss: 0.118132 Train acc: 0.971667\n",
      "Epoch: 461/500 Iteration: 4150 Validation loss: 0.146958 Validation acc: 0.948333\n",
      "Epoch: 461/500 Iteration: 4155 Train loss: 0.147921 Train acc: 0.950000\n",
      "Epoch: 462/500 Iteration: 4160 Train loss: 0.168259 Train acc: 0.943333\n",
      "Epoch: 462/500 Iteration: 4165 Train loss: 0.141473 Train acc: 0.956667\n",
      "Epoch: 463/500 Iteration: 4170 Train loss: 0.159434 Train acc: 0.950000\n",
      "Epoch: 463/500 Iteration: 4175 Train loss: 0.179280 Train acc: 0.935000\n",
      "Epoch: 463/500 Iteration: 4175 Validation loss: 0.147152 Validation acc: 0.946667\n",
      "Epoch: 464/500 Iteration: 4180 Train loss: 0.134262 Train acc: 0.961667\n",
      "Epoch: 464/500 Iteration: 4185 Train loss: 0.156712 Train acc: 0.951667\n",
      "Epoch: 465/500 Iteration: 4190 Train loss: 0.126670 Train acc: 0.960000\n",
      "Epoch: 466/500 Iteration: 4195 Train loss: 0.108323 Train acc: 0.965000\n",
      "Epoch: 466/500 Iteration: 4200 Train loss: 0.140676 Train acc: 0.951667\n",
      "Epoch: 466/500 Iteration: 4200 Validation loss: 0.136095 Validation acc: 0.948333\n",
      "Epoch: 467/500 Iteration: 4205 Train loss: 0.171658 Train acc: 0.948333\n",
      "Epoch: 467/500 Iteration: 4210 Train loss: 0.150795 Train acc: 0.958333\n",
      "Epoch: 468/500 Iteration: 4215 Train loss: 0.151349 Train acc: 0.940000\n",
      "Epoch: 468/500 Iteration: 4220 Train loss: 0.172694 Train acc: 0.936667\n",
      "Epoch: 469/500 Iteration: 4225 Train loss: 0.155377 Train acc: 0.951667\n",
      "Epoch: 469/500 Iteration: 4225 Validation loss: 0.139008 Validation acc: 0.950833\n",
      "Epoch: 469/500 Iteration: 4230 Train loss: 0.160118 Train acc: 0.948333\n",
      "Epoch: 470/500 Iteration: 4235 Train loss: 0.131728 Train acc: 0.948333\n",
      "Epoch: 471/500 Iteration: 4240 Train loss: 0.117296 Train acc: 0.961667\n",
      "Epoch: 471/500 Iteration: 4245 Train loss: 0.160419 Train acc: 0.953333\n",
      "Epoch: 472/500 Iteration: 4250 Train loss: 0.158991 Train acc: 0.950000\n",
      "Epoch: 472/500 Iteration: 4250 Validation loss: 0.162602 Validation acc: 0.941667\n",
      "Epoch: 472/500 Iteration: 4255 Train loss: 0.144094 Train acc: 0.951667\n",
      "Epoch: 473/500 Iteration: 4260 Train loss: 0.139559 Train acc: 0.948333\n",
      "Epoch: 473/500 Iteration: 4265 Train loss: 0.175392 Train acc: 0.950000\n",
      "Epoch: 474/500 Iteration: 4270 Train loss: 0.144172 Train acc: 0.961667\n",
      "Epoch: 474/500 Iteration: 4275 Train loss: 0.157803 Train acc: 0.956667\n",
      "Epoch: 474/500 Iteration: 4275 Validation loss: 0.144371 Validation acc: 0.947500\n",
      "Epoch: 475/500 Iteration: 4280 Train loss: 0.145219 Train acc: 0.951667\n",
      "Epoch: 476/500 Iteration: 4285 Train loss: 0.115561 Train acc: 0.965000\n",
      "Epoch: 476/500 Iteration: 4290 Train loss: 0.145001 Train acc: 0.953333\n",
      "Epoch: 477/500 Iteration: 4295 Train loss: 0.161959 Train acc: 0.953333\n",
      "Epoch: 477/500 Iteration: 4300 Train loss: 0.133241 Train acc: 0.960000\n",
      "Epoch: 477/500 Iteration: 4300 Validation loss: 0.145406 Validation acc: 0.950833\n",
      "Epoch: 478/500 Iteration: 4305 Train loss: 0.153683 Train acc: 0.933333\n",
      "Epoch: 478/500 Iteration: 4310 Train loss: 0.164473 Train acc: 0.951667\n",
      "Epoch: 479/500 Iteration: 4315 Train loss: 0.133730 Train acc: 0.960000\n",
      "Epoch: 479/500 Iteration: 4320 Train loss: 0.140012 Train acc: 0.956667\n",
      "Epoch: 480/500 Iteration: 4325 Train loss: 0.126182 Train acc: 0.958333\n",
      "Epoch: 480/500 Iteration: 4325 Validation loss: 0.132988 Validation acc: 0.949167\n",
      "Epoch: 481/500 Iteration: 4330 Train loss: 0.117019 Train acc: 0.968333\n",
      "Epoch: 481/500 Iteration: 4335 Train loss: 0.150901 Train acc: 0.953333\n",
      "Epoch: 482/500 Iteration: 4340 Train loss: 0.155124 Train acc: 0.955000\n",
      "Epoch: 482/500 Iteration: 4345 Train loss: 0.133583 Train acc: 0.956667\n",
      "Epoch: 483/500 Iteration: 4350 Train loss: 0.154264 Train acc: 0.943333\n",
      "Epoch: 483/500 Iteration: 4350 Validation loss: 0.137490 Validation acc: 0.946667\n",
      "Epoch: 483/500 Iteration: 4355 Train loss: 0.171643 Train acc: 0.940000\n",
      "Epoch: 484/500 Iteration: 4360 Train loss: 0.126492 Train acc: 0.966667\n",
      "Epoch: 484/500 Iteration: 4365 Train loss: 0.144160 Train acc: 0.948333\n",
      "Epoch: 485/500 Iteration: 4370 Train loss: 0.117647 Train acc: 0.966667\n",
      "Epoch: 486/500 Iteration: 4375 Train loss: 0.110146 Train acc: 0.963333\n",
      "Epoch: 486/500 Iteration: 4375 Validation loss: 0.144421 Validation acc: 0.947500\n",
      "Epoch: 486/500 Iteration: 4380 Train loss: 0.153071 Train acc: 0.948333\n",
      "Epoch: 487/500 Iteration: 4385 Train loss: 0.146226 Train acc: 0.950000\n",
      "Epoch: 487/500 Iteration: 4390 Train loss: 0.143508 Train acc: 0.961667\n",
      "Epoch: 488/500 Iteration: 4395 Train loss: 0.159688 Train acc: 0.950000\n",
      "Epoch: 488/500 Iteration: 4400 Train loss: 0.187587 Train acc: 0.938333\n",
      "Epoch: 488/500 Iteration: 4400 Validation loss: 0.150733 Validation acc: 0.945833\n",
      "Epoch: 489/500 Iteration: 4405 Train loss: 0.133586 Train acc: 0.961667\n",
      "Epoch: 489/500 Iteration: 4410 Train loss: 0.151606 Train acc: 0.956667\n",
      "Epoch: 490/500 Iteration: 4415 Train loss: 0.122348 Train acc: 0.965000\n",
      "Epoch: 491/500 Iteration: 4420 Train loss: 0.114030 Train acc: 0.963333\n",
      "Epoch: 491/500 Iteration: 4425 Train loss: 0.146250 Train acc: 0.950000\n",
      "Epoch: 491/500 Iteration: 4425 Validation loss: 0.145766 Validation acc: 0.948333\n",
      "Epoch: 492/500 Iteration: 4430 Train loss: 0.141014 Train acc: 0.951667\n",
      "Epoch: 492/500 Iteration: 4435 Train loss: 0.131850 Train acc: 0.961667\n",
      "Epoch: 493/500 Iteration: 4440 Train loss: 0.147309 Train acc: 0.946667\n",
      "Epoch: 493/500 Iteration: 4445 Train loss: 0.156619 Train acc: 0.945000\n",
      "Epoch: 494/500 Iteration: 4450 Train loss: 0.137184 Train acc: 0.963333\n",
      "Epoch: 494/500 Iteration: 4450 Validation loss: 0.149726 Validation acc: 0.946667\n",
      "Epoch: 494/500 Iteration: 4455 Train loss: 0.142688 Train acc: 0.961667\n",
      "Epoch: 495/500 Iteration: 4460 Train loss: 0.136373 Train acc: 0.953333\n",
      "Epoch: 496/500 Iteration: 4465 Train loss: 0.121770 Train acc: 0.961667\n",
      "Epoch: 496/500 Iteration: 4470 Train loss: 0.160837 Train acc: 0.950000\n",
      "Epoch: 497/500 Iteration: 4475 Train loss: 0.157435 Train acc: 0.955000\n",
      "Epoch: 497/500 Iteration: 4475 Validation loss: 0.155885 Validation acc: 0.942500\n",
      "Epoch: 497/500 Iteration: 4480 Train loss: 0.131003 Train acc: 0.961667\n",
      "Epoch: 498/500 Iteration: 4485 Train loss: 0.142915 Train acc: 0.953333\n",
      "Epoch: 498/500 Iteration: 4490 Train loss: 0.157110 Train acc: 0.943333\n",
      "Epoch: 499/500 Iteration: 4495 Train loss: 0.141578 Train acc: 0.960000\n",
      "Epoch: 499/500 Iteration: 4500 Train loss: 0.144623 Train acc: 0.965000\n",
      "Epoch: 499/500 Iteration: 4500 Validation loss: 0.149922 Validation acc: 0.945000\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%25 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "    saver.save(sess,\"checkpoints-crnn/har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAF3CAYAAAC2bHyQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0426be2dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 25 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0426be2ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 25 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.855000\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-crnn'))\n",
    "    \n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1}\n",
    "        \n",
    "        batch_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
